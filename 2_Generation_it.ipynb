{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SXghWytGaz4E"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from google.colab import drive\n",
    "import os, zipfile, glob\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8gGPuN1LbDYP",
    "outputId": "3cbf26d4-6fe8-4aa4-b766-58aa684cf027"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0b3Xp-FOR7h9"
   },
   "outputs": [],
   "source": [
    "# Italian books from the gutenberg.org for text generation (top 5 in volume with the right encoding)\n",
    "\n",
    "'65391-0'\n",
    "'52377-0'\n",
    "'48188-0'\n",
    "'47278-0'\n",
    "'56431-0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MNng6ICsStYj",
    "outputId": "2d3243e6-5863-4aa6-9f80-f8d5897b0154"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-18 18:37:05--  https://www.gutenberg.org/files/65391/65391-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2828283 (2.7M) [text/plain]\n",
      "Saving to: ‘65391-0.txt.1’\n",
      "\n",
      "65391-0.txt.1       100%[===================>]   2.70M  17.7MB/s    in 0.2s    \n",
      "\n",
      "2022-01-18 18:37:26 (17.7 MB/s) - ‘65391-0.txt.1’ saved [2828283/2828283]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://www.gutenberg.org/files/65391/65391-0.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9-wqBHK4SfoT",
    "outputId": "9486edbd-6138-4994-9e07-e1355373774e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-18 18:37:26--  https://www.gutenberg.org/files/52377/52377-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2570135 (2.5M) [text/plain]\n",
      "Saving to: ‘52377-0.txt.1’\n",
      "\n",
      "52377-0.txt.1       100%[===================>]   2.45M  16.1MB/s    in 0.2s    \n",
      "\n",
      "2022-01-18 18:37:30 (16.1 MB/s) - ‘52377-0.txt.1’ saved [2570135/2570135]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://www.gutenberg.org/files/52377/52377-0.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JFhljiMVSfrC",
    "outputId": "7fa97a08-507c-49bb-de65-dacb9bd37566"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-18 18:37:30--  https://www.gutenberg.org/files/48188/48188-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2537076 (2.4M) [text/plain]\n",
      "Saving to: ‘48188-0.txt.1’\n",
      "\n",
      "48188-0.txt.1       100%[===================>]   2.42M  16.0MB/s    in 0.2s    \n",
      "\n",
      "2022-01-18 18:37:39 (16.0 MB/s) - ‘48188-0.txt.1’ saved [2537076/2537076]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://www.gutenberg.org/files/48188/48188-0.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vS_TBqxKSft0",
    "outputId": "9d0ac9e4-6c2f-4638-b740-d99f60b0f261"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-18 18:37:39--  https://www.gutenberg.org/files/47278/47278-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2521913 (2.4M) [text/plain]\n",
      "Saving to: ‘47278-0.txt.1’\n",
      "\n",
      "47278-0.txt.1       100%[===================>]   2.40M  14.9MB/s    in 0.2s    \n",
      "\n",
      "2022-01-18 18:37:47 (14.9 MB/s) - ‘47278-0.txt.1’ saved [2521913/2521913]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://www.gutenberg.org/files/47278/47278-0.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eUEKj3PySf3t",
    "outputId": "64667ced-f4f1-412d-cb57-b5be53b3dce6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-18 18:37:47--  https://www.gutenberg.org/files/56431/56431-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2512215 (2.4M) [text/plain]\n",
      "Saving to: ‘56431-0.txt.1’\n",
      "\n",
      "56431-0.txt.1       100%[===================>]   2.40M  14.7MB/s    in 0.2s    \n",
      "\n",
      "2022-01-18 18:37:53 (14.7 MB/s) - ‘56431-0.txt.1’ saved [2512215/2512215]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://www.gutenberg.org/files/56431/56431-0.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w_MSI2ySRthF",
    "outputId": "ae67770b-6d76-4936-9cdb-6b71b0c38322"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ZTBdWWP1BSWC"
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "from bs4 import BeautifulSoup\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "lv7154iU-xLD"
   },
   "outputs": [],
   "source": [
    "list_books = ['65391-0', '52377-0', '48188-0', '47278-0', '56431-0']\n",
    "text_add = ''\n",
    "for b in list_books:\n",
    "  with open(b + '.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "    info_beg = text.find('Title: ')\n",
    "    begin = text.find('*** START OF THIS PROJECT GUTENBERG EBOOK ')\n",
    "    title = ''\n",
    "    if begin > -1:\n",
    "      for i in text[info_beg : begin].split('\\n'):\n",
    "        if i.find('Title: ') > -1:\n",
    "          title = i.split('Title: ')[1]\n",
    "          begin_2 = begin + 42 + len(title) + 3\n",
    "                \n",
    "    else:\n",
    "      begin = text.find('START OF THE PROJECT GUTENBERG EBOOK ')\n",
    "      for i in text[info_beg : begin].split('\\n'):\n",
    "        if i.find('Title: ') > -1:\n",
    "          title = i.split('Title: ')[1]\n",
    "          begin_2 = begin + 37 + len(title) + 3\n",
    "\n",
    "    end = text.find('*** END OF THIS PROJECT GUTENBERG EBOOK')\n",
    "    if end == -1:\n",
    "      end = text.find('*** END')\n",
    "    text = text[begin_2 : end]\n",
    "\n",
    "    #remove html tags from text\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text(separator=\" \")\n",
    "    \n",
    "    #remove accented characters from text, e.g. café\n",
    "    text = unidecode.unidecode(text)\n",
    "\n",
    "    text_add = text_add + text + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SZ1ufTF8-xjI",
    "outputId": "d75381f0-1d8e-4546-88e6-67729a05bfbf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "356603"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "8oLzuNb15tpf"
   },
   "outputs": [],
   "source": [
    "# Create two dictionaries:\n",
    "# int2char -- maps integers to characters\n",
    "# char2int -- maps characters to unique integers\n",
    "\n",
    "int2char = dict(enumerate(set(text)))\n",
    "char2int = { val: idx  for idx, val in int2char.items()}\n",
    "\n",
    "# encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vi0bo7KDMxFx",
    "outputId": "57e7c7ad-5798-4e1a-d9db-8c8dabc80772"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(char2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "QxFS2B9nD-kg"
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # arr - array of integers\n",
    "    # n_labels - number of labels (the size of a one-hot-encoded vector)\n",
    "\n",
    "    one_hot = np.eye(n_labels)[arr]\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hrcsQ3qVECzT",
    "outputId": "57ea6ebc-1cb3-4740-d5ed-c51cb134e5e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# check that the function works correctly\n",
    "test_indx = np.array([[7, 2, 5]])\n",
    "one_hot = one_hot_encode(test_indx, 8)\n",
    "\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Q-0ZuGZhEFd_"
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    # Create a generator that returns batches of size batch_size x seq_length\n",
    "    \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    ## Get the number of batches we can make\n",
    "    n_batches = int(len(arr) / batch_size_total)\n",
    "    \n",
    "    ## Keep only enough characters to make full batches\n",
    "    arr = arr[: n_batches*batch_size_total] \n",
    "    \n",
    "    ## Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1)) \n",
    "    \n",
    "    ## Iterate over the batches using a window of size seq_length\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        x = arr[:, n : n + seq_length ]\n",
    "        # The target is a version of x shifted by one (do not forget border conditions)\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1] = x[:, 1:]\n",
    "        if n + seq_length < arr.shape[1]:\n",
    "            y[:, -1] = arr[:, n + seq_length]\n",
    "        else:\n",
    "            arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "4CCx96r-ESVp"
   },
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 4, 30)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GcefjsfQEXyg",
    "outputId": "9625ed7d-5b60-49c3-f559-4d8bf8395c96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "q9YXlR9GEa3L"
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        # Define the LSTM layer\n",
    "        ## YOUR CODE HERE\n",
    "        self.lstm = nn.LSTM(input_size=len(self.chars), hidden_size=self.n_hidden, \n",
    "                            num_layers=self.n_layers, dropout=self.drop_prob, \n",
    "                            batch_first=True)\n",
    "\n",
    "        # Define a dropout layer\n",
    "        ## YOUR CODE HERE\n",
    "        self.dropout = nn.Dropout(self.drop_prob)\n",
    "\n",
    "        # Define the final, fully-connected output layer\n",
    "        ## YOUR CODE HERE\n",
    "        self.linear = nn.Linear(in_features=self.n_hidden, out_features=len(self.chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        # Get the outputs and the new hidden state from the lstm\n",
    "        ## YOUR CODE HERE\n",
    "        out, hidden = self.lstm(x.float(), hidden)\n",
    "\n",
    "        # Pass through a dropout layer\n",
    "        ## YOUR CODE HERE\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # Stack up LSTM outputs using view\n",
    "        # you may need to use contiguous to reshape the output\n",
    "        ## YOUR CODE HERE\n",
    "        out = out.reshape(out.size()[0] * out.size()[1], self.n_hidden)\n",
    "\n",
    "\n",
    "        # Put x through the fully-connected layer\n",
    "        ## YOUR CODE HERE\n",
    "        out = self.linear(out)\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "6h3NSG5kGIuo"
   },
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                now = datetime.now()\n",
    "                dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)),\n",
    "                      \"Time: \", dt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dpIky4vhGI0X",
    "outputId": "f96d018a-f046-470d-c23c-dbea3b5581ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(78, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (linear): Linear(in_features=512, out_features=78, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 512 \n",
    "n_layers = 2\n",
    "\n",
    "chars = tuple(set(text))\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "pUQx6NIPSQza"
   },
   "outputs": [],
   "source": [
    "with open('/content/drive/MyDrive/2022-01-15_Course_project/LSTM_generator_it/dict_chars.pickle', 'wb') as f:\n",
    "    pickle.dump(chars, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "dNxzMAoDULi9"
   },
   "outputs": [],
   "source": [
    "with open('/content/drive/MyDrive/2022-01-15_Course_project/LSTM_generator_it/dict_chars.pickle', 'rb') as f:\n",
    "  c = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fH3lwyzAULlc",
    "outputId": "88a89bda-df83-48ac-c465-d0e8c13ed99c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 'G', ',', 'H', 'o', 'm', '[', 'i', 'N', '-', '(', 'a', 'R', 'M', 'e', 'h', 'B', 'b', '\\n', 'x', '?', '1', 'X', ';', 'V', '9', \"'\", 'r', '8', '4', 's', 'w', ':', 'v', 'j', 'O', 'c', '/', 'z', 't', 'L', 'l', 'd', 'F', 'U', 'J', 'S', 'g', 'P', '*', '0', ')', 'n', 'q', 'p', 'D', 'u', 'Q', 'I', 'Z', 'E', 'C', '_', '.', 'k', '6', ' ', '5', 'T', 'A', 'K', '3', 'y', ']', 'Y', 'f', '7', '2')\n"
     ]
    }
   ],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6UA8rn_7GI7j",
    "outputId": "3cc80f45-b439-49b4-8533-9dfdb3ee20f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/80... Step: 500... Loss: 1.9112... Val Loss: 1.8159 Time:  18/01/2022 11:23:49\n",
      "Epoch: 12/80... Step: 1000... Loss: 1.5220... Val Loss: 1.4529 Time:  18/01/2022 11:25:24\n",
      "Epoch: 18/80... Step: 1500... Loss: 1.3952... Val Loss: 1.3166 Time:  18/01/2022 11:26:57\n",
      "Epoch: 24/80... Step: 2000... Loss: 1.2989... Val Loss: 1.2521 Time:  18/01/2022 11:28:30\n",
      "Epoch: 30/80... Step: 2500... Loss: 1.1976... Val Loss: 1.2138 Time:  18/01/2022 11:30:04\n",
      "Epoch: 36/80... Step: 3000... Loss: 1.1694... Val Loss: 1.1918 Time:  18/01/2022 11:31:36\n",
      "Epoch: 42/80... Step: 3500... Loss: 1.1567... Val Loss: 1.1766 Time:  18/01/2022 11:33:09\n",
      "Epoch: 48/80... Step: 4000... Loss: 1.1256... Val Loss: 1.1676 Time:  18/01/2022 11:34:41\n",
      "Epoch: 53/80... Step: 4500... Loss: 1.1147... Val Loss: 1.1641 Time:  18/01/2022 11:36:15\n",
      "Epoch: 59/80... Step: 5000... Loss: 1.0794... Val Loss: 1.1628 Time:  18/01/2022 11:37:48\n",
      "Epoch: 65/80... Step: 5500... Loss: 1.0668... Val Loss: 1.1712 Time:  18/01/2022 11:39:22\n",
      "Epoch: 71/80... Step: 6000... Loss: 1.0675... Val Loss: 1.1701 Time:  18/01/2022 11:40:56\n",
      "Epoch: 77/80... Step: 6500... Loss: 1.0544... Val Loss: 1.1692 Time:  18/01/2022 11:42:29\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "seq_length = 100 \n",
    "n_epochs = 80\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "oPfr5ScHHndn"
   },
   "outputs": [],
   "source": [
    "path = \"/content/drive/MyDrive/2022-01-15_Course_project/LSTM_generator_it/text_generator.pt\" \n",
    "torch.save(net.state_dict(), path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R6SnXShsKOHh",
    "outputId": "f4cc9899-581e-44b5-a8a1-22d1733e248d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/content/drive/MyDrive/2022-01-15_Course_project/LSTM_generator_it/text_generator.pt\" \n",
    "\n",
    "\n",
    "with open('/content/drive/MyDrive/2022-01-15_Course_project/LSTM_generator_it/dict_chars.pickle', 'rb') as f:\n",
    "    chars_2 = pickle.load(f)\n",
    "\n",
    "n_hidden = 512  \n",
    "n_layers = 2\n",
    "net_2 = CharRNN(chars_2, n_hidden, n_layers)\n",
    "net_2.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m9Cq8T_XPkD-",
    "outputId": "10955156-163f-4de4-da7d-aa3976a0a3f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharRNN(\n",
       "  (lstm): LSTM(78, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (linear): Linear(in_features=512, out_features=78, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "uex89qSKP-v_"
   },
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "RnK_hi4vQAEq"
   },
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1qcw9eH1ecvU",
    "outputId": "70e7e144-f139-4ca9-8324-e328b9e95426"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ora passa il carattere precedente e prendine uno nuovo alla pace: il che resto condonuto la\n",
      "sua consunta contra de' Veneziani, e volesse essere\n",
      "in terra possona di quelle.\n",
      "E ricupero _Giovanni Maria della\n",
      "Navarro_, e di vigor della lega e dilotta tenuta de' Bisogna: santo.\n",
      "Trovavasi a Firenze un principe, poco poita da _Filippo\n",
      "III_ suo fratello _Carlo VIII_ re di Spagna, il ritirarsi soccorso di quelle nozze, e diedero\n",
      "limesi di chiedere\n",
      "un vicile ostia che notava ai capitani del papa rimadessero i suoi castidoli d'armata. Cola fu da meno o fra\n",
      "gl'imperiali, ebbe in procrame alcuna di vederse non\n",
      "poche nemiche sfagni delle lor due glorie attribuito a quella parte, convenne compartrone di pace. Negli Svizzeri era sicuro\n",
      "dalle minaccie di lunghive,\n",
      "e presi se non quasi tutti\n",
      "ogni di fine.\n",
      "\n",
      "Sorse la pupa il papa, a guerra agl'interessi, e ne i suoi Franzesi e dello stesso novello\n",
      "concelto per la repubblica.\n",
      "Passarono quei di esso Francesco s'oggide ricorsa quella del suo armamento, col maggior pomba piu di\n",
      "alcune\n",
      "clemenza che dacche fu ultima presidiata contro i corsari p!po nel mese di maggio. Scrive\n",
      "poscia una nuova sua brovi gia vergogna\n",
      "delle collera in\n",
      "quel sue\n",
      "commendi il governo. Cesso di solo fatto con chi avea offesa sala per\n",
      "la miglia dispracio ed ottomento, il _cardinale\n",
      "Clemente_, e al _duca d'Urbino_ a\n",
      "mitrere altri legni per non aver fatto i nemici doglianze da _Cosimo de Medici_, per sostenere quel ministero. Anche per qualche acquisto fin questo favorevole prosperita di pregare il gastigo de' Veneziani. Quella di Cremona, a piedo ranno a\n",
      "questo al re nemico, e a desiderare quel delitto trovarono restate piu convenevol vicinanze per\n",
      "aspettarne\n",
      "nelle sue mali. Trovo sul fin contro lo stesso\n",
      "suo figlio\n",
      "ora tutto\n",
      "padro\n",
      "a farsi servire alla cavalleria cavalleria. L'imperadore i Franzesi galee in Venezia due figliuoli pretesti dalla santa Savoia e il cardinale, unio della mala comandata,\n",
      "e rivolsero\n",
      "anche la pace. Si trovo lo stesso pontefice in quel\n",
      "feroce conto suo protettore, e dopo aver pochi colta.\n",
      "\n",
      "Trovo questa lagria del suo zio, con\n",
      "lui anche molti del\n",
      "suo figlio, dopo\n",
      "avere ucciso la fabbrica di quella\n",
      "parta,\n",
      "cioe\n",
      "Alessandro Caraffa, ogni muno, che\n",
      "la viena di guerra, si abitava perche tutto di sangue di\n",
      "parte; o _Giovanni Barbaro_ alla guasnato di Aversa, Cesare, e i Franzesi sfecerata la sentenza, senza osservar guerra al\n",
      "sacro forte della Francia, ed anche di _Pio V_ di altri nobili\n",
      "regni. Per ricuperare il _re Costono_ nulla manco che il _principe Alessandro Farnese_ ora cristiano\n",
      "venita l'utale ed egli pure tosto esestato in piegi e fanti visitati dell'impresa di\n",
      "Brescia. Se la prolo destinato principe, con obbligarsi in essa anche al grido messo. In questi tempi dentro nella\n",
      "corte di Napoli,\n",
      "perche campanto\n",
      "si tratteneva la Chiesa d'Italia, antenova di sempre, se\n",
      "giunto si conchiuse, ancorche il non\n",
      "modero miseramente proseguita da cio che se ne torno per li cautoli di _Locedico, Sostando_ e il Giustinio, che si avviso di\n",
      "dar quella gente, e colle leggi i capitani del suddetto duca di Savoia. O\n",
      "che fatto il campo, di credere, di piu; laonde nel di 30 di\n",
      "eta di azzo dell'anno presente\n",
      "questo si puo siffatte due! d'artiglieria, attaccarono montante che di far\n",
      "santo veneto, per farla spaventarsi e trovatoli ero in decredito.\n",
      "\n",
      "Toccato ai\n",
      "prantita di forze esso Gragidino, con tante difese turchesche andasse la fortuna di vinistra, con impedir che padrone di Roma, a ricevere in suo passaggio arrivo, con\n",
      "petto di gente abbandonarono il re di Spagna, ma presa di non sapere dassi piu molto prigione,\n",
      "ma amaro solamente di tutti gli storici, quando\n",
      "quanto bastione mostro piu altro nei contribuzzi vi mettevoli e disune sacrologe senza li soccorsi da quel solo seco da lui. Non tira una sua frena non essendo fuggente\n",
      "le bombarde fu altre degne di Venezia; a spargere ai primi aggiugni col duca di\n",
      "Mantova, e\n",
      "tal loto con pur passare senza bene provvisione di varii glai sudditi. Torno a pavi nelle\n",
      "spese di quelle persone, che vi sarebbe era cattino in paraccamento di passare\n",
      "in questi contorni, giacche il _conte di Fannasso Emina_, giorne seguito per la di lui preda e\n",
      "zabile. A cui fu assai la rapi la traflia. Si diede di solqaede (fatta, che da li\n",
      "a\n",
      "contrargia a reprimere l'imperadore a _don Ferdinando_, attese a topare alle porte di _Pietro, Gian-Francesco Maria_ pretendio all'altro di Andrea. Si menirono lo spirito di un corpo di sua truppe, e si ridusse a\n",
      "Mantova; ma il duca di fare a rivoltar la spedizione della repubblica veneta all'assalto del\n",
      "papa stesso. Quanto pote con gl'imperiali gia gli olore in male, considerando i popoli fenti dei Veneziani. Rinfrescava il fucor\n",
      "verso _Pietro Nivare_, i re di\n",
      "Spagna ch'egli si fierissima riputazione desiderata ne' cappelli agostitatori. Prese fu camicata la protizione di Citta: Casiel vi pote far voglia di salvarsi per capralle, che fatta la difesa, atterrita per le lunghie\n",
      "porto\n",
      "alla ricchezza, avea ebbe piu tecuti che sure tolte per disegno d'intendere, si manco che\n",
      "nel di 4\n",
      "di luglio d'industria, e maggior\n",
      "pripage il pontefice di dar comparire\n",
      "ai patti d'allora, e restitui quella citta il senato fautose, da questa tragianca le pentenze del re Arrigo re re alcune castella milizia, ora gran copia di secolare stato promettere in provveduta,\n",
      "per attestato\n",
      "del marchese di\n",
      "Mantova, che sommamente accompagnamento ed amatime del vescovato, costrinsero alla rubitante milizie suddetti. Contanto il papa per poco venire a San Pietro, la cui\n",
      "disovera s'arracchevol'e donna\n",
      "il di potea furor piu del re Ferdinando, che allora divenne riconobbe il prociso delle citta condussero\n",
      "a mancanza di fanteria all'accordo veneto, per cagion de' quali continuato brucio verso il suo servigio contra del Collalto, e gli\n",
      "sperava che l'altra maggioropo brigate, se\n",
      "piu cano, venivano dapribliata la pelquenza si aboli della citta. Parlio il duca di Mantova _Gian-Pietro_ sorella del _duca d'Urbino_, dove intervenne in quel bollore che faceano orgribil difesa, per cui era a sollacar prima. Morto per l'affaro, con pubblica sua zelo si negunarono con sangue ricaverne che la propria casa.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    Anno di CRISTO MDXIII. Indizione XI.\n",
      "\n",
      "    PIO IV papa 1.\n",
      "    RODOLFO II imperadore 31.\n",
      "\n",
      "\n",
      "Con questa torre\n",
      "colla sua gente faceano quella della Pivilla e di altri due mila fanti, sapendo egli non veniva\n",
      "proclamato re Francesco, avea precato\n",
      "d'essa citta\n",
      "di speranza delle cagioni franzesi, durava la ripisazion d'interporizaone. Ma il Girolamo guerra al campo grosso da un imperadore, dove circa quattro terrido, messi in adio seguitava a\n",
      "introderre. Allorche fu che\n",
      "il papa\n",
      "non volto il dominio di Castro, e ch'era di non poter forzare effocantavano cola sua piissima battaglia, dopo aver fia\n",
      "che si trovasse in fortezza il disegno di savere facilita la vita. Essendo sempre alle cure del re Grigioni,\n",
      "cominciarono molti soldati stati coi piu dell'Italia, mise salta delle sue nazioni malittizio, ma senza gran disnoligio. Dove era il _cardinale di Priolo_ in\n",
      "Italia, ed ebbe anche la\n",
      "principessa\n",
      "capitale di _Federigo duca_ di Urbino e dal capitan generale, come fu succeto, che sotto un'armata impresa di gran numero ce' quasi piu in quei capitani, ma egli intorno allo Stato epplension d'essa vigalesta contra di\n",
      "lui; e in cui riampiro l'armeta e battaglia, circa la testa si dichiarava colle condul sortate: il che e fuor di vista. Non impediano ne' vecchi facolo appoco che il suo\n",
      "principe\n",
      "concirta\n",
      "ben ammissata gran dissipulazione.\n",
      "Invio,\n",
      "_Giulio Sarri_ conte\n",
      "d'Urbino si\n",
      "trasse dietro, ebbe nel di 16 di maggio.\n",
      "Intanto, segretamente trovandosi al servigio dell'insegnamento suo. Nepur non inviarono quella tragedia, furono uccisi il _pontefice Storio_\n",
      "l'utalo invento de' Franzesi, e timor della niversi di essa il giovinetto,\n",
      "ricevette all'anno 1705, co veniva a Mondalfo all'autore della Chiesa sua piu comparsa di\n",
      "valore. Riusci verso di Francia in Venezia\n",
      "per la dieti di Torino, con amore della colte anche dell'imperio, costato o diro\n",
      "a schiavere con\n",
      "promessa dalla peranche di quella delle\n",
      "case contro i pacen\n"
     ]
    }
   ],
   "source": [
    "# Example of generated text\n",
    "print(sample(net_2, 8000, prime='Ora passa il carattere precedente e prendine uno nuovo', top_k=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "fU3wXKAJir0F"
   },
   "outputs": [],
   "source": [
    "text_ph = text.split('. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "v_cULwHCL5WZ"
   },
   "outputs": [],
   "source": [
    "list_b1 = [ i.split(' ')[0].lower() for i in text_ph]\n",
    "list_b2 = [ ' '.join(j.lower() for j in i.split(' ')[: 2]) for i in text_ph]\n",
    "list_b3 = [ ' '.join(j.lower() for j in i.split(' ')[: 3]) for i in text_ph]\n",
    "list_b4 = [ ' '.join(j.lower() for j in i.split(' ')[: 4]) for i in text_ph]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jM_fi5jCnISh",
    "outputId": "4ea64695-a485-423c-d644-e9d55fd17b92"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ma', 566),\n",
       " ('il', 283),\n",
       " ('non', 267),\n",
       " ('fu', 251),\n",
       " ('per', 230),\n",
       " ('in', 212),\n",
       " ('e', 210),\n",
       " ('si', 206),\n",
       " ('di', 172),\n",
       " ('nel', 161)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(list_b1).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Vsg1vD0jm3Kf"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "6s0PlDHjkN6n"
   },
   "outputs": [],
   "source": [
    "list_of_begins = set()\n",
    "for i in Counter(list_b1).most_common(10):\n",
    "  list_of_begins.add(re.sub(\"[^A-Za-z] \", \"\", i[0]).capitalize())\n",
    "\n",
    "for i in Counter(list_b2).most_common(10):\n",
    "  list_of_begins.add(re.sub(\"[^A-Za-z] \", \"\", i[0]).capitalize())\n",
    "\n",
    "for i in Counter(list_b3).most_common(10):\n",
    "  list_of_begins.add(re.sub(\"[^A-Za-z] \", \"\", i[0]).capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O36e-_TSnB4D",
    "outputId": "b95c80ba-c073-4c05-db2f-ff98351223b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Il',\n",
       " 'In',\n",
       " 'Ma',\n",
       " 'Indiz',\n",
       " 'Ma non',\n",
       " 'Ma il',\n",
       " 'Non si',\n",
       " '2rer',\n",
       " 'Iitom',\n",
       " 'Venet.tom',\n",
       " 'Nel di',\n",
       " 'Non',\n",
       " 'Fu',\n",
       " 'Ne si dee',\n",
       " 'Si',\n",
       " 'Di',\n",
       " 'A questo',\n",
       " 'Rer',\n",
       " 'Per',\n",
       " 'Ital',\n",
       " 'E',\n",
       " 'Nel']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(list_of_begins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Pad0hALYpnUB"
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ITJIToF2pLUC",
    "outputId": "1ce27d6a-ff6a-4f47-d0fe-e814b3f7f480"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/01/2022 11:45:42 |  number of processed files: 0 0.0 %\n",
      "18/01/2022 11:59:33 |  number of processed files: 200 1.3 %\n",
      "18/01/2022 12:13:32 |  number of processed files: 400 2.7 %\n",
      "18/01/2022 12:27:23 |  number of processed files: 600 4.0 %\n",
      "18/01/2022 12:41:12 |  number of processed files: 800 5.3 %\n",
      "18/01/2022 12:55:07 |  number of processed files: 1000 6.7 %\n",
      "18/01/2022 13:09:01 |  number of processed files: 1200 8.0 %\n",
      "18/01/2022 13:22:55 |  number of processed files: 1400 9.3 %\n",
      "18/01/2022 13:36:54 |  number of processed files: 1600 10.7 %\n",
      "18/01/2022 13:51:00 |  number of processed files: 1800 12.0 %\n",
      "18/01/2022 14:05:00 |  number of processed files: 2000 13.3 %\n",
      "18/01/2022 14:18:56 |  number of processed files: 2200 14.7 %\n",
      "18/01/2022 14:32:53 |  number of processed files: 2400 16.0 %\n",
      "18/01/2022 14:46:45 |  number of processed files: 2600 17.299999999999997 %\n",
      "18/01/2022 15:00:36 |  number of processed files: 2800 18.7 %\n",
      "18/01/2022 15:14:29 |  number of processed files: 3000 20.0 %\n",
      "18/01/2022 15:28:27 |  number of processed files: 3200 21.3 %\n",
      "18/01/2022 15:42:19 |  number of processed files: 3400 22.7 %\n",
      "18/01/2022 15:56:12 |  number of processed files: 3600 24.0 %\n",
      "18/01/2022 16:10:07 |  number of processed files: 3800 25.3 %\n",
      "18/01/2022 16:24:06 |  number of processed files: 4000 26.700000000000003 %\n",
      "18/01/2022 16:38:05 |  number of processed files: 4200 28.000000000000004 %\n",
      "18/01/2022 16:51:58 |  number of processed files: 4400 29.299999999999997 %\n",
      "18/01/2022 17:05:53 |  number of processed files: 4600 30.7 %\n",
      "18/01/2022 17:19:48 |  number of processed files: 4800 32.0 %\n",
      "18/01/2022 17:33:44 |  number of processed files: 5000 33.300000000000004 %\n",
      "18/01/2022 17:47:39 |  number of processed files: 5200 34.699999999999996 %\n",
      "18/01/2022 18:01:36 |  number of processed files: 5400 36.0 %\n",
      "18/01/2022 18:15:28 |  number of processed files: 5600 37.3 %\n",
      "18/01/2022 18:29:19 |  number of processed files: 5800 38.7 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_docs = 15000\n",
    "\n",
    "path = '/content/drive/MyDrive/2022-01-15_Course_project/gen_text_it/'\n",
    "\n",
    "for i in range(num_docs):\n",
    "  start = random.choice(list(list_of_begins))\n",
    "  text_gen = sample(net, 8000, prime= start, top_k=10)\n",
    "\n",
    "  with open(path + 'doc_'+str(i)+'.txt', 'w+') as output_file:\n",
    "    try:\n",
    "      output_file.write(text_gen)\n",
    "    except:\n",
    "      continue\n",
    "  if i % 200 == 0:\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    print(dt_string, '| ',  'number of processed files: ' + str(i), round(i/num_docs, 3)*100, '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FNPcLUAhspTR",
    "outputId": "366d3361-76c2-49b6-8f97-831fa065672b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5855, 0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If the generator does not work completely, you need to run this code (it will start from the last place)\n",
    "\n",
    "# This is the code to generate (to determine how many have already been generated)\n",
    "\n",
    "list_gen = glob.glob('/content/drive/MyDrive/2022-01-15_Course_project/gen_text_it/*.txt')\n",
    "list_num = [ int(i.split('/')[-1].split('.txt')[0].split('doc_')[-1]) for i in list_gen]\n",
    "max(list_num), min(list_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FLjqrUaWAt_G",
    "outputId": "79fcb7bd-f559-4877-8516-0aecbc37ee6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/01/2022 18:50:38 |  number of processed files: 6000 40.0 %\n",
      "18/01/2022 19:04:34 |  number of processed files: 6200 41.3 %\n",
      "18/01/2022 19:18:30 |  number of processed files: 6400 42.699999999999996 %\n",
      "18/01/2022 19:32:28 |  number of processed files: 6600 44.0 %\n",
      "18/01/2022 19:46:28 |  number of processed files: 6800 45.300000000000004 %\n",
      "18/01/2022 20:00:30 |  number of processed files: 7000 46.7 %\n",
      "18/01/2022 20:14:37 |  number of processed files: 7200 48.0 %\n",
      "18/01/2022 20:28:42 |  number of processed files: 7400 49.3 %\n",
      "18/01/2022 20:42:43 |  number of processed files: 7600 50.7 %\n",
      "18/01/2022 20:56:43 |  number of processed files: 7800 52.0 %\n",
      "18/01/2022 21:10:41 |  number of processed files: 8000 53.300000000000004 %\n",
      "18/01/2022 21:24:38 |  number of processed files: 8200 54.7 %\n",
      "18/01/2022 21:38:34 |  number of processed files: 8400 56.00000000000001 %\n",
      "18/01/2022 21:52:33 |  number of processed files: 8600 57.3 %\n",
      "18/01/2022 22:06:32 |  number of processed files: 8800 58.699999999999996 %\n",
      "18/01/2022 22:20:34 |  number of processed files: 9000 60.0 %\n",
      "18/01/2022 22:34:32 |  number of processed files: 9200 61.3 %\n",
      "18/01/2022 22:48:30 |  number of processed files: 9400 62.7 %\n",
      "18/01/2022 23:02:32 |  number of processed files: 9600 64.0 %\n",
      "18/01/2022 23:16:43 |  number of processed files: 9800 65.3 %\n",
      "18/01/2022 23:30:51 |  number of processed files: 10000 66.7 %\n",
      "18/01/2022 23:45:00 |  number of processed files: 10200 68.0 %\n",
      "18/01/2022 23:59:11 |  number of processed files: 10400 69.3 %\n",
      "19/01/2022 00:13:27 |  number of processed files: 10600 70.7 %\n",
      "19/01/2022 00:27:35 |  number of processed files: 10800 72.0 %\n",
      "19/01/2022 00:41:48 |  number of processed files: 11000 73.3 %\n",
      "19/01/2022 00:55:52 |  number of processed files: 11200 74.7 %\n",
      "19/01/2022 01:09:49 |  number of processed files: 11400 76.0 %\n",
      "19/01/2022 01:23:46 |  number of processed files: 11600 77.3 %\n",
      "19/01/2022 01:37:45 |  number of processed files: 11800 78.7 %\n",
      "19/01/2022 01:52:02 |  number of processed files: 12000 80.0 %\n",
      "19/01/2022 02:06:16 |  number of processed files: 12200 81.3 %\n",
      "19/01/2022 02:20:16 |  number of processed files: 12400 82.69999999999999 %\n",
      "19/01/2022 02:34:15 |  number of processed files: 12600 84.0 %\n",
      "19/01/2022 02:48:17 |  number of processed files: 12800 85.3 %\n",
      "19/01/2022 03:02:20 |  number of processed files: 13000 86.7 %\n",
      "19/01/2022 03:16:18 |  number of processed files: 13200 88.0 %\n",
      "19/01/2022 03:30:19 |  number of processed files: 13400 89.3 %\n",
      "19/01/2022 03:44:21 |  number of processed files: 13600 90.7 %\n",
      "19/01/2022 03:58:23 |  number of processed files: 13800 92.0 %\n",
      "19/01/2022 04:12:23 |  number of processed files: 14000 93.30000000000001 %\n",
      "19/01/2022 04:26:22 |  number of processed files: 14200 94.69999999999999 %\n",
      "19/01/2022 04:40:20 |  number of processed files: 14400 96.0 %\n",
      "19/01/2022 04:54:20 |  number of processed files: 14600 97.3 %\n",
      "19/01/2022 05:08:22 |  number of processed files: 14800 98.7 %\n"
     ]
    }
   ],
   "source": [
    "# If the generator does not work completely, you need to run this code (it will start from the last place)\n",
    "\n",
    "num_docs = 15000\n",
    "\n",
    "path = '/content/drive/MyDrive/2022-01-15_Course_project/gen_text_it/'\n",
    "\n",
    "for i in range(max(list_num), num_docs, 1):\n",
    "  start = random.choice(list(list_of_begins))\n",
    "  text_gen = sample(net_2, 8000, prime= start, top_k=10)\n",
    "\n",
    "  with open(path + 'doc_'+str(i)+'.txt', 'w+') as output_file:\n",
    "    try:\n",
    "      output_file.write(text_gen)\n",
    "    except:\n",
    "      continue\n",
    "  if i % 200 == 0:\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    print(dt_string, '| ',  'number of processed files: ' + str(i), round(i/num_docs, 3)*100, '%')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Generation_it.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
