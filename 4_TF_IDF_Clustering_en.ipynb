{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cdb50c2b"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6120d187",
    "outputId": "7cd24366-b4ef-4941-9efb-0a489abe048c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse.linalg import svds\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WRt8kPDwn-_b",
    "outputId": "221f263d-0c3e-40d2-ff4d-9093fa2d8f97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3bd2f029"
   },
   "outputs": [],
   "source": [
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5a7425c8",
    "outputId": "12c1dbd0-baf8-4d70-adba-b08787377789"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MaplzLoMnLuZ"
   },
   "outputs": [],
   "source": [
    "# Wishart clustering function\n",
    "# https://github.com/Radi4/BotDetection/blob/master/Wishart.py\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import gamma\n",
    "from sklearn.neighbors import KDTree\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Wishart:\n",
    "    def __init__(self, wishart_neighbors, significance_level):\n",
    "        self.wishart_neighbors = wishart_neighbors  # Number of neighbors\n",
    "        self.significance_level = significance_level  # Significance level\n",
    "\n",
    "    def fit(self, X):\n",
    "        from sklearn.neighbors import KDTree\n",
    "        kdt = KDTree(X, metric='euclidean')\n",
    "\n",
    "        #add one because you are your neighb.\n",
    "        distances, neighbors = kdt.query(X, k = self.wishart_neighbors + 1, return_distance = True)\n",
    "        neighbors = neighbors[:, 1:]\n",
    "\n",
    "\n",
    "        distances = distances[:, -1]\n",
    "        indexes = np.argsort(distances)\n",
    "        \n",
    "        size, dim = X.shape\n",
    "\n",
    "        self.object_labels = np.zeros(size, dtype = int) - 1\n",
    "\n",
    "        #index in tuple\n",
    "        #min_dist, max_dist, flag_to_significant\n",
    "        self.clusters = np.array([(1., 1., 0)])\n",
    "        self.clusters_to_objects = defaultdict(list)\n",
    "        print('Start clustering')\n",
    "\n",
    "        for index in indexes:\n",
    "            neighbors_clusters =\\\n",
    "                np.concatenate([self.object_labels[neighbors[index]], self.object_labels[neighbors[index]]])\n",
    "            unique_clusters = np.unique(neighbors_clusters).astype(int)\n",
    "            unique_clusters = unique_clusters[unique_clusters != -1]\n",
    "\n",
    "\n",
    "            if len(unique_clusters) == 0:\n",
    "                self._create_new_cluster(index, distances[index])\n",
    "            else:\n",
    "                max_cluster = unique_clusters[-1]\n",
    "                min_cluster = unique_clusters[0]\n",
    "                if max_cluster == min_cluster:\n",
    "                    if self.clusters[max_cluster][-1] < 0.5:\n",
    "                        self._add_elem_to_exist_cluster(index, distances[index], max_cluster)\n",
    "                    else:\n",
    "                        self._add_elem_to_noise(index)\n",
    "                else:\n",
    "                    my_clusters = self.clusters[unique_clusters]\n",
    "                    flags = my_clusters[:, -1]\n",
    "                    if np.min(flags) > 0.5:\n",
    "                        self._add_elem_to_noise(index)\n",
    "                    else:\n",
    "                        significan = np.power(my_clusters[:, 0], -dim) - np.power(my_clusters[:, 1], -dim)\n",
    "                        significan *= self.wishart_neighbors\n",
    "                        significan /= size\n",
    "                        significan /= np.power(np.pi, dim / 2)\n",
    "                        significan *= gamma(dim / 2 + 1)\n",
    "                        significan_index = significan >= self.significance_level\n",
    "\n",
    "                        significan_clusters = unique_clusters[significan_index]\n",
    "                        not_significan_clusters = unique_clusters[~significan_index]\n",
    "                        significan_clusters_count = len(significan_clusters)\n",
    "                        if significan_clusters_count > 1 or min_cluster == 0:\n",
    "                            self._add_elem_to_noise(index)\n",
    "                            self.clusters[significan_clusters, -1] = 1\n",
    "                            for not_sig_cluster in not_significan_clusters:\n",
    "                                if not_sig_cluster == 0:\n",
    "                                    continue\n",
    "\n",
    "                                for bad_index in self.clusters_to_objects[not_sig_cluster]:\n",
    "                                    self._add_elem_to_noise(bad_index)\n",
    "                                self.clusters_to_objects[not_sig_cluster].clear()\n",
    "                        else:\n",
    "                            for cur_cluster in unique_clusters:\n",
    "                                if cur_cluster == min_cluster:\n",
    "                                    continue\n",
    "\n",
    "                                for bad_index in self.clusters_to_objects[cur_cluster]:\n",
    "                                    self._add_elem_to_exist_cluster(bad_index, distances[bad_index], min_cluster)\n",
    "                                self.clusters_to_objects[cur_cluster].clear()\n",
    "\n",
    "                            self._add_elem_to_exist_cluster(index, distances[index], min_cluster)\n",
    "\n",
    "        return self.clean_data()\n",
    "\n",
    "    def clean_data(self):\n",
    "        unique = np.unique(self.object_labels)\n",
    "        index = np.argsort(unique)\n",
    "        if unique[0] != 0:\n",
    "            index += 1\n",
    "        true_cluster = {unq :  index for unq, index in zip(unique, index)}\n",
    "        result = np.zeros(len(self.object_labels), dtype = int)\n",
    "        for index, unq in enumerate(self.object_labels):\n",
    "            result[index] = true_cluster[unq]\n",
    "        return result\n",
    "\n",
    "    def _add_elem_to_noise(self, index):\n",
    "        self.object_labels[index] = 0\n",
    "        self.clusters_to_objects[0].append(index)\n",
    "\n",
    "    def _create_new_cluster(self, index, dist):\n",
    "        self.object_labels[index] = len(self.clusters)\n",
    "        self.clusters_to_objects[len(self.clusters)].append(index)\n",
    "        self.clusters = np.append(self.clusters, [(dist, dist, 0)], axis = 0)\n",
    "\n",
    "    def _add_elem_to_exist_cluster(self, index, dist, cluster_label):\n",
    "        self.object_labels[index] = cluster_label\n",
    "        self.clusters_to_objects[cluster_label].append(index)\n",
    "        self.clusters[cluster_label][0] = min(self.clusters[cluster_label][0], dist)\n",
    "        self.clusters[cluster_label][1] = max(self.clusters[cluster_label][1], dist)\n",
    "\n",
    "\n",
    "class PreTrainWishart:\n",
    "    def __init__(self, wishart_neighbors, significance_level, distances, neighbors):\n",
    "        self.wishart_neighbors = wishart_neighbors  # Number of neighbors\n",
    "        self.significance_level = significance_level  # Significance level\n",
    "        self.distances = distances\n",
    "        self.neighbors = neighbors\n",
    "\n",
    "    def fit(self, X):\n",
    "        from sklearn.neighbors import KDTree\n",
    "        kdt = KDTree(X, metric='euclidean')\n",
    "\n",
    "        #add one because you are your neighb.\n",
    "        neighbors = self.neighbors[:, 1 : self.wishart_neighbors + 1]\n",
    "        distances = self.distances[:, self.wishart_neighbors]\n",
    "        indexes = np.argsort(distances)\n",
    "        \n",
    "        size, dim = X.shape\n",
    "\n",
    "        self.object_labels = np.zeros(size, dtype = int) - 1\n",
    "\n",
    "        #index in tuple\n",
    "        #min_dist, max_dist, flag_to_significant\n",
    "        self.clusters = np.array([(1., 1., 0)])\n",
    "        self.clusters_to_objects = defaultdict(list)\n",
    "\n",
    "        for index in indexes:\n",
    "            neighbors_clusters =\\\n",
    "                np.concatenate([self.object_labels[neighbors[index]], self.object_labels[neighbors[index]]])\n",
    "            unique_clusters = np.unique(neighbors_clusters).astype(int)\n",
    "            unique_clusters = unique_clusters[unique_clusters != -1]\n",
    "\n",
    "\n",
    "            if len(unique_clusters) == 0:\n",
    "                self._create_new_cluster(index, distances[index])\n",
    "            else:\n",
    "                max_cluster = unique_clusters[-1]\n",
    "                min_cluster = unique_clusters[0]\n",
    "                if max_cluster == min_cluster:\n",
    "                    if self.clusters[max_cluster][-1] < 0.5:\n",
    "                        self._add_elem_to_exist_cluster(index, distances[index], max_cluster)\n",
    "                    else:\n",
    "                        self._add_elem_to_noise(index)\n",
    "                else:\n",
    "                    my_clusters = self.clusters[unique_clusters]\n",
    "                    flags = my_clusters[:, -1]\n",
    "                    if np.min(flags) > 0.5:\n",
    "                        self._add_elem_to_noise(index)\n",
    "                    else:\n",
    "                        significan = np.power(my_clusters[:, 0], -dim) - np.power(my_clusters[:, 1], -dim)\n",
    "                        significan *= self.wishart_neighbors\n",
    "                        significan /= size\n",
    "                        significan /= np.power(np.pi, dim / 2)\n",
    "                        significan *= gamma(dim / 2 + 1)\n",
    "                        significan_index = significan >= self.significance_level\n",
    "\n",
    "                        significan_clusters = unique_clusters[significan_index]\n",
    "                        not_significan_clusters = unique_clusters[~significan_index]\n",
    "                        significan_clusters_count = len(significan_clusters)\n",
    "                        if significan_clusters_count > 1 or min_cluster == 0:\n",
    "                            self._add_elem_to_noise(index)\n",
    "                            self.clusters[significan_clusters, -1] = 1\n",
    "                            for not_sig_cluster in not_significan_clusters:\n",
    "                                if not_sig_cluster == 0:\n",
    "                                    continue\n",
    "\n",
    "                                for bad_index in self.clusters_to_objects[not_sig_cluster]:\n",
    "                                    self._add_elem_to_noise(bad_index)\n",
    "                                self.clusters_to_objects[not_sig_cluster].clear()\n",
    "                        else:\n",
    "                            for cur_cluster in unique_clusters:\n",
    "                                if cur_cluster == min_cluster:\n",
    "                                    continue\n",
    "\n",
    "                                for bad_index in self.clusters_to_objects[cur_cluster]:\n",
    "                                    self._add_elem_to_exist_cluster(bad_index, distances[bad_index], min_cluster)\n",
    "                                self.clusters_to_objects[cur_cluster].clear()\n",
    "\n",
    "                            self._add_elem_to_exist_cluster(index, distances[index], min_cluster)\n",
    "\n",
    "        return self.clean_data()\n",
    "\n",
    "    def clean_data(self):\n",
    "        unique = np.unique(self.object_labels)\n",
    "        index = np.argsort(unique)\n",
    "        if unique[0] != 0:\n",
    "            index += 1\n",
    "        true_cluster = {unq :  index for unq, index in zip(unique, index)}\n",
    "        result = np.zeros(len(self.object_labels), dtype = int)\n",
    "        for index, unq in enumerate(self.object_labels):\n",
    "            result[index] = true_cluster[unq]\n",
    "        return result\n",
    "\n",
    "    def _add_elem_to_noise(self, index):\n",
    "        self.object_labels[index] = 0\n",
    "        self.clusters_to_objects[0].append(index)\n",
    "\n",
    "    def _create_new_cluster(self, index, dist):\n",
    "        self.object_labels[index] = len(self.clusters)\n",
    "        self.clusters_to_objects[len(self.clusters)].append(index)\n",
    "        self.clusters = np.append(self.clusters, [(dist, dist, 0)], axis = 0)\n",
    "\n",
    "    def _add_elem_to_exist_cluster(self, index, dist, cluster_label):\n",
    "        self.object_labels[index] = cluster_label\n",
    "        self.clusters_to_objects[cluster_label].append(index)\n",
    "        self.clusters[cluster_label][0] = min(self.clusters[cluster_label][0], dist)\n",
    "        self.clusters[cluster_label][1] = max(self.clusters[cluster_label][1], dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7f043e55"
   },
   "source": [
    "## Ð¡reate a vector representation based on TfidfVectorizer (on human texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7b1f3a37",
    "outputId": "4a0aa08f-fc1b-483e-f818-0f5e02a70cdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Stopwords for English\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4d3f9554"
   },
   "outputs": [],
   "source": [
    "def make_corpus(input_path, output_file_path):\n",
    "    i = 0\n",
    "    file_list = glob.glob(input_path + '*')\n",
    "    \n",
    "    with open(output_file_path, 'w+') as output_file:\n",
    "        for file in file_list:\n",
    "            if i % 500 == 0:\n",
    "                now = datetime.now()\n",
    "                dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "                print(dt_string, '| ',  'number of processed files: ' + str(i), '| ', \n",
    "                      'percentage of completion:', str(round(i/len(file_list), 2)* 100) + ' %' )\n",
    "            i+=1\n",
    "            with open(file, 'r') as input_file:\n",
    "                output_file.write(input_file.read().replace('\\n', ' '))\n",
    "                output_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "_bWXrYE-5MqU"
   },
   "outputs": [],
   "source": [
    "# Let's select 10k texts in a folder: '/content/drive/MyDrive/2022-01-15_Course_project/10000_cut_en/'\n",
    "# Because clustering works for a very long time on large datasets\n",
    "\n",
    "import shutil\n",
    "file_list = glob.glob('/content/drive/MyDrive/2022-01-15_Course_project/cut_en/*')\n",
    "k = 0\n",
    "\n",
    "for i in file_list:\n",
    "  shutil.copy(i, '/content/drive/MyDrive/2022-01-15_Course_project/10000_cut_en/')\n",
    "  k += 1\n",
    "  if k >= 10000:\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1N6ZypOS5M1_",
    "outputId": "3c4e8d3d-399a-4143-cd92-041641fd1a7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14788"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glob.glob('/content/drive/MyDrive/2022-01-15_Course_project/cut_en/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CgyB_rEs5M8e",
    "outputId": "32a66b3e-cd71-4be6-b8a9-8064f3031eb0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glob.glob('/content/drive/MyDrive/2022-01-15_Course_project/10000_cut_en/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "de124e1e",
    "outputId": "9de88723-175b-4a2e-b73e-6c6f8784178d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/01/2022 17:40:15 |  number of processed files: 0 |  percentage of completion: 0.0 %\n",
      "22/01/2022 17:43:39 |  number of processed files: 500 |  percentage of completion: 5.0 %\n",
      "22/01/2022 17:43:39 |  number of processed files: 1000 |  percentage of completion: 10.0 %\n",
      "22/01/2022 17:43:40 |  number of processed files: 1500 |  percentage of completion: 15.0 %\n",
      "22/01/2022 17:43:40 |  number of processed files: 2000 |  percentage of completion: 20.0 %\n",
      "22/01/2022 17:43:40 |  number of processed files: 2500 |  percentage of completion: 25.0 %\n",
      "22/01/2022 17:43:41 |  number of processed files: 3000 |  percentage of completion: 30.0 %\n",
      "22/01/2022 17:43:41 |  number of processed files: 3500 |  percentage of completion: 35.0 %\n",
      "22/01/2022 17:43:42 |  number of processed files: 4000 |  percentage of completion: 40.0 %\n",
      "22/01/2022 17:43:42 |  number of processed files: 4500 |  percentage of completion: 45.0 %\n",
      "22/01/2022 17:43:42 |  number of processed files: 5000 |  percentage of completion: 50.0 %\n",
      "22/01/2022 17:43:43 |  number of processed files: 5500 |  percentage of completion: 55.00000000000001 %\n",
      "22/01/2022 17:43:43 |  number of processed files: 6000 |  percentage of completion: 60.0 %\n",
      "22/01/2022 17:43:44 |  number of processed files: 6500 |  percentage of completion: 65.0 %\n",
      "22/01/2022 17:43:44 |  number of processed files: 7000 |  percentage of completion: 70.0 %\n",
      "22/01/2022 17:43:45 |  number of processed files: 7500 |  percentage of completion: 75.0 %\n",
      "22/01/2022 17:43:45 |  number of processed files: 8000 |  percentage of completion: 80.0 %\n",
      "22/01/2022 17:43:45 |  number of processed files: 8500 |  percentage of completion: 85.0 %\n",
      "22/01/2022 17:43:46 |  number of processed files: 9000 |  percentage of completion: 90.0 %\n",
      "22/01/2022 17:43:46 |  number of processed files: 9500 |  percentage of completion: 95.0 %\n"
     ]
    }
   ],
   "source": [
    "# Let's make corpus for human-texts\n",
    "\n",
    "make_corpus('/content/drive/MyDrive/2022-01-15_Course_project/10000_cut_en/', \n",
    "            '/content/drive/MyDrive/2022-01-15_Course_project/TF_IDF_clustering_en/10000_dataset_human_en.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "07c89c06"
   },
   "outputs": [],
   "source": [
    "# TF_IDF corpus\n",
    "\n",
    "def make_table_and_dict(corpus_path, min_df, max_df, token_pattern = None, use_idf = True, stop_words = 'english'):\n",
    "    \n",
    "    with open(corpus_path, 'r') as corpus_file:\n",
    "        if token_pattern:\n",
    "            vectorizer = TfidfVectorizer(analyzer='word', min_df=min_df, token_pattern=token_pattern, use_idf=use_idf)\n",
    "        else:\n",
    "            vectorizer = TfidfVectorizer(analyzer='word', min_df=min_df)\n",
    "        data_vectorized = vectorizer.fit_transform(corpus_file)\n",
    "    return data_vectorized, vectorizer.get_feature_names(), vectorizer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8f4de5bf"
   },
   "outputs": [],
   "source": [
    "def create_table(data_vectorized, k, name, path):\n",
    "    u, sigma, vt = svds(data_vectorized, k)\n",
    "    print(sigma)\n",
    "    dict_ = np.dot(np.diag(sigma), vt).T\n",
    "        \n",
    "    with open(path + name + str(k) + '.pkl', 'wb') as f:\n",
    "        pickle.dump(dict_, f)\n",
    "    return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "516f0a4f",
    "outputId": "2e4ef14f-381a-4ae4-c39e-6b0235edbac8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#create TF_IDF on human text\n",
    "en_data_vectorized, en_dictionary, idfs = make_table_and_dict('/content/drive/MyDrive/2022-01-15_Course_project/TF_IDF_clustering_en/10000_dataset_human_en.txt', \n",
    "                                                                0.05,  0.7 , token_pattern = '[A-Za-z]+', \n",
    "                                                                stop_words = stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51a7a097",
    "outputId": "4486c157-70ae-44c5-a169-fee86296ab19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.38702534  1.38991427  1.39921949  1.40407253  1.40515473  1.41273601\n",
      "  1.41685925  1.42255831  1.42512507  1.43312768  1.44899768  1.45360187\n",
      "  1.45775257  1.46675168  1.46970726  1.48272918  1.48541388  1.49475139\n",
      "  1.49852761  1.51090422  1.51314042  1.52491342  1.53397807  1.55123462\n",
      "  1.56130332  1.56708245  1.5771239   1.58572565  1.5987485   1.60246324\n",
      "  1.61639271  1.62697481  1.63935747  1.65498008  1.66925057  1.67444444\n",
      "  1.67578791  1.69653738  1.70503331  1.71010821  1.73648524  1.74197619\n",
      "  1.7527861   1.77473808  1.7914766   1.80187569  1.8174394   1.82934986\n",
      "  1.85557111  1.85997274  1.87772936  1.89107364  1.91973095  1.92822272\n",
      "  1.96137951  1.97213453  1.99313352  2.01216444  2.04662524  2.06448466\n",
      "  2.0765497   2.10583484  2.12314859  2.14921483  2.18436206  2.22654352\n",
      "  2.23527498  2.25754432  2.30579807  2.3501784   2.37064469  2.41853709\n",
      "  2.47262482  2.48654544  2.54329048  2.63465823  2.66614169  2.70165518\n",
      "  2.74540376  2.77135995  2.82950277  2.97132028  3.00108007  3.0729773\n",
      "  3.08286945  3.19274713  3.486825    3.64689297  3.7912048   3.92143645\n",
      "  4.21261306  4.96620235  5.32032881  6.60645906  6.98846821  7.3179772\n",
      "  7.76695303 16.10894158 24.73035392 87.88178316]\n"
     ]
    }
   ],
   "source": [
    "dict_ = create_table(en_data_vectorized, 100, '10000_SVD_human_en_', \n",
    "                     '/content/drive/MyDrive/2022-01-15_Course_project/TF_IDF_clustering_en/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "3d47f3a1"
   },
   "outputs": [],
   "source": [
    "pairs_0 = list(zip(idfs, dict_))\n",
    "pairs_idf = dict(zip(en_dictionary, pairs_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "cb22926f"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    file = open('/content/drive/MyDrive/2022-01-15_Course_project/TF_IDF_clustering_en/10000_TF_IDF_human_en.pkl', 'wb')\n",
    "    pickle.dump(pairs_idf, file)\n",
    "    file.close()\n",
    "except:\n",
    "    print(\"Something went wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "b4f4f187"
   },
   "outputs": [],
   "source": [
    "# Removing frequently used words\n",
    "dict_cut = dict()\n",
    "for w in pairs_idf.keys():\n",
    "    if pairs_idf[w][0] > 1.5:\n",
    "        dict_cut[w] = pairs_idf[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "15b2dcdb"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    file = open('/content/drive/MyDrive/2022-01-15_Course_project/TF_IDF_clustering_en/10000_TF_IDF_cut_human_en.pkl', 'wb')\n",
    "    pickle.dump(dict_cut, file)\n",
    "    file.close()\n",
    "except:\n",
    "    print(\"Something went wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ed1449ad",
    "outputId": "61f1c88b-f021-498c-d247-1e485a2964da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1607"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "895287d1",
    "outputId": "f1d91588-1766-484b-feda-8bf4fd6f8a04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1528"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_cut.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ce276f30"
   },
   "source": [
    "# Making n-grams and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "3edea2f9"
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from math import log\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics import calinski_harabasz_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "8b9b8248"
   },
   "outputs": [],
   "source": [
    "def divide(data, labels):\n",
    "    clusters = set(labels)\n",
    "    clusters_data = []\n",
    "    for cluster in clusters:\n",
    "        clusters_data.append(data[labels == cluster, :])\n",
    "    return clusters_data\n",
    "\n",
    "def get_centroids(clusters):\n",
    "    centroids = []\n",
    "    for cluster_data in clusters:\n",
    "        centroids.append(cluster_data.mean(axis=0))\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "9eb06fd2"
   },
   "outputs": [],
   "source": [
    "def cohesion(data, labels):\n",
    "    clusters = sorted(set(labels))\n",
    "    sse = 0\n",
    "    for cluster in clusters:\n",
    "        cluster_data = data[labels == cluster, :]\n",
    "        centroid = cluster_data.mean(axis = 0)\n",
    "        sse += ((cluster_data - centroid)**2).sum()\n",
    "    return sse\n",
    "\n",
    "def separation(data, labels, cohesion_score):\n",
    "    # calculate separation as SST - SSE\n",
    "    return cohesion(data, np.zeros(data.shape[0])) - cohesion_score\n",
    "\n",
    "def SST(data):\n",
    "    c = get_centroids([data])\n",
    "    return ((data - c) ** 2).sum()\n",
    "\n",
    "def SSE(clusters, centroids):\n",
    "    result = 0\n",
    "    for cluster, centroid in zip(clusters, centroids):\n",
    "        result += ((cluster - centroid) ** 2).sum()\n",
    "    return result\n",
    "\n",
    "# Clear the store before running each time\n",
    "within_cluster_dist_sum_store = {}\n",
    "def within_cluster_dist_sum(cluster, centroid, cluster_id):\n",
    "    if cluster_id in within_cluster_dist_sum_store:\n",
    "        return within_cluster_dist_sum_store[cluster_id]\n",
    "    else:\n",
    "        result = (((cluster - centroid) ** 2).sum(axis=1)**.5).sum()\n",
    "        within_cluster_dist_sum_store[cluster_id] = result\n",
    "    return result\n",
    "\n",
    "def RMSSTD(data, clusters, centroids):\n",
    "    df = data.shape[0] - len(clusters)\n",
    "    attribute_num = data.shape[1]\n",
    "    return (SSE(clusters, centroids) / (attribute_num * df)) ** .5\n",
    "\n",
    "# equal to separation / (cohesion + separation)\n",
    "def RS(data, clusters, centroids):\n",
    "    sst = SST(data)\n",
    "    sse = SSE(clusters, centroids)\n",
    "    return (sst - sse) / sst\n",
    "\n",
    "def DB_find_max_j(clusters, centroids, i):\n",
    "    max_val = 0\n",
    "    max_j = 0\n",
    "    for j in range(len(clusters)):\n",
    "        if j == i:\n",
    "            continue\n",
    "        cluster_i_stat = within_cluster_dist_sum(clusters[i], centroids[i], i) / clusters[i].shape[0]\n",
    "        cluster_j_stat = within_cluster_dist_sum(clusters[j], centroids[j], j) / clusters[j].shape[0]\n",
    "        val = (cluster_i_stat + cluster_j_stat) / (((centroids[i] - centroids[j]) ** 2).sum() ** .5)\n",
    "        if val > max_val:\n",
    "            max_val = val\n",
    "            max_j = j\n",
    "    return max_val\n",
    "\n",
    "def DB(data, clusters, centroids):\n",
    "    result = 0\n",
    "    for i in range(len(clusters)):\n",
    "        result += DB_find_max_j(clusters, centroids, i)\n",
    "    return result / len(clusters)\n",
    "\n",
    "def XB(data, clusters, centroids):\n",
    "    sse = SSE(clusters, centroids)\n",
    "    min_dist = ((centroids[0] - centroids[1]) ** 2).sum()\n",
    "    for centroid_i, centroid_j in list(product(centroids, centroids)):\n",
    "        if (centroid_i - centroid_j).sum() == 0:\n",
    "            continue\n",
    "        dist = ((centroid_i - centroid_j) ** 2).sum()\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "    return sse / (data.shape[0] * min_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "93e7bacc"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "# Some metrics can work for a very long time (commented out)\n",
    "\n",
    "def get_validation_scores(data, labels, max_clust = None):\n",
    "    #within_cluster_dist_sum_store.clear()\n",
    "    \n",
    "    clusters = divide(data, labels)\n",
    "    centroids = get_centroids(clusters)\n",
    "    \n",
    "    scores = {}\n",
    "    if max_clust:\n",
    "        if len(clusters) > max_clust:\n",
    "            scores['cohesion'] = cohesion(data, labels)\n",
    "            scores['separation'] = separation(data, labels, scores['cohesion'])\n",
    "            scores['calinski_harabaz_score'] = None\n",
    "            scores['RMSSTD'] = RMSSTD(data, clusters, centroids)\n",
    "            scores['RS'] = RS(data, clusters, centroids)\n",
    "            #scores['DB'] = None\n",
    "            #scores['XB'] = XB(data, clusters, centroids)\n",
    "            scores['silhouette'] = None\n",
    "        else:\n",
    "            scores['cohesion'] = cohesion(data, labels)\n",
    "            scores['separation'] = separation(data, labels, scores['cohesion'])\n",
    "            scores['calinski_harabaz_score'] = calinski_harabasz_score(data, labels)\n",
    "            scores['RMSSTD'] = RMSSTD(data, clusters, centroids)\n",
    "            scores['RS'] = RS(data, clusters, centroids)\n",
    "            #scores['DB'] = DB(data, clusters, centroids)\n",
    "            #scores['XB'] = XB(data, clusters, centroids)\n",
    "            scores['silhouette'] = silhouette_score(data, labels)\n",
    "    else:\n",
    "        scores['cohesion'] = cohesion(data, labels)\n",
    "        scores['separation'] = separation(data, labels, scores['cohesion'])\n",
    "        scores['calinski_harabaz_score'] = calinski_harabasz_score(data, labels)\n",
    "        scores['RMSSTD'] = RMSSTD(data, clusters, centroids)\n",
    "        scores['RS'] = RS(data, clusters, centroids)\n",
    "        #scores['DB'] = DB(data, clusters, centroids)\n",
    "        #scores['XB'] = XB(data, clusters, centroids)\n",
    "        scores['silhouette'] = silhouette_score(data, labels)\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "4a8e01ac"
   },
   "outputs": [],
   "source": [
    "def make_ngrams(input_corpus,  dict_, N = 2, m = None, uniq = False):\n",
    "    dict_grams = dict()\n",
    "    num_ = 0\n",
    "    i = 0\n",
    "    j = 0\n",
    "    \n",
    "    print('Count documents: ', len(input_corpus))\n",
    "    for sentence in input_corpus:\n",
    "        sentence = sentence.split(' ')\n",
    "        grams = [sentence[i:i+N] for i in range(len(sentence)-N+1)]\n",
    "        for g in grams:\n",
    "            g_key = '_'.join(elem for elem in g)\n",
    "\n",
    "            if uniq:\n",
    "                if all(elem in dict_.keys()  for elem in g) and (g_key not in dict_grams.keys()):\n",
    "                    dict_grams[g_key] = []\n",
    "                    for elem in g:\n",
    "                            if m:\n",
    "                                dict_grams[g_key] += list(dict_[elem][1][:m])\n",
    "                            else:\n",
    "                                dict_grams[g_key] += list(dict_[elem][1])\n",
    "            else:\n",
    "                if all(elem in dict_.keys()  for elem in g):\n",
    "                    concat = []\n",
    "                    for elem in g:\n",
    "                        if m:\n",
    "                            concat += list(dict_[elem][1][:m])\n",
    "                        else:\n",
    "                            concat += list(dict_[elem][1])\n",
    "                    dict_grams[i] = (j, g_key, concat)\n",
    "                    i += 1\n",
    "            j += 1\n",
    "       \n",
    "            \n",
    "        if num_ % 500 == 0:\n",
    "            now = datetime.now()\n",
    "            dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "            print(dt_string, '| ',  'number of processed documents: ' + str(num_), '| ', \n",
    "                      'percentage of completion:', str(round(num_/len(input_corpus), 2)* 100) + ' %' )\n",
    "        num_ += 1\n",
    "    return dict_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "8eae3315"
   },
   "outputs": [],
   "source": [
    "with open('/content/drive/MyDrive/2022-01-15_Course_project/TF_IDF_clustering_en/10000_dataset_human_en.txt', 'r') as corpus_file:\n",
    "    corpus = corpus_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3b29d21d",
    "outputId": "3329a211-62cd-4546-eba2-0e5882e147e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count documents:  10000\n",
      "22/01/2022 18:00:12 |  number of processed documents: 0 |  percentage of completion: 0.0 %\n",
      "22/01/2022 18:00:14 |  number of processed documents: 500 |  percentage of completion: 5.0 %\n",
      "22/01/2022 18:00:16 |  number of processed documents: 1000 |  percentage of completion: 10.0 %\n",
      "22/01/2022 18:00:18 |  number of processed documents: 1500 |  percentage of completion: 15.0 %\n",
      "22/01/2022 18:00:20 |  number of processed documents: 2000 |  percentage of completion: 20.0 %\n",
      "22/01/2022 18:00:22 |  number of processed documents: 2500 |  percentage of completion: 25.0 %\n",
      "22/01/2022 18:00:24 |  number of processed documents: 3000 |  percentage of completion: 30.0 %\n",
      "22/01/2022 18:00:26 |  number of processed documents: 3500 |  percentage of completion: 35.0 %\n",
      "22/01/2022 18:00:28 |  number of processed documents: 4000 |  percentage of completion: 40.0 %\n",
      "22/01/2022 18:00:30 |  number of processed documents: 4500 |  percentage of completion: 45.0 %\n",
      "22/01/2022 18:00:32 |  number of processed documents: 5000 |  percentage of completion: 50.0 %\n",
      "22/01/2022 18:00:34 |  number of processed documents: 5500 |  percentage of completion: 55.00000000000001 %\n",
      "22/01/2022 18:00:36 |  number of processed documents: 6000 |  percentage of completion: 60.0 %\n",
      "22/01/2022 18:00:38 |  number of processed documents: 6500 |  percentage of completion: 65.0 %\n",
      "22/01/2022 18:00:40 |  number of processed documents: 7000 |  percentage of completion: 70.0 %\n",
      "22/01/2022 18:00:42 |  number of processed documents: 7500 |  percentage of completion: 75.0 %\n",
      "22/01/2022 18:00:44 |  number of processed documents: 8000 |  percentage of completion: 80.0 %\n",
      "22/01/2022 18:00:45 |  number of processed documents: 8500 |  percentage of completion: 85.0 %\n",
      "22/01/2022 18:00:47 |  number of processed documents: 9000 |  percentage of completion: 90.0 %\n",
      "22/01/2022 18:00:49 |  number of processed documents: 9500 |  percentage of completion: 95.0 %\n"
     ]
    }
   ],
   "source": [
    "dict_grams_human = make_ngrams(corpus,  dict_cut, N = 2, m = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "20e4a2cd",
    "outputId": "fb0f32ca-5951-4bfd-cb38-b44ac3d856bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "538810"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_grams_human.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "27febfd3"
   },
   "outputs": [],
   "source": [
    "X0 = []\n",
    "for i in dict_grams_human.keys():\n",
    "    X0.append( dict_grams_human[i][2])\n",
    "\n",
    "list_gramm = [dict_grams_human[i][1] for i in dict_grams_human.keys()]\n",
    "    \n",
    "X_human = pd.DataFrame(X0)\n",
    "X_human['ind'] = dict_grams_human.keys()\n",
    "X_human['name'] = list_gramm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b90f609b",
    "outputId": "6b6c3fba-980c-42c0-f794-6443920ee524"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "o_clock         1186\n",
       "every_thing     1002\n",
       "pass_through     783\n",
       "sit_down         650\n",
       "slave_trade      641\n",
       "Name: name, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_human['name'].value_counts()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d07090f8",
    "outputId": "3abca1a5-b41e-44dd-c541-36b3f66c32bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(538810, 22)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_human.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d995d904",
    "outputId": "0d27c321-4973-4616-d97f-76e7929773b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(538810, 22)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_human.to_csv('/content/drive/MyDrive/2022-01-15_Course_project/TF_IDF_clustering_en/10000_n_2gramm_human_en.csv')\n",
    "X_human.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "a16b0d8a"
   },
   "outputs": [],
   "source": [
    "list_col = list(X_human.columns)\n",
    "for i in ['Unnamed: 0', 'ind', 'name']:\n",
    "    if i in list_col:\n",
    "        list_col.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ffc9dd9",
    "outputId": "32dd7200-f594-4d4b-81bc-8b6488cd6ed3"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-22 18:03:35.608 | begin | significance:  1000 | neighbors:  50\n",
      "Start clustering\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: divide by zero encountered in power\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-22 21:02:59.145 | end | {'cohesion': 8571.319560256108, 'separation': 12140.398810480781, 'calinski_harabaz_score': 195.9886519420432, 'RMSSTD': 0.028304474174194103, 'RS': 0.5861608676387597, 'silhouette': -0.1965591040947928, 'significance': 1000, 'neighbors': 50, 'cluster_num': 3867}\n",
      "2022-01-22 21:03:21.913 | begin | significance:  1000 | neighbors:  100\n",
      "Start clustering\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: divide by zero encountered in power\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-23 00:37:20.579 | end | {'cohesion': 8146.544521532364, 'separation': 12565.173849204526, 'calinski_harabaz_score': 685.8482355045229, 'RMSSTD': 0.027525935621499728, 'RS': 0.6066697907092811, 'silhouette': -0.137816160048998, 'significance': 1000, 'neighbors': 100, 'cluster_num': 1210}\n",
      "2022-01-23 00:37:43.757 | begin | significance:  100000 | neighbors:  50\n",
      "Start clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: divide by zero encountered in power\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-23 03:35:54.893 | end | {'cohesion': 8571.319560256108, 'separation': 12140.398810480781, 'calinski_harabaz_score': 195.9886519420432, 'RMSSTD': 0.028304474174194103, 'RS': 0.5861608676387597, 'silhouette': -0.1965591040947928, 'significance': 100000, 'neighbors': 50, 'cluster_num': 3867}\n",
      "2022-01-23 03:36:17.993 | begin | significance:  100000 | neighbors:  100\n",
      "Start clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: divide by zero encountered in power\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-23 07:10:09.765 | end | {'cohesion': 8146.544521532364, 'separation': 12565.173849204526, 'calinski_harabaz_score': 685.8482355045229, 'RMSSTD': 0.027525935621499728, 'RS': 0.6066697907092811, 'silhouette': -0.137816160048998, 'significance': 100000, 'neighbors': 100, 'cluster_num': 1210}\n"
     ]
    }
   ],
   "source": [
    "#GridSearch for Clustering\n",
    "grid_result = []\n",
    "for sig in [1000, 100000]:\n",
    "    for nei in [50, 100]:\n",
    "        \n",
    "        print(datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3], '| begin |', 'significance: ', sig, '| neighbors: ', nei )\n",
    "        clust = Wishart(significance_level = sig, wishart_neighbors = nei)\n",
    "        result = clust.fit(X_human[list_col])\n",
    "        dict_r = get_validation_scores(np.array(X_human[list_col]), clust.object_labels, max_clust = 10000)\n",
    "        dict_r['significance'] = sig\n",
    "        dict_r['neighbors'] = nei\n",
    "        dict_r['cluster_num'] = len(set(clust.object_labels))\n",
    "        grid_result.append(dict_r)\n",
    "        \n",
    "        #add clustering result to table\n",
    "        name_col = 'cluster_' + str(sig) + str(nei)\n",
    "        X_human[name_col] = clust.object_labels\n",
    "        \n",
    "        print(datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3], '| end |',  dict_r)\n",
    "\n",
    "        X_human.to_csv('/content/drive/MyDrive/2022-01-15_Course_project/TF_IDF_clustering_en/10000_n_2gramm_human_en.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MztZ5X9m1UT0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "name": "TF_IDF_Clustering_en.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
