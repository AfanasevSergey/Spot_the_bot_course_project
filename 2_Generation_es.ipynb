{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SXghWytGaz4E"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from google.colab import drive\n",
    "import os, zipfile, glob\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8gGPuN1LbDYP",
    "outputId": "a5c5f1d8-a260-415c-c555-52c52d3309cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0b3Xp-FOR7h9"
   },
   "outputs": [],
   "source": [
    "# Spanish books from the gutenberg.org for text generation (top 5 in volume with the right encoding)\n",
    "'2000-0'\n",
    "'62870-0'\n",
    "'64465-0'\n",
    "'59539-0'\n",
    "'57654-0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MNng6ICsStYj",
    "outputId": "c4eaed85-f69b-42d7-8808-1c06c5094b84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-17 23:21:40--  https://www.gutenberg.org/files/2000/2000-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2226045 (2.1M) [text/plain]\n",
      "Saving to: ‘2000-0.txt.3’\n",
      "\n",
      "2000-0.txt.3        100%[===================>]   2.12M  1.63MB/s    in 1.3s    \n",
      "\n",
      "2022-01-17 23:21:42 (1.63 MB/s) - ‘2000-0.txt.3’ saved [2226045/2226045]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://www.gutenberg.org/files/2000/2000-0.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9-wqBHK4SfoT",
    "outputId": "4160cdbd-c92f-45be-e461-e5f4b1ac750a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-17 23:21:43--  https://www.gutenberg.org/files/62870/62870-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1870046 (1.8M) [text/plain]\n",
      "Saving to: ‘62870-0.txt.3’\n",
      "\n",
      "62870-0.txt.3       100%[===================>]   1.78M  1.37MB/s    in 1.3s    \n",
      "\n",
      "2022-01-17 23:21:45 (1.37 MB/s) - ‘62870-0.txt.3’ saved [1870046/1870046]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://www.gutenberg.org/files/62870/62870-0.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JFhljiMVSfrC",
    "outputId": "2b7b562a-7a60-4212-e7b2-240a605529b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-17 23:21:45--  https://www.gutenberg.org/files/64465/64465-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1397772 (1.3M) [text/plain]\n",
      "Saving to: ‘64465-0.txt.3’\n",
      "\n",
      "64465-0.txt.3       100%[===================>]   1.33M  1.19MB/s    in 1.1s    \n",
      "\n",
      "2022-01-17 23:21:47 (1.19 MB/s) - ‘64465-0.txt.3’ saved [1397772/1397772]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://www.gutenberg.org/files/64465/64465-0.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vS_TBqxKSft0",
    "outputId": "06a1d636-9efc-4afc-e3a5-c15a85de63d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-17 23:21:47--  https://www.gutenberg.org/files/59539/59539-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1252731 (1.2M) [text/plain]\n",
      "Saving to: ‘59539-0.txt.3’\n",
      "\n",
      "59539-0.txt.3       100%[===================>]   1.19M  1.07MB/s    in 1.1s    \n",
      "\n",
      "2022-01-17 23:21:49 (1.07 MB/s) - ‘59539-0.txt.3’ saved [1252731/1252731]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://www.gutenberg.org/files/59539/59539-0.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eUEKj3PySf3t",
    "outputId": "3b771cc9-63a8-4616-ddb6-d8fab92d09e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-17 23:21:49--  https://www.gutenberg.org/files/57654/57654-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1131399 (1.1M) [text/plain]\n",
      "Saving to: ‘57654-0.txt.3’\n",
      "\n",
      "57654-0.txt.3       100%[===================>]   1.08M   991KB/s    in 1.1s    \n",
      "\n",
      "2022-01-17 23:21:52 (991 KB/s) - ‘57654-0.txt.3’ saved [1131399/1131399]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://www.gutenberg.org/files/57654/57654-0.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w_MSI2ySRthF",
    "outputId": "1fa1eba7-9a19-421c-fd9b-40ca92b4f9cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ZTBdWWP1BSWC"
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "from bs4 import BeautifulSoup\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "lv7154iU-xLD"
   },
   "outputs": [],
   "source": [
    "list_books = ['2000-0', '62870-0', '64465-0', '59539-0', '57654-0']\n",
    "text_add = ''\n",
    "for b in list_books:\n",
    "  with open(b + '.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "    info_beg = text.find('Title: ')\n",
    "    begin = text.find('*** START OF THIS PROJECT GUTENBERG EBOOK ')\n",
    "    title = ''\n",
    "    if begin > -1:\n",
    "      for i in text[info_beg : begin].split('\\n'):\n",
    "        if i.find('Title: ') > -1:\n",
    "          title = i.split('Title: ')[1]\n",
    "          begin_2 = begin + 42 + len(title) + 3\n",
    "                \n",
    "    else:\n",
    "      begin = text.find('START OF THE PROJECT GUTENBERG EBOOK ')\n",
    "      for i in text[info_beg : begin].split('\\n'):\n",
    "        if i.find('Title: ') > -1:\n",
    "          title = i.split('Title: ')[1]\n",
    "          begin_2 = begin + 37 + len(title) + 3\n",
    "\n",
    "    end = text.find('*** END OF THIS PROJECT GUTENBERG EBOOK')\n",
    "    if end == -1:\n",
    "      end = text.find('*** END')\n",
    "    text = text[begin_2 : end]\n",
    "\n",
    "    #remove html tags from text\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text(separator=\" \")\n",
    "    \n",
    "    #remove accented characters from text, e.g. café\n",
    "    text = unidecode.unidecode(text)\n",
    "\n",
    "    text_add = text_add + text + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SZ1ufTF8-xjI",
    "outputId": "15ef94a7-b888-4733-ebf5-d3a846655964"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172417"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "8oLzuNb15tpf"
   },
   "outputs": [],
   "source": [
    "# Create two dictionaries:\n",
    "# int2char -- maps integers to characters\n",
    "# char2int -- maps characters to unique integers\n",
    "\n",
    "int2char = dict(enumerate(set(text)))\n",
    "char2int = { val: idx  for idx, val in int2char.items()}\n",
    "\n",
    "# encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vi0bo7KDMxFx",
    "outputId": "779efc55-ecba-4282-d515-25445d9557ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(char2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "QxFS2B9nD-kg"
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # arr - array of integers\n",
    "    # n_labels - number of labels (the size of a one-hot-encoded vector)\n",
    "\n",
    "    one_hot = np.eye(n_labels)[arr]\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hrcsQ3qVECzT",
    "outputId": "49cafba3-ca46-41d0-de83-94af506db362"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# check that the function works correctly\n",
    "test_indx = np.array([[7, 2, 5]])\n",
    "one_hot = one_hot_encode(test_indx, 8)\n",
    "\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Q-0ZuGZhEFd_"
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    # Create a generator that returns batches of size batch_size x seq_length\n",
    "    \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    ## Get the number of batches we can make\n",
    "    n_batches = int(len(arr) / batch_size_total)\n",
    "    \n",
    "    ## Keep only enough characters to make full batches\n",
    "    arr = arr[: n_batches*batch_size_total] \n",
    "    \n",
    "    ## Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1)) \n",
    "    \n",
    "    ## Iterate over the batches using a window of size seq_length\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        x = arr[:, n : n + seq_length ]\n",
    "        # The target is a version of x shifted by one (do not forget border conditions)\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1] = x[:, 1:]\n",
    "        if n + seq_length < arr.shape[1]:\n",
    "            y[:, -1] = arr[:, n + seq_length]\n",
    "        else:\n",
    "            arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "4CCx96r-ESVp"
   },
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 4, 30)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GcefjsfQEXyg",
    "outputId": "ada4d116-cfc9-4b24-929c-87e42eacbc13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "q9YXlR9GEa3L"
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        # Define the LSTM layer\n",
    "        ## YOUR CODE HERE\n",
    "        self.lstm = nn.LSTM(input_size=len(self.chars), hidden_size=self.n_hidden, \n",
    "                            num_layers=self.n_layers, dropout=self.drop_prob, \n",
    "                            batch_first=True)\n",
    "\n",
    "        # Define a dropout layer\n",
    "        ## YOUR CODE HERE\n",
    "        self.dropout = nn.Dropout(self.drop_prob)\n",
    "\n",
    "        # Define the final, fully-connected output layer\n",
    "        ## YOUR CODE HERE\n",
    "        self.linear = nn.Linear(in_features=self.n_hidden, out_features=len(self.chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        # Get the outputs and the new hidden state from the lstm\n",
    "        ## YOUR CODE HERE\n",
    "        out, hidden = self.lstm(x.float(), hidden)\n",
    "\n",
    "        # Pass through a dropout layer\n",
    "        ## YOUR CODE HERE\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # Stack up LSTM outputs using view\n",
    "        # you may need to use contiguous to reshape the output\n",
    "        ## YOUR CODE HERE\n",
    "        out = out.reshape(out.size()[0] * out.size()[1], self.n_hidden)\n",
    "\n",
    "\n",
    "        # Put x through the fully-connected layer\n",
    "        ## YOUR CODE HERE\n",
    "        out = self.linear(out)\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "6h3NSG5kGIuo"
   },
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                now = datetime.now()\n",
    "                dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)),\n",
    "                      \"Time: \", dt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dpIky4vhGI0X",
    "outputId": "d714b9ab-da8e-499d-90a9-89d298190bb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(82, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (linear): Linear(in_features=512, out_features=82, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 512 \n",
    "n_layers = 2\n",
    "\n",
    "chars = tuple(set(text))\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "pUQx6NIPSQza"
   },
   "outputs": [],
   "source": [
    "with open('/content/drive/MyDrive/2022-01-15_Course_project/LSTM_generator_es/dict_chars.pickle', 'wb') as f:\n",
    "    pickle.dump(chars, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "dNxzMAoDULi9"
   },
   "outputs": [],
   "source": [
    "with open('/content/drive/MyDrive/2022-01-15_Course_project/LSTM_generator_es/dict_chars.pickle', 'rb') as f:\n",
    "  c = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fH3lwyzAULlc",
    "outputId": "bd559eb8-5056-4e01-b25a-56acad32de88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T', 'h', '\"', '_', 'O', 'q', 'M', 'i', '9', 'o', '/', 'v', '*', 'L', 't', 'X', 'U', '[', 'x', 'Y', 'W', 'b', 'F', 'D', 'e', 'S', 'C', '>', 'N', '<', ':', 'r', ')', 'A', 'p', '4', 'k', 'y', 'f', '\\n', '(', 'B', 'j', 'u', 's', '2', 'a', 'G', '0', 'z', 'n', 'w', 'K', ' ', ';', ']', 'I', 'l', '5', 'g', ',', '8', '1', 'J', 'Z', 'P', 'V', '?', 'c', 'R', 'E', 'Q', 'H', 'm', \"'\", '3', '6', '!', '-', '.', 'd', '7')\n"
     ]
    }
   ],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6UA8rn_7GI7j",
    "outputId": "5d9ec1da-0ecc-4709-c13a-d0c6960c8eea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/80... Step: 500... Loss: 1.8649... Val Loss: 1.7833 Time:  17/01/2022 23:23:28\n",
      "Epoch: 28/80... Step: 1000... Loss: 1.5954... Val Loss: 1.4797 Time:  17/01/2022 23:24:17\n",
      "Epoch: 41/80... Step: 1500... Loss: 1.3695... Val Loss: 1.3599 Time:  17/01/2022 23:25:05\n",
      "Epoch: 55/80... Step: 2000... Loss: 1.2714... Val Loss: 1.3037 Time:  17/01/2022 23:25:53\n",
      "Epoch: 68/80... Step: 2500... Loss: 1.1658... Val Loss: 1.2853 Time:  17/01/2022 23:26:43\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "seq_length = 100 \n",
    "n_epochs = 80\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "oPfr5ScHHndn"
   },
   "outputs": [],
   "source": [
    "path = \"/content/drive/MyDrive/2022-01-15_Course_project/LSTM_generator_es/text_generator.pt\" \n",
    "torch.save(net.state_dict(), path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R6SnXShsKOHh",
    "outputId": "68458a5a-1c6f-453a-dbb5-433c15e33353"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/content/drive/MyDrive/2022-01-15_Course_project/LSTM_generator_es/text_generator.pt\" \n",
    "\n",
    "\n",
    "with open('/content/drive/MyDrive/2022-01-15_Course_project/LSTM_generator_es/dict_chars.pickle', 'rb') as f:\n",
    "    chars_2 = pickle.load(f)\n",
    "\n",
    "n_hidden = 512  \n",
    "n_layers = 2\n",
    "net_2 = CharRNN(chars_2, n_hidden, n_layers)\n",
    "net_2.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m9Cq8T_XPkD-",
    "outputId": "a6926888-ddca-4f16-f57e-72992321fd27"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharRNN(\n",
       "  (lstm): LSTM(82, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (linear): Linear(in_features=512, out_features=82, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "uex89qSKP-v_"
   },
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "RnK_hi4vQAEq"
   },
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f-9Gt8-DQDjR",
    "outputId": "a199896f-db0d-4567-c4a3-1b540c6f7c91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahora pase el personaje anterior y obtenga uno nuevo de\n",
      "el a obscura y pila; pues todavia los mismos teucros--al en en seguida ligero gran voyo a la\n",
      "necha infentilo en triste miembra y le acundaba del carro; y si\n",
      "no lo riseran, dijo a los teucros, recopias a los amigos\n",
      "de quien persucera a los teucros. Tambien\n",
      "su hermano, para que le mata por las naves y a Antiloco, dirigiera\n",
      "los mortales, ne se largante a un dios. Y si lo amada te sabe faso\n",
      "venido, tensa en la mucerte cincuenta llona. ?Que interriris\n",
      "los rodillos! Creto a Hector y a mi hijo seria\n",
      "funesto de la batalla, ni es un dios, conterron, los\n",
      "aqueos les fuitas por el pecho y el apatito, con el combate de\n",
      "los dioses, te hizo infulgo que a la divina estaba ceyas lor donde se\n",
      "ojos a ohillas y aquel, todo como el seno y anima con interignale\n",
      "inselvacian con las hijas del guarria de grandes olas.>>\n",
      "\n",
      "545 Asi dijo; y el heroe, se puso en la blondira del timo:.\n",
      "\n",
      "191 Diomedes, entregandole con sus hijos y a la libado, y\n",
      "con el corazon acobardo el corazon al dios los teucros.\n",
      "\n",
      "845 Mas asi que para muchas peleas protucharan a los aqueos en vina siempre y\n",
      "en tirar el poderoso Priamo y de la muerte atado con los terribles. Ordeno, huia,\n",
      "aunque los que habitaban en ella; para nocher lucierono cerrada y animo a\n",
      "los hombres. No es ducien las corazas, sino que le dio mi canto.\n",
      "Jupiter alcanzo se lo arrojare; pues Aquiles se\n",
      "te ya tu puede presenciar oles ni tengara sorreba. Atravesa la expresnidas lanza con rodillas sacrificios por matar al auriga.>>\n",
      "\n",
      "163 Contesto Juno, la diosa de los niveos brazos: <<!Talidamonio, los\n",
      "caudillos que voy a suchamante inmortales\n",
      "que peletio en el polvo, os sangre y corrio el ajulo del hueso de la\n",
      "tierra, sin mentar a combatir! No quieras que vigoroso,\n",
      "limo nos muchos ya obedecerme la papta contra Priamo y si\n",
      "aquellos escucharon nuestra espalda de propositos vilos. Pero consejas,\n",
      "si quieres esta. Por tentro, el cadaver de\n",
      "Jupiter, estos honras de aquellos y los teucros vencian en el mismo, pues que vengas fuego a\n",
      "la anciano del magnanimo Hestulo, de, de hermosa\n",
      "palteca. Temir cerca de las puertas, el de los pies de la\n",
      "junta, sino que no dejas de la misma, una nodo ayudan en\n",
      "su paetico: el estargo cuando los dioses abuntante penetran en las\n",
      "voces casa de fleria que les eria la que he se despadara a\n",
      "su padre, pues tenea pesar altiracion y antenlian corriermos con ofrezados. El\n",
      "fornido Hercules taba habia, pues cayo con atribles, echo cidado de\n",
      "brazo para pronta parte de lido: todos el dios mientras las naves\n",
      "con un goste cuanto aparecieran; con ellos una torre\n",
      "de pelea: ha de persandir\n",
      "al divino Minesva cuando le sacando, un creo que tambien los muchos combatientes\n",
      "se echan y sera el pelero en su mano, pero no levantose por el bronce\n",
      "tiene\n",
      "el pusto y los peclos, no nos hizo remar; y a aquel combatiera el aqueo\n",
      "detracios a los aqueos. Y ya que dices que me enviandose,\n",
      "detiendo la nave con igual alma coruna el terdero carro y las saltaban de la\n",
      "muchedumbre.>>\n",
      "\n",
      "363 Dijo, y a los pioles de los dioses; los el veloz Agamenon,\n",
      "podriandole llorar y obedecieron el nombre de la tierra con una solida\n",
      "coraza y no lego para desirgir sabar la discursa, y humase la colera del\n",
      "valiando mientras asintiendo a la pila y ligera\n",
      "alenta. Fue el visto aparevio la muralla, la males que a su volaz\n",
      "mas que despertare irritado en su beneficio, de una de los hombres\n",
      "como con infantes de los fuertes dioses en las batallas y guardendos\n",
      "aguasdoles y les dio gloria las bien canstruidas premas, hasta que Aquiles mata a Aquiles porque\n",
      "en un cioquen la suecte no es lientas ni por la llanura tuvo de la vida.>>\n",
      "\n",
      "252 Asi hablo. Mas, por dobo dea parde de Tecisto, caro con las lanzas.\n",
      "\n",
      "586 Juno, la diosa de los carilentos de Autonomo,\n",
      "despojandoba muchas sales de ellos dio. Entonces cecieron las tiaisas\n",
      "por estas raldaneras. A Patroclo, famolo por los danaos, por encima\n",
      "de la lucha, puso en repar la lanza. Los teucros se arrojaran lo entregaron de aquellos y\n",
      "audaz el iba hondo pira los terrobles enciendes del espumoso combate y en la licia a un repochendo del\n",
      "combate, tendiendo las valientes prevastadas de citoca.\n",
      "\n",
      "184 Al lograrlo Tidomo atraveso un rio y pasto del fuego a ellos en los cayos, cada cual tropondo\n",
      "un gran bronce hestio el escudo y tu bajo hasta\n",
      "de durce la velez. Equetaba y los padres\n",
      "la agradabla poblada se embarcieron hasta las cunadas del Atrida. El\n",
      "Pelida dos reino a los mismos, donde samos, los\n",
      "cuerpos ha insolido la combatienda y salvara al gran Jupiter, no se adelantaron tu pecho. De mi agua de\n",
      "todo hasta que le estaba dividado a lo mas divino que haya las cercas del\n",
      "troyano al mar del luccance conceden. En survidores se cumplaren hacia al\n",
      "alma.>>\n",
      "\n",
      "447 Asi hablo el rey Menelao; hijo de Bilos con gran combate; que los\n",
      "hombres ora de los enemigos a que te pronto llegara a ser el terrible teucro, para famisao como libio\n",
      "a hororban de darde. Ahora esto\n",
      "para compiencer el aquio y el dios hasta poseia un hombre,\n",
      "para que recibiaba roinembas a la ciudad y otra cepa en el alzo\n",
      "supor. Tu padre te vaneraname tiempo y cuando asi no\n",
      "habiese luchado el dia; los danaos muy mandicaientos al mismo en\n",
      "alta; pero te obra al campo de lo que el mejor\n",
      "de la mas; los danaos tienen corriente no me entregas, a instruiguad el animo\n",
      "a los demas, por los veinte cercas, y cayo de\n",
      "espelanza que remiso un batil. Pero ti al mas padre, que los aqueos ha sotrechado y sobre\n",
      "una parte del sombra: el padre del tenebro las homas\n",
      "de pelear, que, a ti por los troyanos, cuando ahora venimos a\n",
      "sus armadios; pero asi que, temo mas pagalio en la diente de Jove, no quieras bien aun asientro.\n",
      "Tu lo hiciera, librandome en la cumbre de Hector cuando argivos las\n",
      "tugalente contra el si pereciera a Patroclo y siguiera el companero,\n",
      "vistio el divino Centoo, para que con el pie tragaro colvo\n",
      "al sol, con el encuentro en irrepieran para venter\n",
      "los muros, mi cadaver a su pie, la armadura de la magnivida coraza.>>\n",
      "\n",
      "478 Dijo, y permanecia en inmortal y vino, serial postron\n",
      "llamado, se quedo si con todo clavillo, haciendolos hambraban\n",
      "siguieron el tropo propio establos, oh Ayax, pia,\n",
      "timiempo a su mas coma convencillante. Si suspinaba son a los\n",
      "enemigos, con traderamente la defendar los inmortales.\n",
      "Como el\n",
      "patria, respeta el arto, que males; pues las peleas no dejo de rechazar a los hambres en que no\n",
      "perdieren de la muerte del rey Marte. No le posemos de repisar en la\n",
      "facta mas troyanamos huyenda por la punta, pues hasta sus espiritu,\n",
      "y una saltando conve vivo pugba que me ha primorosas y saper con razon\n",
      "los troyanos. Y Jupiter no puede los ancianos, subio en la conterdicion de los teucros; la figurada que el otro los\n",
      "monte se coleraba el carro; pero no podrio otra el caramento\n",
      "de recretenlalmente y tan gorte, como la noche, pronto en carros, y con\n",
      "el mas hico mas hermosa feral combate, caballoro de Aquiles; porque\n",
      "ya no\n",
      "salcan incensiones en cumplirlos, la que si el impetuoso Aquiles hubiese cuando\n",
      "Patroclo es librar de la luciente pica, a quien se salva o corriente; el\n",
      "delicedable tedor, pronto conviane haber valecido, en las naves. Todo\n",
      "ves otras vivas que descendieron con rofues la esposa no se hallan los jovenes otros\n",
      "del rio.>>\n",
      "\n",
      "628 Asi dijo. Al extenmo dies no debia de conocer la negra sangre sea notara, si fue en todos tunimas, animo\n",
      "en la ciudad, los demas cerbandos desde hijo\n",
      "los que eran tuniestes no tonian volver a la meno\n",
      "de Aquiles, porque el asgan envarado por sus carros.--Par el mando el rey, de escolado\n",
      "de tente hoso un dios, y modiendose una derrema para cerca que fuese el navio para\n",
      "sin destruira olas.>>\n",
      "\n",
      "709 Para hacerlos las diosas, y se despertaren de la palabia, y pensando en cada\n",
      "alve. Esformiento con las corves naves pelaaron. El\n",
      "divino Aquiles, el de los pies ligeros, se abroce en mulos\n",
      "dijo en vocepientes. Un cordon diciendo supirmanse\n",
      "sin produjoso con las mismas para mis armados, arrancaran entre la muerte de\n",
      "Aquiles, por esta llamado, dentro del centro del cuerpo, y se mitara\n",
      "desde altos, porque salto del cielo a ti, por ellos puse, a voz retirado. El\n",
      "grato sus cuales, al men\n"
     ]
    }
   ],
   "source": [
    "# Example of generated text\n",
    "print(sample(net, 8000, prime='Ahora pase el personaje anterior y obtenga uno nuevo', top_k=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "fU3wXKAJir0F"
   },
   "outputs": [],
   "source": [
    "text_ph = text.split('. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "v_cULwHCL5WZ"
   },
   "outputs": [],
   "source": [
    "list_b1 = [ i.split(' ')[0].lower() for i in text_ph]\n",
    "list_b2 = [ ' '.join(j.lower() for j in i.split(' ')[: 2]) for i in text_ph]\n",
    "list_b3 = [ ' '.join(j.lower() for j in i.split(' ')[: 3]) for i in text_ph]\n",
    "list_b4 = [ ' '.join(j.lower() for j in i.split(' ')[: 4]) for i in text_ph]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jM_fi5jCnISh",
    "outputId": "4904b885-1957-4178-ac7d-d2c83dbe4dfa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('y', 331),\n",
       " ('el', 155),\n",
       " ('pero', 112),\n",
       " ('los', 81),\n",
       " ('como', 78),\n",
       " ('mas', 69),\n",
       " ('no', 66),\n",
       " ('en', 62),\n",
       " ('nombre', 59),\n",
       " ('a', 56)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(list_b1).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Vsg1vD0jm3Kf"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "6s0PlDHjkN6n"
   },
   "outputs": [],
   "source": [
    "list_of_begins = set()\n",
    "for i in Counter(list_b1).most_common(10):\n",
    "  list_of_begins.add(re.sub(\"[^A-Za-z] \", \"\", i[0]).capitalize())\n",
    "\n",
    "for i in Counter(list_b2).most_common(10):\n",
    "  list_of_begins.add(re.sub(\"[^A-Za-z] \", \"\", i[0]).capitalize())\n",
    "\n",
    "for i in Counter(list_b3).most_common(10):\n",
    "  list_of_begins.add(re.sub(\"[^A-Za-z] \", \"\", i[0]).capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O36e-_TSnB4D",
    "outputId": "8fe00701-a915-4589-892c-0f9fb65ae312"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Mas',\n",
       " 'En',\n",
       " 'El',\n",
       " 'Mas ?por que',\n",
       " 'El guerrero cayo',\n",
       " 'Eaobremos todos',\n",
       " 'Y',\n",
       " 'Y ahora',\n",
       " 'Pero no',\n",
       " 'Nombre',\n",
       " 'Nombre patronimico de',\n",
       " 'El hijo de',\n",
       " 'Nombre patronimico',\n",
       " 'No',\n",
       " ' ',\n",
       " 'El atrida',\n",
       " 'Y en',\n",
       " 'Y si',\n",
       " 'Nombre patronimico de\\n',\n",
       " 'Como',\n",
       " 'Los',\n",
       " 'Pero',\n",
       " 'Y cuando',\n",
       " 'Y el',\n",
       " 'A',\n",
       " 'Como el',\n",
       " 'Fue muerto por']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(list_of_begins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "KQENaOqFYGNA"
   },
   "outputs": [],
   "source": [
    "# Removing the empty starting characters\n",
    "list_of_begins.remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "CHjyZoRUM-Sn"
   },
   "outputs": [],
   "source": [
    "# Removing the empty starting characters\n",
    "list_of_begins.remove(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "otKbQCMrNSi7",
    "outputId": "58c0533a-cb72-426b-9571-fd98955a5926"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mas',\n",
       " 'En',\n",
       " 'El',\n",
       " 'Mas ?por que',\n",
       " 'El guerrero cayo',\n",
       " 'Eaobremos todos',\n",
       " 'Y',\n",
       " 'Y ahora',\n",
       " 'Pero no',\n",
       " 'Nombre',\n",
       " 'Nombre patronimico de',\n",
       " 'El hijo de',\n",
       " 'Nombre patronimico',\n",
       " 'No',\n",
       " 'El atrida',\n",
       " 'Y en',\n",
       " 'Y si',\n",
       " 'Nombre patronimico de\\n',\n",
       " 'Como',\n",
       " 'Los',\n",
       " 'Pero',\n",
       " 'Y cuando',\n",
       " 'Y el',\n",
       " 'A',\n",
       " 'Como el',\n",
       " 'Fue muerto por']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(list_of_begins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "Pad0hALYpnUB"
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ITJIToF2pLUC",
    "outputId": "cb6c3f3e-0094-490d-b0ce-c2062bf9bc81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/01/2022 23:30:37 |  number of processed files: 0 0.0 %\n",
      "17/01/2022 23:44:40 |  number of processed files: 200 1.3 %\n",
      "17/01/2022 23:58:41 |  number of processed files: 400 2.7 %\n",
      "18/01/2022 00:12:43 |  number of processed files: 600 4.0 %\n",
      "18/01/2022 00:26:49 |  number of processed files: 800 5.3 %\n",
      "18/01/2022 00:40:53 |  number of processed files: 1000 6.7 %\n",
      "18/01/2022 00:54:58 |  number of processed files: 1200 8.0 %\n",
      "18/01/2022 01:09:04 |  number of processed files: 1400 9.3 %\n",
      "18/01/2022 01:23:12 |  number of processed files: 1600 10.7 %\n",
      "18/01/2022 01:37:19 |  number of processed files: 1800 12.0 %\n",
      "18/01/2022 01:51:30 |  number of processed files: 2000 13.3 %\n",
      "18/01/2022 02:05:41 |  number of processed files: 2200 14.7 %\n",
      "18/01/2022 02:19:51 |  number of processed files: 2400 16.0 %\n",
      "18/01/2022 02:34:00 |  number of processed files: 2600 17.299999999999997 %\n",
      "18/01/2022 02:48:11 |  number of processed files: 2800 18.7 %\n",
      "18/01/2022 03:02:21 |  number of processed files: 3000 20.0 %\n",
      "18/01/2022 03:16:31 |  number of processed files: 3200 21.3 %\n",
      "18/01/2022 03:30:42 |  number of processed files: 3400 22.7 %\n",
      "18/01/2022 03:44:53 |  number of processed files: 3600 24.0 %\n",
      "18/01/2022 03:59:05 |  number of processed files: 3800 25.3 %\n",
      "18/01/2022 04:13:16 |  number of processed files: 4000 26.700000000000003 %\n",
      "18/01/2022 04:27:29 |  number of processed files: 4200 28.000000000000004 %\n",
      "18/01/2022 04:41:39 |  number of processed files: 4400 29.299999999999997 %\n",
      "18/01/2022 04:55:50 |  number of processed files: 4600 30.7 %\n",
      "18/01/2022 05:10:02 |  number of processed files: 4800 32.0 %\n",
      "18/01/2022 05:24:13 |  number of processed files: 5000 33.300000000000004 %\n",
      "18/01/2022 05:38:24 |  number of processed files: 5200 34.699999999999996 %\n",
      "18/01/2022 05:52:35 |  number of processed files: 5400 36.0 %\n",
      "18/01/2022 06:06:47 |  number of processed files: 5600 37.3 %\n",
      "18/01/2022 06:21:00 |  number of processed files: 5800 38.7 %\n",
      "18/01/2022 06:35:16 |  number of processed files: 6000 40.0 %\n",
      "18/01/2022 06:49:29 |  number of processed files: 6200 41.3 %\n",
      "18/01/2022 07:03:42 |  number of processed files: 6400 42.699999999999996 %\n",
      "18/01/2022 07:17:54 |  number of processed files: 6600 44.0 %\n",
      "18/01/2022 07:32:06 |  number of processed files: 6800 45.300000000000004 %\n",
      "18/01/2022 07:46:18 |  number of processed files: 7000 46.7 %\n",
      "18/01/2022 08:00:30 |  number of processed files: 7200 48.0 %\n",
      "18/01/2022 08:14:43 |  number of processed files: 7400 49.3 %\n",
      "18/01/2022 08:28:57 |  number of processed files: 7600 50.7 %\n",
      "18/01/2022 08:43:09 |  number of processed files: 7800 52.0 %\n",
      "18/01/2022 08:57:21 |  number of processed files: 8000 53.300000000000004 %\n",
      "18/01/2022 09:11:34 |  number of processed files: 8200 54.7 %\n",
      "18/01/2022 09:25:47 |  number of processed files: 8400 56.00000000000001 %\n",
      "18/01/2022 09:40:00 |  number of processed files: 8600 57.3 %\n",
      "18/01/2022 09:54:15 |  number of processed files: 8800 58.699999999999996 %\n",
      "18/01/2022 10:08:29 |  number of processed files: 9000 60.0 %\n",
      "18/01/2022 10:22:41 |  number of processed files: 9200 61.3 %\n",
      "18/01/2022 10:36:53 |  number of processed files: 9400 62.7 %\n",
      "18/01/2022 10:51:08 |  number of processed files: 9600 64.0 %\n",
      "18/01/2022 11:05:24 |  number of processed files: 9800 65.3 %\n",
      "18/01/2022 11:19:39 |  number of processed files: 10000 66.7 %\n",
      "18/01/2022 11:33:52 |  number of processed files: 10200 68.0 %\n",
      "18/01/2022 11:48:04 |  number of processed files: 10400 69.3 %\n",
      "18/01/2022 12:02:19 |  number of processed files: 10600 70.7 %\n",
      "18/01/2022 12:16:33 |  number of processed files: 10800 72.0 %\n",
      "18/01/2022 12:30:47 |  number of processed files: 11000 73.3 %\n",
      "18/01/2022 12:45:04 |  number of processed files: 11200 74.7 %\n",
      "18/01/2022 12:59:20 |  number of processed files: 11400 76.0 %\n",
      "18/01/2022 13:13:34 |  number of processed files: 11600 77.3 %\n",
      "18/01/2022 13:27:46 |  number of processed files: 11800 78.7 %\n",
      "18/01/2022 13:42:01 |  number of processed files: 12000 80.0 %\n",
      "18/01/2022 13:56:16 |  number of processed files: 12200 81.3 %\n",
      "18/01/2022 14:10:29 |  number of processed files: 12400 82.69999999999999 %\n",
      "18/01/2022 14:24:40 |  number of processed files: 12600 84.0 %\n",
      "18/01/2022 14:38:54 |  number of processed files: 12800 85.3 %\n",
      "18/01/2022 14:53:06 |  number of processed files: 13000 86.7 %\n",
      "18/01/2022 15:07:21 |  number of processed files: 13200 88.0 %\n",
      "18/01/2022 15:21:37 |  number of processed files: 13400 89.3 %\n",
      "18/01/2022 15:35:50 |  number of processed files: 13600 90.7 %\n",
      "18/01/2022 15:50:03 |  number of processed files: 13800 92.0 %\n",
      "18/01/2022 16:04:17 |  number of processed files: 14000 93.30000000000001 %\n",
      "18/01/2022 16:18:33 |  number of processed files: 14200 94.69999999999999 %\n",
      "18/01/2022 16:32:51 |  number of processed files: 14400 96.0 %\n",
      "18/01/2022 16:47:08 |  number of processed files: 14600 97.3 %\n",
      "18/01/2022 17:01:23 |  number of processed files: 14800 98.7 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_docs = 15000\n",
    "\n",
    "path = '/content/drive/MyDrive/2022-01-15_Course_project/gen_text_es/'\n",
    "\n",
    "for i in range(num_docs):\n",
    "  start = random.choice(list(list_of_begins))\n",
    "  text_gen = sample(net, 8000, prime= start, top_k=10)\n",
    "\n",
    "  with open(path + 'doc_'+str(i)+'.txt', 'w+') as output_file:\n",
    "    try:\n",
    "      output_file.write(text_gen)\n",
    "    except:\n",
    "      continue\n",
    "  if i % 200 == 0:\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    print(dt_string, '| ',  'number of processed files: ' + str(i), round(i/num_docs, 3)*100, '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FNPcLUAhspTR",
    "outputId": "9aa92df3-8e18-449f-dfe7-8b10840ec4a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5446, 0)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If the generator does not work completely, you need to run this code (it will start from the last place)\n",
    "\n",
    "# This is the code to generate (to determine how many have already been generated)\n",
    "\n",
    "list_gen = glob.glob('/content/drive/MyDrive/2022-01-15_Course_project/gen_text_es/*.txt')\n",
    "list_num = [ int(i.split('/')[-1].split('.txt')[0].split('doc_')[-1]) for i in list_gen]\n",
    "max(list_num), min(list_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YP-76wSBhHsb"
   },
   "outputs": [],
   "source": [
    "# If the generator does not work completely, you need to run this code (it will start from the last place)\n",
    "\n",
    "num_docs = 15000\n",
    "\n",
    "path = '/content/drive/MyDrive/2022-01-15_Course_project/gen_text_es/'\n",
    "\n",
    "for i in range(max(list_num), num_docs, 1):\n",
    "  start = random.choice(list(list_of_begins))\n",
    "  text_gen = sample(net, 8000, prime= start, top_k=10)\n",
    "\n",
    "  with open(path + 'doc_'+str(i)+'.txt', 'w+') as output_file:\n",
    "    try:\n",
    "      output_file.write(text_gen)\n",
    "    except:\n",
    "      continue\n",
    "  if i % 200 == 0:\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    print(dt_string, '| ',  'number of processed files: ' + str(i), round(i/num_docs, 3)*100, '%')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Generation_es.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
