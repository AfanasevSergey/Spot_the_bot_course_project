{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SXghWytGaz4E"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "import os, zipfile, glob\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gGPuN1LbDYP",
        "outputId": "fdec40af-6fdc-4833-a515-2abf68c7828f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0b3Xp-FOR7h9"
      },
      "outputs": [],
      "source": [
        "# Spanish books from the gutenberg.org for text generation (top 5 in volume with the right encoding)\n",
        "'2000-0'\n",
        "'62870-0'\n",
        "'64465-0'\n",
        "'6528-0'\n",
        "'57654-0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNng6ICsStYj",
        "outputId": "65036de8-9c09-45f4-9cc5-1e9feb808009"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-29 17:04:21--  https://www.gutenberg.org/files/2000/2000-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2226045 (2.1M) [text/plain]\n",
            "Saving to: ‘2000-0.txt’\n",
            "\n",
            "2000-0.txt          100%[===================>]   2.12M  4.41MB/s    in 0.5s    \n",
            "\n",
            "2022-01-29 17:04:22 (4.41 MB/s) - ‘2000-0.txt’ saved [2226045/2226045]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget \"https://www.gutenberg.org/files/2000/2000-0.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-wqBHK4SfoT",
        "outputId": "f4194aa1-fd2d-4abc-e7ab-7ed5b15a0bf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-29 17:04:22--  https://www.gutenberg.org/files/62870/62870-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1870046 (1.8M) [text/plain]\n",
            "Saving to: ‘62870-0.txt’\n",
            "\n",
            "62870-0.txt         100%[===================>]   1.78M  3.73MB/s    in 0.5s    \n",
            "\n",
            "2022-01-29 17:04:23 (3.73 MB/s) - ‘62870-0.txt’ saved [1870046/1870046]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget \"https://www.gutenberg.org/files/62870/62870-0.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFhljiMVSfrC",
        "outputId": "a28c0aaf-b78e-4629-fa55-d60f7d2e63c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-29 17:04:23--  https://www.gutenberg.org/files/64465/64465-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1397772 (1.3M) [text/plain]\n",
            "Saving to: ‘64465-0.txt’\n",
            "\n",
            "64465-0.txt         100%[===================>]   1.33M  3.24MB/s    in 0.4s    \n",
            "\n",
            "2022-01-29 17:04:24 (3.24 MB/s) - ‘64465-0.txt’ saved [1397772/1397772]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget \"https://www.gutenberg.org/files/64465/64465-0.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vS_TBqxKSft0",
        "outputId": "4ba586ba-75b0-4981-a521-5a4379bb44b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-29 17:04:27--  https://www.gutenberg.org/files/6528/6528-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4237055 (4.0M) [text/plain]\n",
            "Saving to: ‘6528-0.txt’\n",
            "\n",
            "6528-0.txt          100%[===================>]   4.04M  7.34MB/s    in 0.6s    \n",
            "\n",
            "2022-01-29 17:04:27 (7.34 MB/s) - ‘6528-0.txt’ saved [4237055/4237055]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget \"https://www.gutenberg.org/files/6528/6528-0.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUEKj3PySf3t",
        "outputId": "97e1d6d1-5f9f-406d-9808-55f383fd24d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-29 17:04:29--  https://www.gutenberg.org/files/57654/57654-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1131399 (1.1M) [text/plain]\n",
            "Saving to: ‘57654-0.txt’\n",
            "\n",
            "57654-0.txt         100%[===================>]   1.08M  2.62MB/s    in 0.4s    \n",
            "\n",
            "2022-01-29 17:04:30 (2.62 MB/s) - ‘57654-0.txt’ saved [1131399/1131399]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget \"https://www.gutenberg.org/files/57654/57654-0.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_MSI2ySRthF",
        "outputId": "d5faaeef-929a-4ed3-f038-d14fafab6e97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 33.8 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20 kB 21.1 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30 kB 17.2 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 40 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 51 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 61 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 71 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 81 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 92 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 102 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 112 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 122 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 133 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 143 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 153 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 163 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 174 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 184 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 194 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 204 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 215 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 225 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235 kB 9.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install unidecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZTBdWWP1BSWC"
      },
      "outputs": [],
      "source": [
        "import codecs\n",
        "from bs4 import BeautifulSoup\n",
        "import unidecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lv7154iU-xLD"
      },
      "outputs": [],
      "source": [
        "list_books = ['2000-0', '62870-0', '64465-0', '6528-0', '57654-0']\n",
        "text_add = ''\n",
        "for b in list_books:\n",
        "  with open(b + '.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "    info_beg = text.find('Title: ')\n",
        "    begin = text.find('*** START OF THIS PROJECT GUTENBERG EBOOK ')\n",
        "    title = ''\n",
        "    if begin > -1:\n",
        "      for i in text[info_beg : begin].split('\\n'):\n",
        "        if i.find('Title: ') > -1:\n",
        "          title = i.split('Title: ')[1]\n",
        "          begin_2 = begin + 42 + len(title) + 3\n",
        "                \n",
        "    else:\n",
        "      begin = text.find('START OF THE PROJECT GUTENBERG EBOOK ')\n",
        "      for i in text[info_beg : begin].split('\\n'):\n",
        "        if i.find('Title: ') > -1:\n",
        "          title = i.split('Title: ')[1]\n",
        "          begin_2 = begin + 37 + len(title) + 3\n",
        "\n",
        "    end = text.find('*** END OF THIS PROJECT GUTENBERG EBOOK')\n",
        "    if end == -1:\n",
        "      end = text.find('*** END')\n",
        "    text = text[begin_2 : end]\n",
        "\n",
        "    #remove html tags from text\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    text = soup.get_text(separator=\" \")\n",
        "    \n",
        "    #remove accented characters from text, e.g. café\n",
        "    text = unidecode.unidecode(text)\n",
        "\n",
        "    text_add = text_add + text + '\\n'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZ1ufTF8-xjI",
        "outputId": "6a097462-4b6c-42df-a6f2-e62a67c32c15"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "172417"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "len(text.split(' '))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8oLzuNb15tpf"
      },
      "outputs": [],
      "source": [
        "# Create two dictionaries:\n",
        "# int2char -- maps integers to characters\n",
        "# char2int -- maps characters to unique integers\n",
        "\n",
        "int2char = dict(enumerate(set(text)))\n",
        "char2int = { val: idx  for idx, val in int2char.items()}\n",
        "\n",
        "# encode the text\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vi0bo7KDMxFx",
        "outputId": "b90476bf-90e6-4087-a6e1-a50580ca9582"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "82"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "len(char2int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "QxFS2B9nD-kg"
      },
      "outputs": [],
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    \n",
        "    # arr - array of integers\n",
        "    # n_labels - number of labels (the size of a one-hot-encoded vector)\n",
        "\n",
        "    one_hot = np.eye(n_labels)[arr]\n",
        "    \n",
        "    return one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrcsQ3qVECzT",
        "outputId": "bf8a5292-230b-453e-d50e-309e8adc6590"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]]]\n"
          ]
        }
      ],
      "source": [
        "# check that the function works correctly\n",
        "test_indx = np.array([[7, 2, 5]])\n",
        "one_hot = one_hot_encode(test_indx, 8)\n",
        "\n",
        "print(one_hot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Q-0ZuGZhEFd_"
      },
      "outputs": [],
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    # Create a generator that returns batches of size batch_size x seq_length\n",
        "    \n",
        "    batch_size_total = batch_size * seq_length\n",
        "    ## Get the number of batches we can make\n",
        "    n_batches = int(len(arr) / batch_size_total)\n",
        "    \n",
        "    ## Keep only enough characters to make full batches\n",
        "    arr = arr[: n_batches*batch_size_total] \n",
        "    \n",
        "    ## Reshape into batch_size rows\n",
        "    arr = arr.reshape((batch_size, -1)) \n",
        "    \n",
        "    ## Iterate over the batches using a window of size seq_length\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        x = arr[:, n : n + seq_length ]\n",
        "        # The target is a version of x shifted by one (do not forget border conditions)\n",
        "        y = np.zeros_like(x)\n",
        "        y[:, :-1] = x[:, 1:]\n",
        "        if n + seq_length < arr.shape[1]:\n",
        "            y[:, -1] = arr[:, n + seq_length]\n",
        "        else:\n",
        "            arr[:, 0]\n",
        "        yield x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4CCx96r-ESVp"
      },
      "outputs": [],
      "source": [
        "batches = get_batches(encoded, 4, 30)\n",
        "x, y = next(batches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcefjsfQEXyg",
        "outputId": "d89a1adb-40a3-4bdd-d1cd-ae6b6deb8079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on GPU\n"
          ]
        }
      ],
      "source": [
        "# check if GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU')\n",
        "else: \n",
        "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "q9YXlR9GEa3L"
      },
      "outputs": [],
      "source": [
        "class CharRNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
        "                               drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        \n",
        "        # creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        # Define the LSTM layer\n",
        "        ## YOUR CODE HERE\n",
        "        self.lstm = nn.LSTM(input_size=len(self.chars), hidden_size=self.n_hidden, \n",
        "                            num_layers=self.n_layers, dropout=self.drop_prob, \n",
        "                            batch_first=True)\n",
        "\n",
        "        # Define a dropout layer\n",
        "        ## YOUR CODE HERE\n",
        "        self.dropout = nn.Dropout(self.drop_prob)\n",
        "\n",
        "        # Define the final, fully-connected output layer\n",
        "        ## YOUR CODE HERE\n",
        "        self.linear = nn.Linear(in_features=self.n_hidden, out_features=len(self.chars))\n",
        "      \n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        ''' Forward pass through the network. \n",
        "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
        "                \n",
        "        # Get the outputs and the new hidden state from the lstm\n",
        "        ## YOUR CODE HERE\n",
        "        out, hidden = self.lstm(x.float(), hidden)\n",
        "\n",
        "        # Pass through a dropout layer\n",
        "        ## YOUR CODE HERE\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        # Stack up LSTM outputs using view\n",
        "        # you may need to use contiguous to reshape the output\n",
        "        ## YOUR CODE HERE\n",
        "        out = out.reshape(out.size()[0] * out.size()[1], self.n_hidden)\n",
        "\n",
        "\n",
        "        # Put x through the fully-connected layer\n",
        "        ## YOUR CODE HERE\n",
        "        out = self.linear(out)\n",
        "\n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        \n",
        "        return hidden    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "6h3NSG5kGIuo"
      },
      "outputs": [],
      "source": [
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    ''' Training a network \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "    \n",
        "    '''\n",
        "    net.train()\n",
        "    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            if(train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            net.zero_grad()\n",
        "            \n",
        "            output, h = net(inputs, h)\n",
        "            \n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward()\n",
        "\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "            \n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                net.train() # reset to train mode after iterationg through validation data\n",
        "                now = datetime.now()\n",
        "                dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)),\n",
        "                      \"Time: \", dt_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpIky4vhGI0X",
        "outputId": "5299e41f-43b4-4ad5-b416-e5ff5c2ec9ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(82, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (linear): Linear(in_features=512, out_features=82, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "n_hidden = 512 \n",
        "n_layers = 2\n",
        "\n",
        "chars = tuple(set(text))\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "pUQx6NIPSQza"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/2022-01-15_Course_project/LSTM_generator_es/new_dict_chars.pickle', 'wb') as f:\n",
        "    pickle.dump(chars, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "dNxzMAoDULi9"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/2022-01-15_Course_project/LSTM_generator_es/new_dict_chars.pickle', 'rb') as f:\n",
        "  c = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fH3lwyzAULlc",
        "outputId": "7c8a9815-2d80-40c3-89d5-4e2f7906f056"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('A', '0', 'Z', 'Q', '5', '\\n', 'l', '[', 'K', 's', 'I', 'u', 'E', 'x', 'G', '?', 'z', '6', '<', '*', ' ', 'C', '8', 'H', '4', 'j', 't', 'R', 'h', 'm', 'P', '3', 'O', 'i', '/', '7', 'a', ',', 'V', 'c', 'p', ')', 'M', ';', '.', 'd', '!', 'k', 'f', 'y', \"'\", 'W', 'w', 'X', 'n', 'D', 'U', 'N', 'e', 'F', '1', 'B', '>', 'r', 'T', '9', 'o', ':', 'Y', ']', 'q', 'v', 'S', '-', '2', 'L', 'J', 'b', '\"', '(', 'g', '_')\n"
          ]
        }
      ],
      "source": [
        "print(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UA8rn_7GI7j",
        "outputId": "1cb18ada-7d38-4e39-f09c-c2aa92c0a677"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 14/80... Step: 500... Loss: 1.8864... Val Loss: 1.7935 Time:  29/01/2022 17:06:36\n",
            "Epoch: 28/80... Step: 1000... Loss: 1.6097... Val Loss: 1.4722 Time:  29/01/2022 17:07:25\n",
            "Epoch: 41/80... Step: 1500... Loss: 1.3666... Val Loss: 1.3412 Time:  29/01/2022 17:08:13\n",
            "Epoch: 55/80... Step: 2000... Loss: 1.2512... Val Loss: 1.2769 Time:  29/01/2022 17:09:01\n",
            "Epoch: 68/80... Step: 2500... Loss: 1.1633... Val Loss: 1.2759 Time:  29/01/2022 17:09:50\n"
          ]
        }
      ],
      "source": [
        "batch_size = 256\n",
        "seq_length = 100 \n",
        "n_epochs = 80\n",
        "\n",
        "# train the model\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "oPfr5ScHHndn"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/MyDrive/2022-01-15_Course_project/LSTM_generator_es/new_text_generator.pt\" \n",
        "torch.save(net.state_dict(), path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6SnXShsKOHh",
        "outputId": "50f0aaa7-879c-43e2-b027-058c6b4b8102"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "path = \"/content/drive/MyDrive/2022-01-15_Course_project/LSTM_generator_es/new_text_generator.pt\" \n",
        "\n",
        "\n",
        "with open('/content/drive/MyDrive/2022-01-15_Course_project/LSTM_generator_es/new_dict_chars.pickle', 'rb') as f:\n",
        "    chars_2 = pickle.load(f)\n",
        "\n",
        "n_hidden = 512  \n",
        "n_layers = 2\n",
        "net_2 = CharRNN(chars_2, n_hidden, n_layers)\n",
        "net_2.load_state_dict(torch.load(path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9Cq8T_XPkD-",
        "outputId": "bfdfe373-d35d-44ba-a911-f5b6b8c87739"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CharRNN(\n",
              "  (lstm): LSTM(82, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (linear): Linear(in_features=512, out_features=82, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "net_2.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "uex89qSKP-v_"
      },
      "outputs": [],
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        \n",
        "        # tensor inputs\n",
        "        x = np.array([[net.char2int[char]]])\n",
        "        x = one_hot_encode(x, len(net.chars))\n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        # detach hidden state from history\n",
        "        h = tuple([each.data for each in h])\n",
        "        # get the output of the model\n",
        "        out, h = net(inputs, h)\n",
        "\n",
        "        # get the character probabilities\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "        \n",
        "        # get top characters\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        # select the likely next character with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return net.int2char[char], h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "RnK_hi4vQAEq"
      },
      "outputs": [],
      "source": [
        "def sample(net, size, prime='The', top_k=None):\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval() # eval mode\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-9Gt8-DQDjR",
        "outputId": "48c83d2f-7d4e-4aa8-e149-8c8ba109d871"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ahora pase el personaje anterior y obtenga uno nuevo,\n",
            "los enemigos y amenazadores, ni dibo las provercidas de\n",
            "verguence con su amigo de igual vasta Hector, que padren a los\n",
            "correntes cuantos cerca en la ciera, arranto la loniza o para\n",
            "el mar librarando de la guerra. Ojola vrenejar al dios dinciendo\n",
            "a Tidis, a quien te simo tiene valerosa vido, codo dia frente, menerio. Jumite\n",
            "en la tierra, les exhorto en sus ojos en la parpes facilmente, y\n",
            "dejale al eter y al rio en sus miguras mas ardelicieste.>>\n",
            "\n",
            "442 Respondiole Hector, caro a Jupiter, que lleva la egida! !Con\n",
            "ena que los demas fertines menos! Pero, dejandoso Aquiles, les\n",
            "quitaron las filas cuando aquellas tomando los briylantes ojos,\n",
            "como lo que se ocurate. Y anfian por las diosas\n",
            "orados de buen dentido clavadas al peso veloz mas proponiose de\n",
            "este que fueron muerto; cual destruido por su agito par tantos desapidas\n",
            "mejores; y tan veloras semejantes alerados de lo que prentaparame\n",
            "fagilando vengo al luchas companero Pona internada, y las\n",
            "falanges donde dirigiose lireiente en el pecho en la sonria. Aquiles ha\n",
            "diose la llaman a Hector que salto del hombro y con ella y las deyasas\n",
            "no permanece hasta que se quedo ni los bajeles.>>\n",
            "\n",
            "452 Tal fue yo que remoso corria cuando el cadaver aguarda en\n",
            "acariera se habia la parte seria de ti. Mas asi que fueren que llenas de\n",
            "su casa, le dificil de un bronce en el alma y los volviendo furor. Desaiga\n",
            "el resplanderido, ne logro presentar con pallidas con el\n",
            "ordeno de la muerte, pero le hicio en el alcance a Aquiles, el de los\n",
            "pies ligeros pere tino a los danaos. Cuanto los separa en el\n",
            "heroe con la batalla, el pie del aguerrido Looco, hijo de Tideo,\n",
            "dirigiendole la indempica carroda con el mismo magnifico, lreporondo\n",
            "otan o para el adaz del cerro. Creo, vieron con intentos pelean. Y\n",
            "en alto cima de los eleventados matora los mortales sensa\n",
            "por mizArado, en la lin para lejas con los ejercotos vivos permiientes,\n",
            "corrados de la crisa tierra en las ordenes de las anchos otros.\n",
            "\n",
            "91 Decidme ahora, admiraronse de la colera al verse valle y\n",
            "puede el ancho de los vorciles desocejos: alla en voz placharono.\n",
            "Los aqueos y heridos tropados en el pie, ni el mismo Adrasto y\n",
            "entrogo del carro y fueste sin temblar y mucho. Hector cuerpo a\n",
            "su hijo, con gran porcion de los muros de Priamo. Y por la punta de\n",
            "la nave para nosotros sin yos y amigos por aun mas mando y el\n",
            "corazon exalicios y volveria al desulcin.>>\n",
            "\n",
            "204 Respondiole el pedro de los dioses inmortales! ?Segun diselval\n",
            "de las manos del pecho, y estos ya nosotros vio su corazon\n",
            "y derrama auna de tras casas, quienes dicho que sin trabar los\n",
            "mismos como ni a Hector; mi valor de los aqueos sobre el cielo y\n",
            "hiliendo a aquel guerrero para que podria tener en el fertil coro. Cuando\n",
            "este divine el cuerpo de Talamonio y la nueva; solo tendia de\n",
            "pie hasta el boga con Astrocio., el pecho, aunque sea al Opaleo, escaruado\n",
            "al muerto como antes de prupera valerosamentie al luciente combate por todas\n",
            "partes. Simos el combate y, como cogia mata. No te ha salido de sur encono de la muchedumbre\n",
            "que ha reunido por el pelio de su ruega y ombrio a Hector salto o\n",
            "la pica, hasta al vistos. La dusco al predice es alguna tenida para\n",
            "amedrerarle, causandose con voz canada, luego diestros en la vida.\n",
            "Y si permiteo, encontramento y pasaron el carro de un estrepito.\n",
            "Hector le hablo llegado ante los teucros llevados los miembros. Eneos he\n",
            "hallado de bronce que dio a su vez y peliente; la varan hacia aquel llora\n",
            "contiguo la vida; de cien muchos campos ni manana; de las mujeres\n",
            "con socorres y ricoros y no consituiemos a los teucros por la muerte de\n",
            "Hoceda. Asi heca despona la coraza de rocios vicos. !Antes que es\n",
            "mucho desde lo divino Aquiles levanto el heroe, al alcance\n",
            "tu cabeza una de los solipedos corceles y los hombres sempitarnos del\n",
            "volvermante a las inmortales, y rechazaron con companeras\n",
            "delanteros y por capatar el pecho, sin dejar a combatir a\n",
            "los muertos, y con alumre, y recibiendole entre los troyanos y\n",
            "eran cantos de la fusera del Patroclo, cayeron a nombrerle. Aquiles\n",
            "en todo se los pudo tu cira.>>\n",
            "\n",
            "295 Dijo, y lo dejo el ceudido, venas despues de los argivos,\n",
            "extiendole el padre, permenecia anta cimpad por el dia entre los\n",
            "mortales para algui oigo en la ciudad, y al flechador Apolo, el\n",
            "de los pies ligeros; y ya que era diciendo en la ciudad del muro\n",
            "y para mi siguiendo volvera pelo las armas con traves del Olimpo, sin aquel\n",
            "vocitara. Defiendale inmovilleste sobre el magnanimo Petis, hijo de Eyao,\n",
            "igual al terrible, vuelviente en el corazon estaban par en la deguira\n",
            "las rodillas del combate del belicoso Julete. Pero llevado el volve\n",
            "de Astanteo, fuera del cielo. Y amonentada ain que yo se lagora saltamos\n",
            "para la lanza a Todos, dandole y otros la lucibas corazanes irnangestas de\n",
            "prestal retrocedo. !Polerena! No queres acaba cuando, nueve hamisata!\n",
            "Eso incitaba, y hicieron medie en las concavas naves, y a mi\n",
            "se dispuesta y se encaminaba nunca, y ya, rendido a su\n",
            "hermano y caudillo, que haria esta palabra, hacer piedra de una\n",
            "haz para aflirido. La diosa dejalmos fuera del artido de la ceranda; hasta\n",
            "que el mismo le hizo volver a cuya selva aguaja al heroe, levantose el\n",
            "primero en orden al suelo, y principio a los irmas y ne iban todas\n",
            "contra Aquiles. Y el ejercito, sin despidierno pienda con supara de\n",
            "ciudades. Porque la cual sin hombres patarnos al que, puede lechor a la\n",
            "ciudad y a los teucros. Entonces descansaba de las ancianas y que vendien\n",
            "de las salcas, ablando a los troyanos que y delante de los\n",
            "mortales, tendiendolos animosos en las cimas y foco\n",
            "en la gente nos relidida y espantaban pergonces en el lucharon\n",
            "tanto a su vez al proposito de oro. Agitaba a Aquiles permife\n",
            "con funestas palabras. !PiAdro porque Afenor al olco desde la\n",
            "hubiese me corpalento habia acabadado, volo con el profindo Acascate!\n",
            "?Hobrilente, y vio y la velera nave no acierte de corazon de marcos a Hector,\n",
            "afligidos como los teucros, el inclito Mecton, que quitara la vida, y todavia\n",
            "el mar, y se levantaron con hombres cayo muchas divinas que comparaban\n",
            "al combate. Con el saivo dos lanzas, valerosos, y para decir,\n",
            "rechozaba a las piernas y muchos ercados, pues los danaos inmontares.\n",
            "Como un bronce, mas velemos el galado combate. Pero a todos los cuales\n",
            "hombres mandaban finmes las mortales, a quien estaba cerca, hasta que\n",
            "formara al terror y hubiera constado en arrojor con el sueno de\n",
            "la corriente y de iruelo; hasta que poseian la armadura de un hombro\n",
            "a tiene en el combate a la pica, siguiendole con la lanza un bote en\n",
            "la negra trave para que podea debajo del batalla:\n",
            "\n",
            "639 <<!Eneas! Cuando cendijos a la tienda de Idomeneo.\n",
            "\n",
            "463 El divino Aquiles no furio el cadaver, sin permuturse al\n",
            "cantallero, lo mababan con lastieles para no concedera in\n",
            "en un agumo y fuera del dirigo Astandio. Mientras el rey como dura\n",
            "tu corazon, a quienes apigaron dijo:\n",
            "\n",
            "548 <<Ciudad con Aquiles, hierro armisas; que la vida por la tarpa tencia la\n",
            "conacinia los tuvos de el y para mastrar el partipo que pudo siempre he\n",
            "de saguro la bordada cumbre, dejo de tener con fistamente cono y\n",
            "combatiera; con el arco pueblo derribo del carro, cemple soberneo, hizo de\n",
            "insolerte para ocultor y quienes exillando en una flecha con una suerte\n",
            "para las densas acamentaras y el muso dirocito, y todas solipedos\n",
            "vicos, acabaron en el campamento manda, y el corazon las rodillas y\n",
            "les siguieron disporillas en sus corazanes, y nuevan por el arco de\n",
            "estarla.>>\n",
            "\n",
            "820 De esta suerte hablo, no esparcio por un hombre. Da esta fartal\n",
            "saber, Pileo, acudio con bigar y les presentaron, el divino\n",
            "Aquiles, al terrible piel cara, y reprochas a los pueblos,\n",
            "y festiniendo a los troyanos y hombres con vinajas que fuerzas: los\n",
            "pueblos los afugia que se decia antes permonecia, dimo andelaron,\n",
            "haciendole caminos que ruego a los aqueos que pronto los dos, apartaban\n",
            "de la misma entonces a dor y sus cimosas, pues no cugo tu eplo.>>\n",
            "\n",
            "328 Asi hablo; alla nosatros lo cuales aunque teneas pelearos\n",
            "y a mi acioses mangos. Alli que me encaeno mucho o un venos y ora\n",
            "me realiza. Pero trae hombres que has a\n"
          ]
        }
      ],
      "source": [
        "# Example of generated text\n",
        "print(sample(net, 8000, prime='Ahora pase el personaje anterior y obtenga uno nuevo', top_k=20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "fU3wXKAJir0F"
      },
      "outputs": [],
      "source": [
        "text_ph = text.split('. ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "v_cULwHCL5WZ"
      },
      "outputs": [],
      "source": [
        "list_b1 = [ i.split(' ')[0].lower() for i in text_ph]\n",
        "list_b2 = [ ' '.join(j.lower() for j in i.split(' ')[: 2]) for i in text_ph]\n",
        "list_b3 = [ ' '.join(j.lower() for j in i.split(' ')[: 3]) for i in text_ph]\n",
        "list_b4 = [ ' '.join(j.lower() for j in i.split(' ')[: 4]) for i in text_ph]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM_fi5jCnISh",
        "outputId": "058b040c-022b-4376-b1fc-634641e0d949"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('y', 331),\n",
              " ('el', 155),\n",
              " ('pero', 112),\n",
              " ('los', 81),\n",
              " ('como', 78),\n",
              " ('mas', 69),\n",
              " ('no', 66),\n",
              " ('en', 62),\n",
              " ('nombre', 59),\n",
              " ('a', 56)]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "Counter(list_b1).most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Vsg1vD0jm3Kf"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "6s0PlDHjkN6n"
      },
      "outputs": [],
      "source": [
        "list_of_begins = set()\n",
        "for i in Counter(list_b1).most_common(10):\n",
        "  list_of_begins.add(re.sub(\"[^A-Za-z] \", \"\", i[0]).capitalize())\n",
        "\n",
        "for i in Counter(list_b2).most_common(10):\n",
        "  list_of_begins.add(re.sub(\"[^A-Za-z] \", \"\", i[0]).capitalize())\n",
        "\n",
        "for i in Counter(list_b3).most_common(10):\n",
        "  list_of_begins.add(re.sub(\"[^A-Za-z] \", \"\", i[0]).capitalize())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O36e-_TSnB4D",
        "outputId": "e73f612c-b22b-4b26-e347-19a8395c9cdd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " 'A',\n",
              " ' ',\n",
              " 'El guerrero cayo',\n",
              " 'Y en',\n",
              " 'Eaobremos todos',\n",
              " 'Mas ?por que',\n",
              " 'Mas',\n",
              " 'Y ahora',\n",
              " 'Y el',\n",
              " 'Nombre patronimico de\\n',\n",
              " 'Y',\n",
              " 'Pero',\n",
              " 'Y si',\n",
              " 'Nombre patronimico de',\n",
              " 'Fue muerto por',\n",
              " 'Como el',\n",
              " 'El',\n",
              " 'El hijo de',\n",
              " 'Como',\n",
              " 'Los',\n",
              " 'Pero no',\n",
              " 'Y cuando',\n",
              " 'No',\n",
              " 'El atrida',\n",
              " 'Nombre',\n",
              " 'Nombre patronimico',\n",
              " 'En']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "list(list_of_begins)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "KQENaOqFYGNA"
      },
      "outputs": [],
      "source": [
        "# Removing the empty starting characters\n",
        "list_of_begins.remove('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "CHjyZoRUM-Sn"
      },
      "outputs": [],
      "source": [
        "# Removing the empty starting characters\n",
        "list_of_begins.remove(' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otKbQCMrNSi7",
        "outputId": "54910f23-2ba5-47e1-ad7a-a890c8e7c025"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A',\n",
              " 'El guerrero cayo',\n",
              " 'Y en',\n",
              " 'Eaobremos todos',\n",
              " 'Mas ?por que',\n",
              " 'Mas',\n",
              " 'Y ahora',\n",
              " 'Y el',\n",
              " 'Nombre patronimico de\\n',\n",
              " 'Y',\n",
              " 'Pero',\n",
              " 'Y si',\n",
              " 'Nombre patronimico de',\n",
              " 'Fue muerto por',\n",
              " 'Como el',\n",
              " 'El',\n",
              " 'El hijo de',\n",
              " 'Como',\n",
              " 'Los',\n",
              " 'Pero no',\n",
              " 'Y cuando',\n",
              " 'No',\n",
              " 'El atrida',\n",
              " 'Nombre',\n",
              " 'Nombre patronimico',\n",
              " 'En']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "list(list_of_begins)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Pad0hALYpnUB"
      },
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITJIToF2pLUC",
        "outputId": "68c5bc86-2003-432f-c3d2-36db4e1fecbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29/01/2022 06:59:52 |  number of processed files: 0 0.0 %\n",
            "29/01/2022 07:13:32 |  number of processed files: 200 2.0 %\n",
            "29/01/2022 07:27:11 |  number of processed files: 400 4.0 %\n"
          ]
        }
      ],
      "source": [
        "\n",
        "num_docs = 10000\n",
        "\n",
        "path = '/content/drive/MyDrive/2022-01-15_Course_project/new_gen_text_es/'\n",
        "\n",
        "for i in range(num_docs):\n",
        "  start = random.choice(list(list_of_begins))\n",
        "  text_gen = sample(net, 8000, prime= start, top_k=10)\n",
        "\n",
        "  with open(path + 'doc_'+str(i)+'.txt', 'w+') as output_file:\n",
        "    try:\n",
        "      output_file.write(text_gen)\n",
        "    except:\n",
        "      continue\n",
        "  if i % 200 == 0:\n",
        "    now = datetime.now()\n",
        "    dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
        "    print(dt_string, '| ',  'number of processed files: ' + str(i), round(i/num_docs, 3)*100, '%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNPcLUAhspTR",
        "outputId": "5068f93a-aeb0-43e4-ecc2-3b0ea4db27a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5883, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "# If the generator does not work completely, you need to run this code (it will start from the last place)\n",
        "\n",
        "# This is the code to generate (to determine how many have already been generated)\n",
        "\n",
        "list_gen = glob.glob('/content/drive/MyDrive/2022-01-15_Course_project/new_gen_text_es/*.txt')\n",
        "list_num = [ int(i.split('/')[-1].split('.txt')[0].split('doc_')[-1]) for i in list_gen]\n",
        "max(list_num), min(list_num)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "YP-76wSBhHsb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51fdb2a7-ae70-4833-a76d-0d84a320f05e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29/01/2022 17:29:20 |  number of processed files: 6000 60.0 %\n",
            "29/01/2022 17:42:47 |  number of processed files: 6200 62.0 %\n",
            "29/01/2022 17:56:13 |  number of processed files: 6400 64.0 %\n",
            "29/01/2022 18:09:39 |  number of processed files: 6600 66.0 %\n",
            "29/01/2022 18:23:07 |  number of processed files: 6800 68.0 %\n",
            "29/01/2022 18:36:35 |  number of processed files: 7000 70.0 %\n",
            "29/01/2022 18:50:04 |  number of processed files: 7200 72.0 %\n",
            "29/01/2022 19:03:32 |  number of processed files: 7400 74.0 %\n",
            "29/01/2022 19:17:00 |  number of processed files: 7600 76.0 %\n",
            "29/01/2022 19:30:26 |  number of processed files: 7800 78.0 %\n",
            "29/01/2022 19:43:51 |  number of processed files: 8000 80.0 %\n",
            "29/01/2022 19:57:16 |  number of processed files: 8200 82.0 %\n",
            "29/01/2022 20:10:41 |  number of processed files: 8400 84.0 %\n",
            "29/01/2022 20:24:06 |  number of processed files: 8600 86.0 %\n",
            "29/01/2022 20:37:31 |  number of processed files: 8800 88.0 %\n",
            "29/01/2022 20:50:56 |  number of processed files: 9000 90.0 %\n",
            "29/01/2022 21:04:20 |  number of processed files: 9200 92.0 %\n",
            "29/01/2022 21:17:47 |  number of processed files: 9400 94.0 %\n",
            "29/01/2022 21:31:13 |  number of processed files: 9600 96.0 %\n",
            "29/01/2022 21:44:39 |  number of processed files: 9800 98.0 %\n"
          ]
        }
      ],
      "source": [
        "# If the generator does not work completely, you need to run this code (it will start from the last place)\n",
        "\n",
        "num_docs = 10000\n",
        "\n",
        "path = '/content/drive/MyDrive/2022-01-15_Course_project/new_gen_text_es/'\n",
        "\n",
        "for i in range(max(list_num), num_docs, 1):\n",
        "  start = random.choice(list(list_of_begins))\n",
        "  text_gen = sample(net, 8000, prime= start, top_k=10)\n",
        "\n",
        "  with open(path + 'doc_'+str(i)+'.txt', 'w+') as output_file:\n",
        "    try:\n",
        "      output_file.write(text_gen)\n",
        "    except:\n",
        "      continue\n",
        "  if i % 200 == 0:\n",
        "    now = datetime.now()\n",
        "    dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
        "    print(dt_string, '| ',  'number of processed files: ' + str(i), round(i/num_docs, 3)*100, '%')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "2_Generation_es.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}