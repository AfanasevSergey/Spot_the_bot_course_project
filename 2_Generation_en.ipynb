{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "SXghWytGaz4E"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from google.colab import drive\n",
    "import os, zipfile, glob\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8gGPuN1LbDYP",
    "outputId": "e70172f4-0ec7-4fbb-a8ad-6e91e20af98a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0b3Xp-FOR7h9"
   },
   "outputs": [],
   "source": [
    "# English books from the gutenberg.org for text generation (top 5 in volume with the right encoding)\n",
    "'14860-0'\n",
    "'11100-0'\n",
    "'14140-0'\n",
    "'16847-0'\n",
    "'18762-0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MNng6ICsStYj",
    "outputId": "7e086df8-5dd8-4af0-c079-858cc8a6b48f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-17 17:47:06--  https://www.gutenberg.org/files/14860/14860-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1919120 (1.8M) [text/plain]\n",
      "Saving to: ‘14860-0.txt.2’\n",
      "\n",
      "14860-0.txt.2       100%[===================>]   1.83M  3.98MB/s    in 0.5s    \n",
      "\n",
      "2022-01-17 17:47:06 (3.98 MB/s) - ‘14860-0.txt.2’ saved [1919120/1919120]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://www.gutenberg.org/files/14860/14860-0.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9-wqBHK4SfoT",
    "outputId": "a2efd154-af74-4ca9-ea79-e41b7bb1ce2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-17 17:47:09--  https://www.gutenberg.org/files/11100/11100-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1548962 (1.5M) [text/plain]\n",
      "Saving to: ‘11100-0.txt.2’\n",
      "\n",
      "11100-0.txt.2       100%[===================>]   1.48M  3.74MB/s    in 0.4s    \n",
      "\n",
      "2022-01-17 17:47:10 (3.74 MB/s) - ‘11100-0.txt.2’ saved [1548962/1548962]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://www.gutenberg.org/files/11100/11100-0.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JFhljiMVSfrC",
    "outputId": "2774fce6-f05a-41a7-cfd0-663456fd888c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-17 17:47:11--  https://www.gutenberg.org/files/14140/14140-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1379661 (1.3M) [text/plain]\n",
      "Saving to: ‘14140-0.txt.2’\n",
      "\n",
      "14140-0.txt.2       100%[===================>]   1.32M  3.33MB/s    in 0.4s    \n",
      "\n",
      "2022-01-17 17:47:12 (3.33 MB/s) - ‘14140-0.txt.2’ saved [1379661/1379661]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://www.gutenberg.org/files/14140/14140-0.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vS_TBqxKSft0",
    "outputId": "dbcbe293-c6db-432b-d70d-7f4d8b06b91c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-17 17:47:14--  https://www.gutenberg.org/files/16847/16847-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1340104 (1.3M) [text/plain]\n",
      "Saving to: ‘16847-0.txt.2’\n",
      "\n",
      "16847-0.txt.2       100%[===================>]   1.28M  3.24MB/s    in 0.4s    \n",
      "\n",
      "2022-01-17 17:47:15 (3.24 MB/s) - ‘16847-0.txt.2’ saved [1340104/1340104]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://www.gutenberg.org/files/16847/16847-0.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eUEKj3PySf3t",
    "outputId": "9aefa99e-3ffa-41a2-981e-bdb19a354244"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-17 17:47:16--  https://www.gutenberg.org/files/18762/18762-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1271852 (1.2M) [text/plain]\n",
      "Saving to: ‘18762-0.txt.2’\n",
      "\n",
      "18762-0.txt.2       100%[===================>]   1.21M  3.08MB/s    in 0.4s    \n",
      "\n",
      "2022-01-17 17:47:17 (3.08 MB/s) - ‘18762-0.txt.2’ saved [1271852/1271852]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://www.gutenberg.org/files/18762/18762-0.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w_MSI2ySRthF",
    "outputId": "7cd72144-7d81-45e6-ea9a-bd350cc4e4a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "ZTBdWWP1BSWC"
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "from bs4 import BeautifulSoup\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "lv7154iU-xLD"
   },
   "outputs": [],
   "source": [
    "list_books = ['14860-0', '11100-0', '14140-0', '16847-0', '18762-0']\n",
    "text_add = ''\n",
    "for b in list_books:\n",
    "  with open(b + '.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "    info_beg = text.find('Title: ')\n",
    "    begin = text.find('*** START OF THIS PROJECT GUTENBERG EBOOK ')\n",
    "    title = ''\n",
    "    if begin > -1:\n",
    "      for i in text[info_beg : begin].split('\\n'):\n",
    "        if i.find('Title: ') > -1:\n",
    "          title = i.split('Title: ')[1]\n",
    "          begin_2 = begin + 42 + len(title) + 3\n",
    "                \n",
    "    else:\n",
    "      begin = text.find('START OF THE PROJECT GUTENBERG EBOOK ')\n",
    "      for i in text[info_beg : begin].split('\\n'):\n",
    "        if i.find('Title: ') > -1:\n",
    "          title = i.split('Title: ')[1]\n",
    "          begin_2 = begin + 37 + len(title) + 3\n",
    "\n",
    "    end = text.find('*** END OF THIS PROJECT GUTENBERG EBOOK')\n",
    "    if end == -1:\n",
    "      end = text.find('*** END')\n",
    "    text = text[begin_2 : end]\n",
    "\n",
    "    #remove html tags from text\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text(separator=\" \")\n",
    "    \n",
    "    #remove accented characters from text, e.g. café\n",
    "    text = unidecode.unidecode(text)\n",
    "\n",
    "    text_add = text_add + text + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SZ1ufTF8-xjI",
    "outputId": "ebc9a428-651c-4b34-a727-200244b8ed73"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290243"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "8oLzuNb15tpf"
   },
   "outputs": [],
   "source": [
    "# Create two dictionaries:\n",
    "# int2char -- maps integers to characters\n",
    "# char2int -- maps characters to unique integers\n",
    "\n",
    "int2char = dict(enumerate(set(text)))\n",
    "char2int = { val: idx  for idx, val in int2char.items()}\n",
    "\n",
    "# encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vi0bo7KDMxFx",
    "outputId": "ee29ae61-8182-4c26-9d45-80c03b0c3f08"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(char2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "QxFS2B9nD-kg"
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # arr - array of integers\n",
    "    # n_labels - number of labels (the size of a one-hot-encoded vector)\n",
    "\n",
    "    one_hot = np.eye(n_labels)[arr]\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hrcsQ3qVECzT",
    "outputId": "af36d948-2ab5-420e-aec1-69da8277bc2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# check that the function works correctly\n",
    "test_indx = np.array([[7, 2, 5]])\n",
    "one_hot = one_hot_encode(test_indx, 8)\n",
    "\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "Q-0ZuGZhEFd_"
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    # Create a generator that returns batches of size batch_size x seq_length\n",
    "    \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    ## Get the number of batches we can make\n",
    "    n_batches = int(len(arr) / batch_size_total)\n",
    "    \n",
    "    ## Keep only enough characters to make full batches\n",
    "    arr = arr[: n_batches*batch_size_total] \n",
    "    \n",
    "    ## Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1)) \n",
    "    \n",
    "    ## Iterate over the batches using a window of size seq_length\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        x = arr[:, n : n + seq_length ]\n",
    "        # The target is a version of x shifted by one (do not forget border conditions)\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1] = x[:, 1:]\n",
    "        if n + seq_length < arr.shape[1]:\n",
    "            y[:, -1] = arr[:, n + seq_length]\n",
    "        else:\n",
    "            arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "4CCx96r-ESVp"
   },
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 4, 30)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GcefjsfQEXyg",
    "outputId": "11916390-4fa1-40d7-b91b-a2475bfb56a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "q9YXlR9GEa3L"
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        # Define the LSTM layer\n",
    "        ## YOUR CODE HERE\n",
    "        self.lstm = nn.LSTM(input_size=len(self.chars), hidden_size=self.n_hidden, \n",
    "                            num_layers=self.n_layers, dropout=self.drop_prob, \n",
    "                            batch_first=True)\n",
    "\n",
    "        # Define a dropout layer\n",
    "        ## YOUR CODE HERE\n",
    "        self.dropout = nn.Dropout(self.drop_prob)\n",
    "\n",
    "        # Define the final, fully-connected output layer\n",
    "        ## YOUR CODE HERE\n",
    "        self.linear = nn.Linear(in_features=self.n_hidden, out_features=len(self.chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        # Get the outputs and the new hidden state from the lstm\n",
    "        ## YOUR CODE HERE\n",
    "        out, hidden = self.lstm(x.float(), hidden)\n",
    "\n",
    "        # Pass through a dropout layer\n",
    "        ## YOUR CODE HERE\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # Stack up LSTM outputs using view\n",
    "        # you may need to use contiguous to reshape the output\n",
    "        ## YOUR CODE HERE\n",
    "        out = out.reshape(out.size()[0] * out.size()[1], self.n_hidden)\n",
    "\n",
    "\n",
    "        # Put x through the fully-connected layer\n",
    "        ## YOUR CODE HERE\n",
    "        out = self.linear(out)\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "6h3NSG5kGIuo"
   },
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                now = datetime.now()\n",
    "                dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)),\n",
    "                      \"Time: \", dt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dpIky4vhGI0X",
    "outputId": "8bd2e056-1616-4664-f61f-3f2e055abb3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(87, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (linear): Linear(in_features=512, out_features=87, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 512 \n",
    "n_layers = 2\n",
    "\n",
    "chars = tuple(set(text))\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "pUQx6NIPSQza"
   },
   "outputs": [],
   "source": [
    "with open('/content/drive/MyDrive/2022-01-15_Course_project/LSTM_generator/dict_chars.pickle', 'wb') as f:\n",
    "    pickle.dump(chars, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "dNxzMAoDULi9"
   },
   "outputs": [],
   "source": [
    "with open('/content/drive/MyDrive/2022-01-15_Course_project/LSTM_generator/dict_chars.pickle', 'rb') as f:\n",
    "  c = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fH3lwyzAULlc",
    "outputId": "1f4efc8a-8165-48c5-d9ca-c2608bc66b06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(';', 'J', 'E', '>', 'S', 'V', 'e', '6', 'R', 'K', '~', 'P', 'H', 'I', '*', ')', 'p', 'l', 'n', '0', '_', '/', 'F', 'k', '<', '\"', '?', ' ', 'A', 'O', 'T', 'g', 'W', 'L', \"'\", 'h', 'm', 'r', 'y', 'x', 's', '[', 'c', 'Z', 'q', '^', 'v', '\\\\', 'i', 'w', ']', '2', 'B', 'Y', 't', 'o', 'j', '-', 'N', '7', '(', '8', 'G', '.', '1', 'u', '\\n', 'D', '4', 'U', '3', 'f', ':', 'd', 'Q', 'a', 'b', 'C', ',', 'M', '{', '9', 'z', '!', 'X', '}', '5')\n"
     ]
    }
   ],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6UA8rn_7GI7j",
    "outputId": "f441fc3d-84a0-4295-e698-a23d353cab3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/50... Step: 500... Loss: 1.9644... Val Loss: 1.8510 Time:  17/01/2022 17:50:18\n",
      "Epoch: 24/50... Step: 1000... Loss: 1.6966... Val Loss: 1.5915 Time:  17/01/2022 17:51:07\n",
      "Epoch: 36/50... Step: 1500... Loss: 1.5369... Val Loss: 1.4719 Time:  17/01/2022 17:51:56\n",
      "Epoch: 48/50... Step: 2000... Loss: 1.4564... Val Loss: 1.4141 Time:  17/01/2022 17:52:45\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "seq_length = 100 \n",
    "n_epochs = 50\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "oPfr5ScHHndn"
   },
   "outputs": [],
   "source": [
    "path = \"/content/drive/MyDrive/2022-01-15_Course_project/LSTM_generator/text_generator.pt\" \n",
    "torch.save(net.state_dict(), path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R6SnXShsKOHh",
    "outputId": "9a5eba6f-b466-4108-fb37-aece15281b3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/content/drive/MyDrive/2022-01-15_Course_project/LSTM_generator/text_generator.pt\" \n",
    "\n",
    "\n",
    "with open('/content/drive/MyDrive/2022-01-15_Course_project/LSTM_generator/dict_chars.pickle', 'rb') as f:\n",
    "    chars_2 = pickle.load(f)\n",
    "\n",
    "n_hidden = 512  \n",
    "n_layers = 2\n",
    "net_2 = CharRNN(chars_2, n_hidden, n_layers)\n",
    "net_2.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m9Cq8T_XPkD-",
    "outputId": "8c957221-d687-4cbd-b983-0ca0f3627d54"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharRNN(\n",
       "  (lstm): LSTM(87, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (linear): Linear(in_features=512, out_features=87, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "uex89qSKP-v_"
   },
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "RnK_hi4vQAEq"
   },
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f-9Gt8-DQDjR",
    "outputId": "c3eeec2a-2b60-4bd7-8398-0ed338e258d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now pass in the previous character and get a new onesman,\n",
      "or all, nor somewwere suvery pirlut to murrey miyd you renuties; on\n",
      "the sunce less would betroute the rantrom did the dame\n",
      "(in _Hosry_, his vioter was sturn to streected of an centrementon\n",
      "\"to Mosea still,--but you flowning the let of through\"\n",
      "and among but the land and in my dray, etc., and he proposed to inceward all\n",
      "piercesion that to that he had not called in a momation complate, thear souls in the\n",
      "scinistime with what an irpentation to the buck and roveless of;\n",
      "if you cantalu the red incompased in this scaller's im many\n",
      "appelition, and in the forgance and by a for her soul's cime\n",
      "to disis a temole erople of the sweet in 1826; she say the\n",
      "_true flight decore_ (_Machin_, 1818, p. 405), and Counte,\n",
      "in 1858, old Accion, Fasting Mays, 1, 28, note 1.]\n",
      "\n",
      "{498}[mv]\n",
      "[---- _'ave eney in save_ the man's hels.\n",
      "\n",
      "                      X.\n",
      "\n",
      "    Juan, and whring, and dreads the fall-guind wonder\n",
      "      The world as stoption, how it of much shirs,\n",
      "    And dowred, with a lox consans of brid\n",
      "      Don over the histoor their store adds,\n",
      "    To blood a wumbly's and! a sartried care,\n",
      "      In late of being detination, who to gevere\n",
      "    Which were so stoble as to the ace senotion\n",
      "      That four the passion brhathing of a going\n",
      "    Of pessips charm, but waid of ron, belight\n",
      "      Had the esby chipks wfreant boy doubtet:\n",
      "    I'er all the connuring futh of brisdon--\n",
      "    A cateriture were preased of all the mirn.\n",
      "\n",
      "                      CV.\n",
      "\n",
      "    I wear--or, bose looked by the alextiny,\n",
      "      What even that extrest they were good some black,\n",
      "    Full earn wonder he set even to blow\n",
      "      And opinain: you wood non--we know never,\n",
      "    Appear lived not say--with her admeriat riming--\n",
      "    The exsestrence sayed like a crippiny bind,\n",
      "\n",
      "                      LXI.\n",
      "\n",
      "    But Juan, if no poous duch perpure pire;[336]\n",
      "      And most is been between a sold, so mode,\n",
      "    And some armisled even the fathers fell\n",
      "      Beem this he field in at the each fair song:\n",
      "    Enelames; in the gook of her most noper,\n",
      "      With a beard makes a set have improsed,\n",
      "    And name with her fine his took about,\n",
      "      I must be subport parage and sunment,\n",
      "    They ball, and al angars, which tather cast\n",
      "      From the costusassed, and recoped it gues,\n",
      "    And sutcessed which sees her ord were gled conton,\n",
      "      And rual hea, each gindaral ames lead,\n",
      "    And thinks is the redation, his grave dovers\n",
      "      In sper, more metions in the mopetur in\n",
      "    Fruves is which are to deflent to man,\n",
      "      And chanted but the skie fills and that spors\n",
      "    Utalfly her sword might lay--when time,\n",
      "      But he things no matters, gare with it misteries!\n",
      "    But they're right and bown of hers I have nered,\n",
      "      And sent the solds; whise is a damp, has abdet,\n",
      "    With ray out designed along a persion,\n",
      "      Which most same hatumery garteallets and foir,\n",
      "    And onfort with his comolest's fell\n",
      "      Of inliginacited of hope's hrow song;\n",
      "    Which shails, who, I thought expoped it wourd to pack;\n",
      "      Also all so two griad appruse a fear;\n",
      "    Not the which find are mount in hrassity some,\n",
      "    And were his eyes! to it a carring of the\n",
      "                      XLVI.\n",
      "\n",
      "    In fights that line and know her repetion, he\n",
      "      That an air not to this meet with boding;\n",
      "    A sam day form as speritation\n",
      "      O'er of cothing thar bound to ramm in its,\n",
      "    For one shall nay on, and not a sevinuge\n",
      "    Misparant her nocking same at least unweallit,\n",
      "    Latefless the poets with freshout respetion.\n",
      "\n",
      "                      XLI.\n",
      "\n",
      "    And she was whor her eyes to tear to sbofe!\n",
      "      So they she kind of geeing to his hast:\n",
      "      Took now my convictions of the mild----\n",
      "    Syem, but but new is muct ave valorred,\n",
      "      And no taling all his spare breatre before,\n",
      "    And in the cash we lovely daess neither deaverring,\n",
      "    Now have at ther men to be dufficled is--\n",
      "    A heng--but they as so for him--et barl.\n",
      "\n",
      "                      LXVIII.\n",
      "\n",
      "    This ond shalk, too by the ceart had persless; that,\n",
      "      Tate the isse outward which yet acted to\n",
      "    (I what you post of age, yet sad beauty deaved--\n",
      "      To be deep talk as a peralle and own?[679]\n",
      "    And, and her lets, and the tenre course in cure\n",
      "      Astere he rand their meltian succores place it--\n",
      "    For had men all, eall on the find that while\n",
      "      On the none leature than which with the friven blain;[ar]\n",
      "    For these died dombbry many a propicial twincion,\n",
      "      And fate as for tw  wiph the wricken but\n",
      "    For blyod-open of seed girdle still,\n",
      "    Some grattly tamity to all_ rach: too,.\n",
      "\n",
      "                      VI.\n",
      "\n",
      "    The whole teting high into do for fant:\n",
      "      Betind us takement hen to clay--the sun\n",
      "    The grand of Maisooners of Hagen; and\n",
      "      Of a true sensol idia high and vidy,\n",
      "    Which half that cave gors born on in untonaled;\n",
      "    As proy he to but temprocal old toctic is.\n",
      "\n",
      "                      IV.\n",
      "\n",
      "    Her cafoluned my, which you some vorce to griet,\n",
      "      The pistuens to stop that old featon incentre;\n",
      "    Whereat he his lambles suspat's been susued\n",
      "      The self at looks the bornor maved to rashed;\n",
      "    When the high awkward feath, and sad my many\n",
      "    And our the day are becume a suy of\n",
      "    Accordians for no good of chomabus,\n",
      "    And wach a mand may fencant she trude of puise.\n",
      "\n",
      "                      LXXX.\n",
      "\n",
      "    I cannot to the ismires liss, and came,\n",
      "      And you had of the woold and time the gayse:\n",
      "    And Juan she had no work or not's bod will have\n",
      "      To cire, even waved I thon I they he way seats:\n",
      "    What your alused, in tumple, and form regloss,\n",
      "    And spey was written adds may such discuse.\n",
      "\n",
      "                      LXIV.\n",
      "\n",
      "    Ohe pussed more mon-worngs, undir distact prant;\n",
      "      And as her words back roed: a trousperith, buk\n",
      "    By bullimant e'mach time get now bow\n",
      "      Sume told them fine for will so, and of all\n",
      "    Such she it a least dast in the through terp,\n",
      "      Or the lady sabet diaviculate,\n",
      "    Less romantom of as goos of seeds\n",
      "      (I have the even his prulest\"--peralt befare.\n",
      "    Then!--the mean of all centragenep as you and\n",
      "      And sau one which fame;--but in the place troug--\n",
      "    The beils she illifated with to compare:\n",
      "      \"Tre fill me moultssom's down, and ther his sray,\n",
      "    For from the bidded, eidlo, time? when the\n",
      "green almays_,\n",
      "      _Of rays of instartles in the daring_.--[MS. erased.]\n",
      "\n",
      "{441}[282] [\"Year he appeased of evilment,\" the best appealt turnod, and _memory from heaving have guessed_, by the foesory of the\n",
      "month and through with all surenessly compartanod. My had crepention appelmageled in a\n",
      "great trates, as he marrains ere for this without present classed have cabritled\n",
      "the farshey men, where any meant in a a worse of memory\n",
      "cacuties of the for hime the burned before his heart,--I ded. Norone. There\n",
      "was not persand, is rock of charmt in trace to all the\n",
      "dampected for the blighdering at little good into my coundry. But the\n",
      "while had good ptocation, when who would im, that no girt of rous that\n",
      "the excences that I never is un nivence with liveing comparal gouds. In acquest,\n",
      "and from stowed whose fortunitidy. The grous sola despersion and dishopious in the\n",
      "pattering grows of forteand by the moddancimons both take another\n",
      "a boak more parbet plared, \"English?\"--_Hist. De la Nouvelle Radser_, ii. 221.]\n",
      "\n",
      "[lb] _On I pray man, we'm a newlecean gord_--\n",
      "\n",
      "                      LXXXIX.\n",
      "\n",
      "    And he could the, a stars, and ammem she to say,\n",
      "      And twenty air on in for ten graw even,\n",
      "    Constiling to this ship she right reprosed,\n",
      "      And showlicl to sen men, and like care speers[60]\n",
      "    Of a dance of those higherovy, and dample\n",
      "      Nor sead that look her bodigh; in their still\n",
      "    Propised as put withstruct a adfacterial\n",
      "      Change in a lion con'trise and so let ency\n",
      "    Thing not began that belief furthingters, but\n",
      "    Through the cactit of Sun live were fact sight toes\n",
      "    Such at sight and mannerios; her eyes the nows.\n",
      "\n",
      "                      XXIII.\n",
      "\n",
      "    The little pitical trets of things beed tern\n",
      "      But hear he was fold so you have been her so\n"
     ]
    }
   ],
   "source": [
    "# Example of generated text\n",
    "print(sample(net, 8000, prime='Now pass in the previous character and get a new one', top_k=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "fU3wXKAJir0F"
   },
   "outputs": [],
   "source": [
    "text_ph = text.split('. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "v_cULwHCL5WZ"
   },
   "outputs": [],
   "source": [
    "list_b1 = [ i.split(' ')[0].lower() for i in text_ph]\n",
    "list_b2 = [ ' '.join(j.lower() for j in i.split(' ')[: 2]) for i in text_ph]\n",
    "list_b3 = [ ' '.join(j.lower() for j in i.split(' ')[: 3]) for i in text_ph]\n",
    "list_b4 = [ ' '.join(j.lower() for j in i.split(' ')[: 4]) for i in text_ph]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jM_fi5jCnISh",
    "outputId": "ed1664d8-93d1-4e5b-a8ad-4f08e58d53ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 115),\n",
       " ('sc', 74),\n",
       " ('he', 69),\n",
       " ('i', 58),\n",
       " ('line', 48),\n",
       " ('it', 39),\n",
       " ('in', 38),\n",
       " ('a', 37),\n",
       " ('lines', 35),\n",
       " ('de', 33)]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(list_b1).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "Vsg1vD0jm3Kf"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "6s0PlDHjkN6n"
   },
   "outputs": [],
   "source": [
    "list_of_begins = set()\n",
    "for i in Counter(list_b1).most_common(10):\n",
    "  list_of_begins.add(re.sub(\"[^A-Za-z] \", \"\", i[0]).capitalize())\n",
    "\n",
    "for i in Counter(list_b2).most_common(10):\n",
    "  list_of_begins.add(re.sub(\"[^A-Za-z] \", \"\", i[0]).capitalize())\n",
    "\n",
    "for i in Counter(list_b3).most_common(10):\n",
    "  list_of_begins.add(re.sub(\"[^A-Za-z] \", \"\", i[0]).capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O36e-_TSnB4D",
    "outputId": "b780af51-bac6-4b0b-8407-7fd96d32138b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He was',\n",
       " 'De',\n",
       " 'In',\n",
       " 'The',\n",
       " 'A',\n",
       " 'Pp',\n",
       " 'P',\n",
       " 'I',\n",
       " 'It is',\n",
       " 'Ii',\n",
       " 'He',\n",
       " 'De la',\n",
       " 'De la nouvelle',\n",
       " 'Ed',\n",
       " 'Lines',\n",
       " 'Line',\n",
       " 'Sc',\n",
       " 'Vi',\n",
       " 'It',\n",
       " 'Cap',\n",
       " 'Seetoo,']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(list_of_begins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "Pad0hALYpnUB"
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "ITJIToF2pLUC",
    "outputId": "a558bd57-f29f-4f75-8e10-6a23df34c77c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/01/2022 10:52:14 |  number of processed files: 0 0.0 %\n",
      "17/01/2022 11:05:44 |  number of processed files: 200 1.3 %\n",
      "17/01/2022 11:19:12 |  number of processed files: 400 2.7 %\n",
      "17/01/2022 11:32:24 |  number of processed files: 600 4.0 %\n",
      "17/01/2022 11:45:38 |  number of processed files: 800 5.3 %\n",
      "17/01/2022 11:58:52 |  number of processed files: 1000 6.7 %\n",
      "17/01/2022 12:12:09 |  number of processed files: 1200 8.0 %\n",
      "17/01/2022 12:25:24 |  number of processed files: 1400 9.3 %\n",
      "17/01/2022 12:38:43 |  number of processed files: 1600 10.7 %\n",
      "17/01/2022 12:52:02 |  number of processed files: 1800 12.0 %\n",
      "17/01/2022 13:05:22 |  number of processed files: 2000 13.3 %\n",
      "17/01/2022 13:18:42 |  number of processed files: 2200 14.7 %\n",
      "17/01/2022 13:32:08 |  number of processed files: 2400 16.0 %\n",
      "17/01/2022 13:45:32 |  number of processed files: 2600 17.299999999999997 %\n",
      "17/01/2022 13:58:44 |  number of processed files: 2800 18.7 %\n",
      "17/01/2022 14:11:58 |  number of processed files: 3000 20.0 %\n",
      "17/01/2022 14:25:15 |  number of processed files: 3200 21.3 %\n",
      "17/01/2022 14:38:31 |  number of processed files: 3400 22.7 %\n",
      "17/01/2022 14:51:50 |  number of processed files: 3600 24.0 %\n",
      "17/01/2022 15:05:10 |  number of processed files: 3800 25.3 %\n",
      "17/01/2022 15:18:31 |  number of processed files: 4000 26.700000000000003 %\n",
      "17/01/2022 15:31:54 |  number of processed files: 4200 28.000000000000004 %\n",
      "17/01/2022 15:45:17 |  number of processed files: 4400 29.299999999999997 %\n",
      "17/01/2022 15:58:41 |  number of processed files: 4600 30.7 %\n",
      "17/01/2022 16:12:05 |  number of processed files: 4800 32.0 %\n",
      "17/01/2022 16:25:29 |  number of processed files: 5000 33.300000000000004 %\n",
      "17/01/2022 16:38:56 |  number of processed files: 5200 34.699999999999996 %\n"
     ]
    }
   ],
   "source": [
    "num_docs = 15000\n",
    "\n",
    "path = '/content/drive/MyDrive/2022-01-15_Course_project/gen_text_en/'\n",
    "\n",
    "for i in range(num_docs):\n",
    "  start = random.choice(list(list_of_begins))\n",
    "  text_gen = sample(net, 8000, prime= start, top_k=10)\n",
    "\n",
    "  with open(path + 'doc_'+str(i)+'.txt', 'w+') as output_file:\n",
    "    try:\n",
    "      output_file.write(text_gen)\n",
    "    except:\n",
    "      continue\n",
    "  if i % 200 == 0:\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    print(dt_string, '| ',  'number of processed files: ' + str(i), round(i/num_docs, 3)*100, '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FNPcLUAhspTR",
    "outputId": "9aa92df3-8e18-449f-dfe7-8b10840ec4a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5446, 0)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If the generator does not work completely, you need to run this code (it will start from the last place)\n",
    "\n",
    "# This is the code to generate (to determine how many have already been generated)\n",
    "\n",
    "list_gen = glob.glob('/content/drive/MyDrive/2022-01-15_Course_project/gen_text_en/*.txt')\n",
    "list_num = [ int(i.split('/')[-1].split('.txt')[0].split('doc_')[-1]) for i in list_gen]\n",
    "max(list_num), min(list_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EkCKEU9qtZmr",
    "outputId": "6e3895d1-20a9-4772-ca71-a0f07a4f9adc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/01/2022 18:10:43 |  number of processed files: 5600 37.3 %\n",
      "17/01/2022 18:24:18 |  number of processed files: 5800 38.7 %\n",
      "17/01/2022 18:37:54 |  number of processed files: 6000 40.0 %\n",
      "17/01/2022 18:51:32 |  number of processed files: 6200 41.3 %\n",
      "17/01/2022 19:05:10 |  number of processed files: 6400 42.699999999999996 %\n",
      "17/01/2022 19:18:48 |  number of processed files: 6600 44.0 %\n",
      "17/01/2022 19:32:26 |  number of processed files: 6800 45.300000000000004 %\n",
      "17/01/2022 19:46:03 |  number of processed files: 7000 46.7 %\n",
      "17/01/2022 19:59:42 |  number of processed files: 7200 48.0 %\n",
      "17/01/2022 20:13:20 |  number of processed files: 7400 49.3 %\n",
      "17/01/2022 20:26:59 |  number of processed files: 7600 50.7 %\n",
      "17/01/2022 20:40:37 |  number of processed files: 7800 52.0 %\n",
      "17/01/2022 20:54:16 |  number of processed files: 8000 53.300000000000004 %\n",
      "17/01/2022 21:07:56 |  number of processed files: 8200 54.7 %\n",
      "17/01/2022 21:21:36 |  number of processed files: 8400 56.00000000000001 %\n",
      "17/01/2022 21:35:17 |  number of processed files: 8600 57.3 %\n",
      "17/01/2022 21:48:57 |  number of processed files: 8800 58.699999999999996 %\n",
      "17/01/2022 22:02:36 |  number of processed files: 9000 60.0 %\n",
      "17/01/2022 22:16:16 |  number of processed files: 9200 61.3 %\n",
      "17/01/2022 22:29:55 |  number of processed files: 9400 62.7 %\n",
      "17/01/2022 22:43:33 |  number of processed files: 9600 64.0 %\n",
      "17/01/2022 22:57:13 |  number of processed files: 9800 65.3 %\n",
      "17/01/2022 23:10:53 |  number of processed files: 10000 66.7 %\n",
      "17/01/2022 23:24:33 |  number of processed files: 10200 68.0 %\n",
      "17/01/2022 23:38:13 |  number of processed files: 10400 69.3 %\n",
      "17/01/2022 23:51:54 |  number of processed files: 10600 70.7 %\n",
      "18/01/2022 00:05:34 |  number of processed files: 10800 72.0 %\n",
      "18/01/2022 00:19:13 |  number of processed files: 11000 73.3 %\n",
      "18/01/2022 00:32:52 |  number of processed files: 11200 74.7 %\n",
      "18/01/2022 00:46:32 |  number of processed files: 11400 76.0 %\n",
      "18/01/2022 01:00:12 |  number of processed files: 11600 77.3 %\n",
      "18/01/2022 01:13:52 |  number of processed files: 11800 78.7 %\n",
      "18/01/2022 01:27:33 |  number of processed files: 12000 80.0 %\n",
      "18/01/2022 01:41:13 |  number of processed files: 12200 81.3 %\n",
      "18/01/2022 01:54:46 |  number of processed files: 12400 82.69999999999999 %\n",
      "18/01/2022 02:08:11 |  number of processed files: 12600 84.0 %\n",
      "18/01/2022 02:21:36 |  number of processed files: 12800 85.3 %\n",
      "18/01/2022 02:35:02 |  number of processed files: 13000 86.7 %\n",
      "18/01/2022 02:48:30 |  number of processed files: 13200 88.0 %\n",
      "18/01/2022 03:01:58 |  number of processed files: 13400 89.3 %\n",
      "18/01/2022 03:15:27 |  number of processed files: 13600 90.7 %\n",
      "18/01/2022 03:28:58 |  number of processed files: 13800 92.0 %\n",
      "18/01/2022 03:42:27 |  number of processed files: 14000 93.30000000000001 %\n",
      "18/01/2022 03:55:57 |  number of processed files: 14200 94.69999999999999 %\n",
      "18/01/2022 04:09:27 |  number of processed files: 14400 96.0 %\n",
      "18/01/2022 04:22:57 |  number of processed files: 14600 97.3 %\n",
      "18/01/2022 04:36:30 |  number of processed files: 14800 98.7 %\n"
     ]
    }
   ],
   "source": [
    "# If the generator does not work completely, you need to run this code (it will start from the last place)\n",
    "\n",
    "num_docs = 15000\n",
    "\n",
    "path = '/content/drive/MyDrive/2022-01-15_Course_project/gen_text_en/'\n",
    "\n",
    "for i in range(max(list_num), num_docs, 1):\n",
    "  start = random.choice(list(list_of_begins))\n",
    "  text_gen = sample(net, 8000, prime= start, top_k=10)\n",
    "\n",
    "  with open(path + 'doc_'+str(i)+'.txt', 'w+') as output_file:\n",
    "    try:\n",
    "      output_file.write(text_gen)\n",
    "    except:\n",
    "      continue\n",
    "  if i % 200 == 0:\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    print(dt_string, '| ',  'number of processed files: ' + str(i), round(i/num_docs, 3)*100, '%')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Generation_en.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
