{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-20T00:05:19.638787Z",
          "start_time": "2022-01-20T00:05:19.628854Z"
        },
        "id": "cdb50c2b"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import glob\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import codecs"
      ],
      "id": "cdb50c2b"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-19T23:16:44.565625Z",
          "start_time": "2022-01-19T23:16:40.800174Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6120d187",
        "outputId": "e35f4ba3-9e0f-4eed-a6a2-6254bf56cde5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from scipy.sparse.linalg import svds\n",
        "import numpy as np"
      ],
      "id": "6120d187"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRt8kPDwn-_b",
        "outputId": "006f189f-33f2-4bb4-cb61-fe440ef25196"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 4.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install unidecode"
      ],
      "id": "WRt8kPDwn-_b"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-19T23:16:51.418514Z",
          "start_time": "2022-01-19T23:16:51.400418Z"
        },
        "id": "3bd2f029"
      },
      "outputs": [],
      "source": [
        "import unidecode"
      ],
      "id": "3bd2f029"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a7425c8",
        "outputId": "c8421bba-9d28-4689-8b63-b0bcfbbeb82f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "5a7425c8"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MaplzLoMnLuZ"
      },
      "outputs": [],
      "source": [
        "# Wishart clustering function\n",
        "# https://github.com/Radi4/BotDetection/blob/master/Wishart.py\n",
        "\n",
        "import numpy as np\n",
        "from scipy.special import gamma\n",
        "from sklearn.neighbors import KDTree\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "class Wishart:\n",
        "    def __init__(self, wishart_neighbors, significance_level):\n",
        "        self.wishart_neighbors = wishart_neighbors  # Number of neighbors\n",
        "        self.significance_level = significance_level  # Significance level\n",
        "\n",
        "    def fit(self, X):\n",
        "        from sklearn.neighbors import KDTree\n",
        "        kdt = KDTree(X, metric='euclidean')\n",
        "\n",
        "        #add one because you are your neighb.\n",
        "        distances, neighbors = kdt.query(X, k = self.wishart_neighbors + 1, return_distance = True)\n",
        "        neighbors = neighbors[:, 1:]\n",
        "\n",
        "\n",
        "        distances = distances[:, -1]\n",
        "        indexes = np.argsort(distances)\n",
        "        \n",
        "        size, dim = X.shape\n",
        "\n",
        "        self.object_labels = np.zeros(size, dtype = int) - 1\n",
        "\n",
        "        #index in tuple\n",
        "        #min_dist, max_dist, flag_to_significant\n",
        "        self.clusters = np.array([(1., 1., 0)])\n",
        "        self.clusters_to_objects = defaultdict(list)\n",
        "        print('Start clustering')\n",
        "\n",
        "        for index in indexes:\n",
        "            neighbors_clusters =\\\n",
        "                np.concatenate([self.object_labels[neighbors[index]], self.object_labels[neighbors[index]]])\n",
        "            unique_clusters = np.unique(neighbors_clusters).astype(int)\n",
        "            unique_clusters = unique_clusters[unique_clusters != -1]\n",
        "\n",
        "\n",
        "            if len(unique_clusters) == 0:\n",
        "                self._create_new_cluster(index, distances[index])\n",
        "            else:\n",
        "                max_cluster = unique_clusters[-1]\n",
        "                min_cluster = unique_clusters[0]\n",
        "                if max_cluster == min_cluster:\n",
        "                    if self.clusters[max_cluster][-1] < 0.5:\n",
        "                        self._add_elem_to_exist_cluster(index, distances[index], max_cluster)\n",
        "                    else:\n",
        "                        self._add_elem_to_noise(index)\n",
        "                else:\n",
        "                    my_clusters = self.clusters[unique_clusters]\n",
        "                    flags = my_clusters[:, -1]\n",
        "                    if np.min(flags) > 0.5:\n",
        "                        self._add_elem_to_noise(index)\n",
        "                    else:\n",
        "                        significan = np.power(my_clusters[:, 0], -dim) - np.power(my_clusters[:, 1], -dim)\n",
        "                        significan *= self.wishart_neighbors\n",
        "                        significan /= size\n",
        "                        significan /= np.power(np.pi, dim / 2)\n",
        "                        significan *= gamma(dim / 2 + 1)\n",
        "                        significan_index = significan >= self.significance_level\n",
        "\n",
        "                        significan_clusters = unique_clusters[significan_index]\n",
        "                        not_significan_clusters = unique_clusters[~significan_index]\n",
        "                        significan_clusters_count = len(significan_clusters)\n",
        "                        if significan_clusters_count > 1 or min_cluster == 0:\n",
        "                            self._add_elem_to_noise(index)\n",
        "                            self.clusters[significan_clusters, -1] = 1\n",
        "                            for not_sig_cluster in not_significan_clusters:\n",
        "                                if not_sig_cluster == 0:\n",
        "                                    continue\n",
        "\n",
        "                                for bad_index in self.clusters_to_objects[not_sig_cluster]:\n",
        "                                    self._add_elem_to_noise(bad_index)\n",
        "                                self.clusters_to_objects[not_sig_cluster].clear()\n",
        "                        else:\n",
        "                            for cur_cluster in unique_clusters:\n",
        "                                if cur_cluster == min_cluster:\n",
        "                                    continue\n",
        "\n",
        "                                for bad_index in self.clusters_to_objects[cur_cluster]:\n",
        "                                    self._add_elem_to_exist_cluster(bad_index, distances[bad_index], min_cluster)\n",
        "                                self.clusters_to_objects[cur_cluster].clear()\n",
        "\n",
        "                            self._add_elem_to_exist_cluster(index, distances[index], min_cluster)\n",
        "\n",
        "        return self.clean_data()\n",
        "\n",
        "    def clean_data(self):\n",
        "        unique = np.unique(self.object_labels)\n",
        "        index = np.argsort(unique)\n",
        "        if unique[0] != 0:\n",
        "            index += 1\n",
        "        true_cluster = {unq :  index for unq, index in zip(unique, index)}\n",
        "        result = np.zeros(len(self.object_labels), dtype = int)\n",
        "        for index, unq in enumerate(self.object_labels):\n",
        "            result[index] = true_cluster[unq]\n",
        "        return result\n",
        "\n",
        "    def _add_elem_to_noise(self, index):\n",
        "        self.object_labels[index] = 0\n",
        "        self.clusters_to_objects[0].append(index)\n",
        "\n",
        "    def _create_new_cluster(self, index, dist):\n",
        "        self.object_labels[index] = len(self.clusters)\n",
        "        self.clusters_to_objects[len(self.clusters)].append(index)\n",
        "        self.clusters = np.append(self.clusters, [(dist, dist, 0)], axis = 0)\n",
        "\n",
        "    def _add_elem_to_exist_cluster(self, index, dist, cluster_label):\n",
        "        self.object_labels[index] = cluster_label\n",
        "        self.clusters_to_objects[cluster_label].append(index)\n",
        "        self.clusters[cluster_label][0] = min(self.clusters[cluster_label][0], dist)\n",
        "        self.clusters[cluster_label][1] = max(self.clusters[cluster_label][1], dist)\n",
        "\n",
        "\n",
        "class PreTrainWishart:\n",
        "    def __init__(self, wishart_neighbors, significance_level, distances, neighbors):\n",
        "        self.wishart_neighbors = wishart_neighbors  # Number of neighbors\n",
        "        self.significance_level = significance_level  # Significance level\n",
        "        self.distances = distances\n",
        "        self.neighbors = neighbors\n",
        "\n",
        "    def fit(self, X):\n",
        "        from sklearn.neighbors import KDTree\n",
        "        kdt = KDTree(X, metric='euclidean')\n",
        "\n",
        "        #add one because you are your neighb.\n",
        "        neighbors = self.neighbors[:, 1 : self.wishart_neighbors + 1]\n",
        "        distances = self.distances[:, self.wishart_neighbors]\n",
        "        indexes = np.argsort(distances)\n",
        "        \n",
        "        size, dim = X.shape\n",
        "\n",
        "        self.object_labels = np.zeros(size, dtype = int) - 1\n",
        "\n",
        "        #index in tuple\n",
        "        #min_dist, max_dist, flag_to_significant\n",
        "        self.clusters = np.array([(1., 1., 0)])\n",
        "        self.clusters_to_objects = defaultdict(list)\n",
        "\n",
        "        for index in indexes:\n",
        "            neighbors_clusters =\\\n",
        "                np.concatenate([self.object_labels[neighbors[index]], self.object_labels[neighbors[index]]])\n",
        "            unique_clusters = np.unique(neighbors_clusters).astype(int)\n",
        "            unique_clusters = unique_clusters[unique_clusters != -1]\n",
        "\n",
        "\n",
        "            if len(unique_clusters) == 0:\n",
        "                self._create_new_cluster(index, distances[index])\n",
        "            else:\n",
        "                max_cluster = unique_clusters[-1]\n",
        "                min_cluster = unique_clusters[0]\n",
        "                if max_cluster == min_cluster:\n",
        "                    if self.clusters[max_cluster][-1] < 0.5:\n",
        "                        self._add_elem_to_exist_cluster(index, distances[index], max_cluster)\n",
        "                    else:\n",
        "                        self._add_elem_to_noise(index)\n",
        "                else:\n",
        "                    my_clusters = self.clusters[unique_clusters]\n",
        "                    flags = my_clusters[:, -1]\n",
        "                    if np.min(flags) > 0.5:\n",
        "                        self._add_elem_to_noise(index)\n",
        "                    else:\n",
        "                        significan = np.power(my_clusters[:, 0], -dim) - np.power(my_clusters[:, 1], -dim)\n",
        "                        significan *= self.wishart_neighbors\n",
        "                        significan /= size\n",
        "                        significan /= np.power(np.pi, dim / 2)\n",
        "                        significan *= gamma(dim / 2 + 1)\n",
        "                        significan_index = significan >= self.significance_level\n",
        "\n",
        "                        significan_clusters = unique_clusters[significan_index]\n",
        "                        not_significan_clusters = unique_clusters[~significan_index]\n",
        "                        significan_clusters_count = len(significan_clusters)\n",
        "                        if significan_clusters_count > 1 or min_cluster == 0:\n",
        "                            self._add_elem_to_noise(index)\n",
        "                            self.clusters[significan_clusters, -1] = 1\n",
        "                            for not_sig_cluster in not_significan_clusters:\n",
        "                                if not_sig_cluster == 0:\n",
        "                                    continue\n",
        "\n",
        "                                for bad_index in self.clusters_to_objects[not_sig_cluster]:\n",
        "                                    self._add_elem_to_noise(bad_index)\n",
        "                                self.clusters_to_objects[not_sig_cluster].clear()\n",
        "                        else:\n",
        "                            for cur_cluster in unique_clusters:\n",
        "                                if cur_cluster == min_cluster:\n",
        "                                    continue\n",
        "\n",
        "                                for bad_index in self.clusters_to_objects[cur_cluster]:\n",
        "                                    self._add_elem_to_exist_cluster(bad_index, distances[bad_index], min_cluster)\n",
        "                                self.clusters_to_objects[cur_cluster].clear()\n",
        "\n",
        "                            self._add_elem_to_exist_cluster(index, distances[index], min_cluster)\n",
        "\n",
        "        return self.clean_data()\n",
        "\n",
        "    def clean_data(self):\n",
        "        unique = np.unique(self.object_labels)\n",
        "        index = np.argsort(unique)\n",
        "        if unique[0] != 0:\n",
        "            index += 1\n",
        "        true_cluster = {unq :  index for unq, index in zip(unique, index)}\n",
        "        result = np.zeros(len(self.object_labels), dtype = int)\n",
        "        for index, unq in enumerate(self.object_labels):\n",
        "            result[index] = true_cluster[unq]\n",
        "        return result\n",
        "\n",
        "    def _add_elem_to_noise(self, index):\n",
        "        self.object_labels[index] = 0\n",
        "        self.clusters_to_objects[0].append(index)\n",
        "\n",
        "    def _create_new_cluster(self, index, dist):\n",
        "        self.object_labels[index] = len(self.clusters)\n",
        "        self.clusters_to_objects[len(self.clusters)].append(index)\n",
        "        self.clusters = np.append(self.clusters, [(dist, dist, 0)], axis = 0)\n",
        "\n",
        "    def _add_elem_to_exist_cluster(self, index, dist, cluster_label):\n",
        "        self.object_labels[index] = cluster_label\n",
        "        self.clusters_to_objects[cluster_label].append(index)\n",
        "        self.clusters[cluster_label][0] = min(self.clusters[cluster_label][0], dist)\n",
        "        self.clusters[cluster_label][1] = max(self.clusters[cluster_label][1], dist)"
      ],
      "id": "MaplzLoMnLuZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f043e55"
      },
      "source": [
        "## Create a vector representation based on TfidfVectorizer (on the bot texts)"
      ],
      "id": "7f043e55"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-20T00:02:39.113091Z",
          "start_time": "2022-01-20T00:02:39.053009Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b1f3a37",
        "outputId": "c56daaf8-ab5c-41ae-9794-441357305426"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'más', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'mí', 'antes', 'algunos', 'qué', 'unos', 'yo', 'otro', 'otras', 'otra', 'él', 'tanto', 'esa', 'estos', 'mucho', 'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tú', 'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosotros', 'vosotras', 'os', 'mío', 'mía', 'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy', 'estás', 'está', 'estamos', 'estáis', 'están', 'esté', 'estés', 'estemos', 'estéis', 'estén', 'estaré', 'estarás', 'estará', 'estaremos', 'estaréis', 'estarán', 'estaría', 'estarías', 'estaríamos', 'estaríais', 'estarían', 'estaba', 'estabas', 'estábamos', 'estabais', 'estaban', 'estuve', 'estuviste', 'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera', 'estuvieras', 'estuviéramos', 'estuvierais', 'estuvieran', 'estuviese', 'estuvieses', 'estuviésemos', 'estuvieseis', 'estuviesen', 'estando', 'estado', 'estada', 'estados', 'estadas', 'estad', 'he', 'has', 'ha', 'hemos', 'habéis', 'han', 'haya', 'hayas', 'hayamos', 'hayáis', 'hayan', 'habré', 'habrás', 'habrá', 'habremos', 'habréis', 'habrán', 'habría', 'habrías', 'habríamos', 'habríais', 'habrían', 'había', 'habías', 'habíamos', 'habíais', 'habían', 'hube', 'hubiste', 'hubo', 'hubimos', 'hubisteis', 'hubieron', 'hubiera', 'hubieras', 'hubiéramos', 'hubierais', 'hubieran', 'hubiese', 'hubieses', 'hubiésemos', 'hubieseis', 'hubiesen', 'habiendo', 'habido', 'habida', 'habidos', 'habidas', 'soy', 'eres', 'es', 'somos', 'sois', 'son', 'sea', 'seas', 'seamos', 'seáis', 'sean', 'seré', 'serás', 'será', 'seremos', 'seréis', 'serán', 'sería', 'serías', 'seríamos', 'seríais', 'serían', 'era', 'eras', 'éramos', 'erais', 'eran', 'fui', 'fuiste', 'fue', 'fuimos', 'fuisteis', 'fueron', 'fuera', 'fueras', 'fuéramos', 'fuerais', 'fueran', 'fuese', 'fueses', 'fuésemos', 'fueseis', 'fuesen', 'sintiendo', 'sentido', 'sentida', 'sentidos', 'sentidas', 'siente', 'sentid', 'tengo', 'tienes', 'tiene', 'tenemos', 'tenéis', 'tienen', 'tenga', 'tengas', 'tengamos', 'tengáis', 'tengan', 'tendré', 'tendrás', 'tendrá', 'tendremos', 'tendréis', 'tendrán', 'tendría', 'tendrías', 'tendríamos', 'tendríais', 'tendrían', 'tenía', 'tenías', 'teníamos', 'teníais', 'tenían', 'tuve', 'tuviste', 'tuvo', 'tuvimos', 'tuvisteis', 'tuvieron', 'tuviera', 'tuvieras', 'tuviéramos', 'tuvierais', 'tuvieran', 'tuviese', 'tuvieses', 'tuviésemos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened']\n"
          ]
        }
      ],
      "source": [
        "# Stopwords\n",
        "print(stopwords.words('spanish'))"
      ],
      "id": "7b1f3a37"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-20T00:03:08.870021Z",
          "start_time": "2022-01-20T00:03:08.861187Z"
        },
        "id": "4d3f9554"
      },
      "outputs": [],
      "source": [
        "def make_corpus(input_path, output_file_path):\n",
        "    i = 0\n",
        "    file_list = glob.glob(input_path + '*')\n",
        "    \n",
        "    with open(output_file_path, 'w+') as output_file:\n",
        "        for file in file_list:\n",
        "            if i % 500 == 0:\n",
        "                now = datetime.now()\n",
        "                dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
        "                print(dt_string, '| ',  'number of processed files: ' + str(i), '| ', \n",
        "                      'percentage of completion:', str(round(i/len(file_list), 2)* 100) + ' %' )\n",
        "            i+=1\n",
        "            with open(file, 'r') as input_file:\n",
        "                output_file.write(input_file.read().replace('\\n', ' '))\n",
        "                output_file.write('\\n')"
      ],
      "id": "4d3f9554"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEKONTR6eLwe",
        "outputId": "3106923b-759f-4eef-c170-19fa4962ba02"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "len(glob.glob('/content/drive/MyDrive/2022-01-15_Course_project/new_prep_gen_text_es/*'))"
      ],
      "id": "GEKONTR6eLwe"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gY0zi4OpuaBy",
        "outputId": "3fea5c01-fe08-41d6-9224-efb54e23e5ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29/01/2022 23:02:24 |  number of processed files: 0 |  percentage of completion: 0.0 %\n",
            "29/01/2022 23:06:03 |  number of processed files: 500 |  percentage of completion: 5.0 %\n",
            "29/01/2022 23:06:03 |  number of processed files: 1000 |  percentage of completion: 10.0 %\n",
            "29/01/2022 23:06:04 |  number of processed files: 1500 |  percentage of completion: 15.0 %\n",
            "29/01/2022 23:06:04 |  number of processed files: 2000 |  percentage of completion: 20.0 %\n",
            "29/01/2022 23:06:05 |  number of processed files: 2500 |  percentage of completion: 25.0 %\n",
            "29/01/2022 23:06:05 |  number of processed files: 3000 |  percentage of completion: 30.0 %\n",
            "29/01/2022 23:06:06 |  number of processed files: 3500 |  percentage of completion: 35.0 %\n",
            "29/01/2022 23:06:06 |  number of processed files: 4000 |  percentage of completion: 40.0 %\n",
            "29/01/2022 23:06:07 |  number of processed files: 4500 |  percentage of completion: 45.0 %\n",
            "29/01/2022 23:06:07 |  number of processed files: 5000 |  percentage of completion: 50.0 %\n",
            "29/01/2022 23:06:07 |  number of processed files: 5500 |  percentage of completion: 55.00000000000001 %\n",
            "29/01/2022 23:06:08 |  number of processed files: 6000 |  percentage of completion: 60.0 %\n",
            "29/01/2022 23:06:08 |  number of processed files: 6500 |  percentage of completion: 65.0 %\n",
            "29/01/2022 23:06:09 |  number of processed files: 7000 |  percentage of completion: 70.0 %\n",
            "29/01/2022 23:06:09 |  number of processed files: 7500 |  percentage of completion: 75.0 %\n",
            "29/01/2022 23:06:10 |  number of processed files: 8000 |  percentage of completion: 80.0 %\n",
            "29/01/2022 23:06:10 |  number of processed files: 8500 |  percentage of completion: 85.0 %\n",
            "29/01/2022 23:06:11 |  number of processed files: 9000 |  percentage of completion: 90.0 %\n",
            "29/01/2022 23:06:11 |  number of processed files: 9500 |  percentage of completion: 95.0 %\n"
          ]
        }
      ],
      "source": [
        "# Let's make corpus for bot texts\n",
        "\n",
        "make_corpus('/content/drive/MyDrive/2022-01-15_Course_project/new_prep_gen_text_es/',\n",
        "            '/content/drive/MyDrive/2022-01-15_Course_project/new_TF_IDF_clustering_bot_es/dataset_generate_es.txt') "
      ],
      "id": "gY0zi4OpuaBy"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-20T00:03:42.696966Z",
          "start_time": "2022-01-20T00:03:42.639936Z"
        },
        "id": "07c89c06"
      },
      "outputs": [],
      "source": [
        "# TF-IDF corpus\n",
        "\n",
        "def make_table_and_dict(corpus_path, min_df, max_df, token_pattern = None, use_idf = True, stop_words = 'spanish'):\n",
        "    \n",
        "    with open(corpus_path, 'r') as corpus_file:\n",
        "        if token_pattern:\n",
        "            vectorizer = TfidfVectorizer(analyzer='word', min_df=min_df, token_pattern=token_pattern, use_idf=use_idf)\n",
        "        else:\n",
        "            vectorizer = TfidfVectorizer(analyzer='word', min_df=min_df)\n",
        "        data_vectorized = vectorizer.fit_transform(corpus_file)\n",
        "    return data_vectorized, vectorizer.get_feature_names(), vectorizer.idf_"
      ],
      "id": "07c89c06"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-20T00:03:43.839418Z",
          "start_time": "2022-01-20T00:03:43.817476Z"
        },
        "id": "8f4de5bf"
      },
      "outputs": [],
      "source": [
        "def create_table(data_vectorized, k, name, path):\n",
        "    u, sigma, vt = svds(data_vectorized, k)\n",
        "    print(sigma)\n",
        "    dict_ = np.dot(np.diag(sigma), vt).T\n",
        "        \n",
        "    with open(path + name + str(k) + '.pkl', 'wb') as f:\n",
        "        pickle.dump(dict_, f)\n",
        "    return dict_"
      ],
      "id": "8f4de5bf"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JALQb0THBpJl"
      },
      "outputs": [],
      "source": [
        "# Vector representation on human texts (used on the bot)\n",
        "f = open('/content/drive/MyDrive/2022-01-15_Course_project/TF_IDF_clustering_es/TF_IDF_cut_human_es.pkl', 'rb')\n",
        "dict_cut = pickle.load(f)"
      ],
      "id": "JALQb0THBpJl"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-20T00:06:30.784942Z",
          "start_time": "2022-01-20T00:06:30.769056Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "895287d1",
        "outputId": "25098520-79e8-4a25-d9d5-a44469cd4641"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2079"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "len(dict_cut.keys())"
      ],
      "id": "895287d1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce276f30"
      },
      "source": [
        "# Making n-grams and Clustering"
      ],
      "id": "ce276f30"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-20T00:06:36.788059Z",
          "start_time": "2022-01-20T00:06:36.765360Z"
        },
        "id": "3edea2f9"
      },
      "outputs": [],
      "source": [
        "from itertools import product\n",
        "from math import log\n",
        "from sklearn.metrics import silhouette_score, silhouette_samples\n",
        "from sklearn.metrics.pairwise import pairwise_distances\n",
        "from sklearn.metrics import calinski_harabasz_score"
      ],
      "id": "3edea2f9"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-20T00:06:39.611702Z",
          "start_time": "2022-01-20T00:06:39.591675Z"
        },
        "id": "8b9b8248"
      },
      "outputs": [],
      "source": [
        "def divide(data, labels):\n",
        "    clusters = set(labels)\n",
        "    clusters_data = []\n",
        "    for cluster in clusters:\n",
        "        clusters_data.append(data[labels == cluster, :])\n",
        "    return clusters_data\n",
        "\n",
        "def get_centroids(clusters):\n",
        "    centroids = []\n",
        "    for cluster_data in clusters:\n",
        "        centroids.append(cluster_data.mean(axis=0))\n",
        "    return centroids"
      ],
      "id": "8b9b8248"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-20T00:06:40.279933Z",
          "start_time": "2022-01-20T00:06:40.241868Z"
        },
        "id": "9eb06fd2"
      },
      "outputs": [],
      "source": [
        "def cohesion(data, labels):\n",
        "    clusters = sorted(set(labels))\n",
        "    sse = 0\n",
        "    for cluster in clusters:\n",
        "        cluster_data = data[labels == cluster, :]\n",
        "        centroid = cluster_data.mean(axis = 0)\n",
        "        sse += ((cluster_data - centroid)**2).sum()\n",
        "    return sse\n",
        "\n",
        "def separation(data, labels, cohesion_score):\n",
        "    # calculate separation as SST - SSE\n",
        "    return cohesion(data, np.zeros(data.shape[0])) - cohesion_score\n",
        "\n",
        "def SST(data):\n",
        "    c = get_centroids([data])\n",
        "    return ((data - c) ** 2).sum()\n",
        "\n",
        "def SSE(clusters, centroids):\n",
        "    result = 0\n",
        "    for cluster, centroid in zip(clusters, centroids):\n",
        "        result += ((cluster - centroid) ** 2).sum()\n",
        "    return result\n",
        "\n",
        "# Clear the store before running each time\n",
        "within_cluster_dist_sum_store = {}\n",
        "def within_cluster_dist_sum(cluster, centroid, cluster_id):\n",
        "    if cluster_id in within_cluster_dist_sum_store:\n",
        "        return within_cluster_dist_sum_store[cluster_id]\n",
        "    else:\n",
        "        result = (((cluster - centroid) ** 2).sum(axis=1)**.5).sum()\n",
        "        within_cluster_dist_sum_store[cluster_id] = result\n",
        "    return result\n",
        "\n",
        "def RMSSTD(data, clusters, centroids):\n",
        "    df = data.shape[0] - len(clusters)\n",
        "    attribute_num = data.shape[1]\n",
        "    return (SSE(clusters, centroids) / (attribute_num * df)) ** .5\n",
        "\n",
        "# equal to separation / (cohesion + separation)\n",
        "def RS(data, clusters, centroids):\n",
        "    sst = SST(data)\n",
        "    sse = SSE(clusters, centroids)\n",
        "    return (sst - sse) / sst\n",
        "\n",
        "def DB_find_max_j(clusters, centroids, i):\n",
        "    max_val = 0\n",
        "    max_j = 0\n",
        "    for j in range(len(clusters)):\n",
        "        if j == i:\n",
        "            continue\n",
        "        cluster_i_stat = within_cluster_dist_sum(clusters[i], centroids[i], i) / clusters[i].shape[0]\n",
        "        cluster_j_stat = within_cluster_dist_sum(clusters[j], centroids[j], j) / clusters[j].shape[0]\n",
        "        val = (cluster_i_stat + cluster_j_stat) / (((centroids[i] - centroids[j]) ** 2).sum() ** .5)\n",
        "        if val > max_val:\n",
        "            max_val = val\n",
        "            max_j = j\n",
        "    return max_val\n",
        "\n",
        "def DB(data, clusters, centroids):\n",
        "    result = 0\n",
        "    for i in range(len(clusters)):\n",
        "        result += DB_find_max_j(clusters, centroids, i)\n",
        "    return result / len(clusters)\n",
        "\n",
        "def XB(data, clusters, centroids):\n",
        "    sse = SSE(clusters, centroids)\n",
        "    min_dist = ((centroids[0] - centroids[1]) ** 2).sum()\n",
        "    for centroid_i, centroid_j in list(product(centroids, centroids)):\n",
        "        if (centroid_i - centroid_j).sum() == 0:\n",
        "            continue\n",
        "        dist = ((centroid_i - centroid_j) ** 2).sum()\n",
        "        if dist < min_dist:\n",
        "            min_dist = dist\n",
        "    return sse / (data.shape[0] * min_dist)"
      ],
      "id": "9eb06fd2"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-20T17:23:21.939658Z",
          "start_time": "2022-01-20T17:23:21.853300Z"
        },
        "id": "93e7bacc"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.metrics import calinski_harabasz_score\n",
        "\n",
        "# Some metrics can work for a very long time (commented out)\n",
        "\n",
        "def get_validation_scores(data, labels, max_clust = None):\n",
        "    #within_cluster_dist_sum_store.clear()\n",
        "    \n",
        "    clusters = divide(data, labels)\n",
        "    centroids = get_centroids(clusters)\n",
        "    \n",
        "    scores = {}\n",
        "    if max_clust:\n",
        "        if len(clusters) > max_clust:\n",
        "            scores['cohesion'] = cohesion(data, labels)\n",
        "            scores['separation'] = separation(data, labels, scores['cohesion'])\n",
        "            scores['calinski_harabaz_score'] = None\n",
        "            scores['RMSSTD'] = RMSSTD(data, clusters, centroids)\n",
        "            scores['RS'] = RS(data, clusters, centroids)\n",
        "            #scores['DB'] = None\n",
        "            #scores['XB'] = XB(data, clusters, centroids)\n",
        "            scores['silhouette'] = None\n",
        "        else:\n",
        "            scores['cohesion'] = cohesion(data, labels)\n",
        "            scores['separation'] = separation(data, labels, scores['cohesion'])\n",
        "            scores['calinski_harabaz_score'] = calinski_harabasz_score(data, labels)\n",
        "            scores['RMSSTD'] = RMSSTD(data, clusters, centroids)\n",
        "            scores['RS'] = RS(data, clusters, centroids)\n",
        "            #scores['DB'] = DB(data, clusters, centroids)\n",
        "            #scores['XB'] = XB(data, clusters, centroids)\n",
        "            scores['silhouette'] = silhouette_score(data, labels)\n",
        "    else:\n",
        "        scores['cohesion'] = cohesion(data, labels)\n",
        "        scores['separation'] = separation(data, labels, scores['cohesion'])\n",
        "        scores['calinski_harabaz_score'] = calinski_harabasz_score(data, labels)\n",
        "        scores['RMSSTD'] = RMSSTD(data, clusters, centroids)\n",
        "        scores['RS'] = RS(data, clusters, centroids)\n",
        "        #scores['DB'] = DB(data, clusters, centroids)\n",
        "        #scores['XB'] = XB(data, clusters, centroids)\n",
        "        scores['silhouette'] = silhouette_score(data, labels)\n",
        "    \n",
        "    return scores"
      ],
      "id": "93e7bacc"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-20T00:06:52.939414Z",
          "start_time": "2022-01-20T00:06:52.923459Z"
        },
        "id": "4a8e01ac"
      },
      "outputs": [],
      "source": [
        "def make_ngrams(input_corpus,  dict_, N = 2, m = None, uniq = False):\n",
        "    dict_grams = dict()\n",
        "    num_ = 0\n",
        "    i = 0\n",
        "    j = 0\n",
        "    \n",
        "    print('Count documents: ', len(input_corpus))\n",
        "    for sentence in input_corpus:\n",
        "        sentence = sentence.split(' ')\n",
        "        grams = [sentence[i:i+N] for i in range(len(sentence)-N+1)]\n",
        "        for g in grams:\n",
        "            g_key = '_'.join(elem for elem in g)\n",
        "\n",
        "            if uniq:\n",
        "                if all(elem in dict_.keys()  for elem in g) and (g_key not in dict_grams.keys()):\n",
        "                    dict_grams[g_key] = []\n",
        "                    for elem in g:\n",
        "                            if m:\n",
        "                                dict_grams[g_key] += list(dict_[elem][1][:m])\n",
        "                            else:\n",
        "                                dict_grams[g_key] += list(dict_[elem][1])\n",
        "            else:\n",
        "                if all(elem in dict_.keys()  for elem in g):\n",
        "                    concat = []\n",
        "                    for elem in g:\n",
        "                        if m:\n",
        "                            concat += list(dict_[elem][1][:m])\n",
        "                        else:\n",
        "                            concat += list(dict_[elem][1])\n",
        "                    dict_grams[i] = (j, g_key, concat)\n",
        "                    i += 1\n",
        "            j += 1\n",
        "       \n",
        "            \n",
        "        if num_ % 500 == 0:\n",
        "            now = datetime.now()\n",
        "            dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
        "            print(dt_string, '| ',  'number of processed documents: ' + str(num_), '| ', \n",
        "                      'percentage of completion:', str(round(num_/len(input_corpus), 2)* 100) + ' %' )\n",
        "        num_ += 1\n",
        "    return dict_grams"
      ],
      "id": "4a8e01ac"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-20T00:07:43.802071Z",
          "start_time": "2022-01-20T00:07:43.415738Z"
        },
        "id": "8eae3315"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/2022-01-15_Course_project/new_TF_IDF_clustering_bot_es/dataset_generate_es.txt', 'r') as corpus_file:\n",
        "    corpus = corpus_file.readlines()"
      ],
      "id": "8eae3315"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-20T00:09:15.616274Z",
          "start_time": "2022-01-20T00:07:51.301224Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b29d21d",
        "outputId": "29821310-0b4a-43c8-a68d-197345fd3577"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count documents:  10000\n",
            "29/01/2022 23:07:59 |  number of processed documents: 0 |  percentage of completion: 0.0 %\n",
            "29/01/2022 23:08:01 |  number of processed documents: 500 |  percentage of completion: 5.0 %\n",
            "29/01/2022 23:08:03 |  number of processed documents: 1000 |  percentage of completion: 10.0 %\n",
            "29/01/2022 23:08:05 |  number of processed documents: 1500 |  percentage of completion: 15.0 %\n",
            "29/01/2022 23:08:07 |  number of processed documents: 2000 |  percentage of completion: 20.0 %\n",
            "29/01/2022 23:08:09 |  number of processed documents: 2500 |  percentage of completion: 25.0 %\n",
            "29/01/2022 23:08:11 |  number of processed documents: 3000 |  percentage of completion: 30.0 %\n",
            "29/01/2022 23:08:12 |  number of processed documents: 3500 |  percentage of completion: 35.0 %\n",
            "29/01/2022 23:08:14 |  number of processed documents: 4000 |  percentage of completion: 40.0 %\n",
            "29/01/2022 23:08:16 |  number of processed documents: 4500 |  percentage of completion: 45.0 %\n",
            "29/01/2022 23:08:18 |  number of processed documents: 5000 |  percentage of completion: 50.0 %\n",
            "29/01/2022 23:08:20 |  number of processed documents: 5500 |  percentage of completion: 55.00000000000001 %\n",
            "29/01/2022 23:08:22 |  number of processed documents: 6000 |  percentage of completion: 60.0 %\n",
            "29/01/2022 23:08:24 |  number of processed documents: 6500 |  percentage of completion: 65.0 %\n",
            "29/01/2022 23:08:25 |  number of processed documents: 7000 |  percentage of completion: 70.0 %\n",
            "29/01/2022 23:08:27 |  number of processed documents: 7500 |  percentage of completion: 75.0 %\n",
            "29/01/2022 23:08:29 |  number of processed documents: 8000 |  percentage of completion: 80.0 %\n",
            "29/01/2022 23:08:31 |  number of processed documents: 8500 |  percentage of completion: 85.0 %\n",
            "29/01/2022 23:08:33 |  number of processed documents: 9000 |  percentage of completion: 90.0 %\n",
            "29/01/2022 23:08:35 |  number of processed documents: 9500 |  percentage of completion: 95.0 %\n"
          ]
        }
      ],
      "source": [
        "dict_grams_bot = make_ngrams(corpus,  dict_cut, N = 2, m = 10)"
      ],
      "id": "3b29d21d"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-20T10:05:04.379102Z",
          "start_time": "2022-01-20T10:05:04.303378Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20e4a2cd",
        "outputId": "5c0d041d-d582-41de-d45a-1c1bc93274e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "604267"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "len(dict_grams_bot.keys())"
      ],
      "id": "20e4a2cd"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "27febfd3"
      },
      "outputs": [],
      "source": [
        "X0 = []\n",
        "for i in dict_grams_bot.keys():\n",
        "    X0.append( dict_grams_bot[i][2])\n",
        "\n",
        "list_gramm = [dict_grams_bot[i][1] for i in dict_grams_bot.keys()]\n",
        "    \n",
        "X_bot = pd.DataFrame(X0)\n",
        "X_bot['ind'] = dict_grams_bot.keys()\n",
        "X_bot['name'] = list_gramm"
      ],
      "id": "27febfd3"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-20T10:07:16.799996Z",
          "start_time": "2022-01-20T10:06:40.865Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b90f609b",
        "outputId": "6c8dc684-9d65-4be1-8f41-4145bfd9f9bb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "a_mi          10949\n",
              "llegar_a      10362\n",
              "pie_ligero     6167\n",
              "llevar_a       4449\n",
              "matar_a        4421\n",
              "Name: name, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "X_bot['name'].value_counts()[:5]"
      ],
      "id": "b90f609b"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-20T10:07:16.801992Z",
          "start_time": "2022-01-20T10:06:41.199Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d07090f8",
        "outputId": "4876dd97-f699-4411-b66d-0509855680af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(604267, 22)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "X_bot.shape"
      ],
      "id": "d07090f8"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-20T00:11:28.047560Z",
          "start_time": "2022-01-20T00:10:38.106966Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d995d904",
        "outputId": "87f690cc-4a44-4f98-eb94-5bf08ff2b078"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(604267, 22)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "X_bot.to_csv('/content/drive/MyDrive/2022-01-15_Course_project/new_TF_IDF_clustering_bot_es/n_2gramm_bot_es.csv')\n",
        "X_bot.shape"
      ],
      "id": "d995d904"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "a16b0d8a"
      },
      "outputs": [],
      "source": [
        "list_col = list(X_bot.columns)\n",
        "for i in ['Unnamed: 0', 'ind', 'name']:\n",
        "    if i in list_col:\n",
        "        list_col.remove(i)"
      ],
      "id": "a16b0d8a"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ffc9dd9",
        "outputId": "86dd7809-2b03-4fd6-ef79-e7473b185b48"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-01-29 23:09:49.794 | begin | significance:  1000 | neighbors:  50\n",
            "Start clustering\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: divide by zero encountered in power\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: invalid value encountered in subtract\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-01-29 23:59:22.605 | end | {'cohesion': 14485.81831072765, 'separation': 15821.372948615282, 'calinski_harabaz_score': None, 'RMSSTD': 0.039203958053728054, 'RS': 0.522033626053617, 'silhouette': None, 'significance': 1000, 'neighbors': 50, 'cluster_num': 133015}\n",
            "2022-01-29 23:59:46.858 | begin | significance:  1000 | neighbors:  100\n",
            "Start clustering\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: divide by zero encountered in power\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: invalid value encountered in subtract\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-01-30 00:55:50.298 | end | {'cohesion': 14226.32451029995, 'separation': 16080.866749042983, 'calinski_harabaz_score': None, 'RMSSTD': 0.03703666307936679, 'RS': 0.5305957457897277, 'silhouette': None, 'significance': 1000, 'neighbors': 100, 'cluster_num': 85707}\n",
            "2022-01-30 00:56:16.321 | begin | significance:  100000 | neighbors:  50\n",
            "Start clustering\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: divide by zero encountered in power\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: invalid value encountered in subtract\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-01-30 01:41:42.581 | end | {'cohesion': 14485.81831072765, 'separation': 15821.372948615282, 'calinski_harabaz_score': None, 'RMSSTD': 0.039203958053728054, 'RS': 0.522033626053617, 'silhouette': None, 'significance': 100000, 'neighbors': 50, 'cluster_num': 133015}\n",
            "2022-01-30 01:42:07.493 | begin | significance:  100000 | neighbors:  100\n",
            "Start clustering\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: divide by zero encountered in power\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: invalid value encountered in subtract\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-01-30 02:38:24.821 | end | {'cohesion': 14230.40270015136, 'separation': 16076.788559191573, 'calinski_harabaz_score': None, 'RMSSTD': 0.03704193554105013, 'RS': 0.53046118400152, 'silhouette': None, 'significance': 100000, 'neighbors': 100, 'cluster_num': 85706}\n"
          ]
        }
      ],
      "source": [
        "#GridSearch for Clustering\n",
        "grid_result = []\n",
        "for sig in [1000, 100000]:\n",
        "    for nei in [50, 100]:\n",
        "        \n",
        "        print(datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3], '| begin |', 'significance: ', sig, '| neighbors: ', nei )\n",
        "        clust = Wishart(significance_level = sig, wishart_neighbors = nei)\n",
        "        result = clust.fit(X_bot[list_col])\n",
        "        dict_r = get_validation_scores(np.array(X_bot[list_col]), clust.object_labels, max_clust = 10000)\n",
        "        dict_r['significance'] = sig\n",
        "        dict_r['neighbors'] = nei\n",
        "        dict_r['cluster_num'] = len(set(clust.object_labels))\n",
        "        grid_result.append(dict_r)\n",
        "        \n",
        "        #add clustering result to table\n",
        "        name_col = 'cluster_' + str(sig) + str(nei)\n",
        "        X_bot[name_col] = clust.object_labels\n",
        "        \n",
        "        print(datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3], '| end |',  dict_r)\n",
        "\n",
        "        X_bot.to_csv('/content/drive/MyDrive/2022-01-15_Course_project/new_TF_IDF_clustering_bot_es/n_2gramm_bot_es.csv')"
      ],
      "id": "7ffc9dd9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MztZ5X9m1UT0"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "MztZ5X9m1UT0"
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "name": "5_TF_IDF_Clustering_bot_es.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}