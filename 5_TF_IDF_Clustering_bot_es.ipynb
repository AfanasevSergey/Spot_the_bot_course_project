{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:05:19.638787Z",
     "start_time": "2022-01-20T00:05:19.628854Z"
    },
    "id": "cdb50c2b"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T23:16:44.565625Z",
     "start_time": "2022-01-19T23:16:40.800174Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6120d187",
    "outputId": "d1fd4105-72e9-4510-a670-249d1fa162a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse.linalg import svds\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WRt8kPDwn-_b",
    "outputId": "df1d09cd-ff3e-44e3-ec9d-ea034517cc63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T23:16:51.418514Z",
     "start_time": "2022-01-19T23:16:51.400418Z"
    },
    "id": "3bd2f029"
   },
   "outputs": [],
   "source": [
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5a7425c8",
    "outputId": "6936b15b-810c-439b-b279-9f1194149e57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MaplzLoMnLuZ"
   },
   "outputs": [],
   "source": [
    "# Wishart clustering function\n",
    "# https://github.com/Radi4/BotDetection/blob/master/Wishart.py\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import gamma\n",
    "from sklearn.neighbors import KDTree\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Wishart:\n",
    "    def __init__(self, wishart_neighbors, significance_level):\n",
    "        self.wishart_neighbors = wishart_neighbors  # Number of neighbors\n",
    "        self.significance_level = significance_level  # Significance level\n",
    "\n",
    "    def fit(self, X):\n",
    "        from sklearn.neighbors import KDTree\n",
    "        kdt = KDTree(X, metric='euclidean')\n",
    "\n",
    "        #add one because you are your neighb.\n",
    "        distances, neighbors = kdt.query(X, k = self.wishart_neighbors + 1, return_distance = True)\n",
    "        neighbors = neighbors[:, 1:]\n",
    "\n",
    "\n",
    "        distances = distances[:, -1]\n",
    "        indexes = np.argsort(distances)\n",
    "        \n",
    "        size, dim = X.shape\n",
    "\n",
    "        self.object_labels = np.zeros(size, dtype = int) - 1\n",
    "\n",
    "        #index in tuple\n",
    "        #min_dist, max_dist, flag_to_significant\n",
    "        self.clusters = np.array([(1., 1., 0)])\n",
    "        self.clusters_to_objects = defaultdict(list)\n",
    "        print('Start clustering')\n",
    "\n",
    "        for index in indexes:\n",
    "            neighbors_clusters =\\\n",
    "                np.concatenate([self.object_labels[neighbors[index]], self.object_labels[neighbors[index]]])\n",
    "            unique_clusters = np.unique(neighbors_clusters).astype(int)\n",
    "            unique_clusters = unique_clusters[unique_clusters != -1]\n",
    "\n",
    "\n",
    "            if len(unique_clusters) == 0:\n",
    "                self._create_new_cluster(index, distances[index])\n",
    "            else:\n",
    "                max_cluster = unique_clusters[-1]\n",
    "                min_cluster = unique_clusters[0]\n",
    "                if max_cluster == min_cluster:\n",
    "                    if self.clusters[max_cluster][-1] < 0.5:\n",
    "                        self._add_elem_to_exist_cluster(index, distances[index], max_cluster)\n",
    "                    else:\n",
    "                        self._add_elem_to_noise(index)\n",
    "                else:\n",
    "                    my_clusters = self.clusters[unique_clusters]\n",
    "                    flags = my_clusters[:, -1]\n",
    "                    if np.min(flags) > 0.5:\n",
    "                        self._add_elem_to_noise(index)\n",
    "                    else:\n",
    "                        significan = np.power(my_clusters[:, 0], -dim) - np.power(my_clusters[:, 1], -dim)\n",
    "                        significan *= self.wishart_neighbors\n",
    "                        significan /= size\n",
    "                        significan /= np.power(np.pi, dim / 2)\n",
    "                        significan *= gamma(dim / 2 + 1)\n",
    "                        significan_index = significan >= self.significance_level\n",
    "\n",
    "                        significan_clusters = unique_clusters[significan_index]\n",
    "                        not_significan_clusters = unique_clusters[~significan_index]\n",
    "                        significan_clusters_count = len(significan_clusters)\n",
    "                        if significan_clusters_count > 1 or min_cluster == 0:\n",
    "                            self._add_elem_to_noise(index)\n",
    "                            self.clusters[significan_clusters, -1] = 1\n",
    "                            for not_sig_cluster in not_significan_clusters:\n",
    "                                if not_sig_cluster == 0:\n",
    "                                    continue\n",
    "\n",
    "                                for bad_index in self.clusters_to_objects[not_sig_cluster]:\n",
    "                                    self._add_elem_to_noise(bad_index)\n",
    "                                self.clusters_to_objects[not_sig_cluster].clear()\n",
    "                        else:\n",
    "                            for cur_cluster in unique_clusters:\n",
    "                                if cur_cluster == min_cluster:\n",
    "                                    continue\n",
    "\n",
    "                                for bad_index in self.clusters_to_objects[cur_cluster]:\n",
    "                                    self._add_elem_to_exist_cluster(bad_index, distances[bad_index], min_cluster)\n",
    "                                self.clusters_to_objects[cur_cluster].clear()\n",
    "\n",
    "                            self._add_elem_to_exist_cluster(index, distances[index], min_cluster)\n",
    "\n",
    "        return self.clean_data()\n",
    "\n",
    "    def clean_data(self):\n",
    "        unique = np.unique(self.object_labels)\n",
    "        index = np.argsort(unique)\n",
    "        if unique[0] != 0:\n",
    "            index += 1\n",
    "        true_cluster = {unq :  index for unq, index in zip(unique, index)}\n",
    "        result = np.zeros(len(self.object_labels), dtype = int)\n",
    "        for index, unq in enumerate(self.object_labels):\n",
    "            result[index] = true_cluster[unq]\n",
    "        return result\n",
    "\n",
    "    def _add_elem_to_noise(self, index):\n",
    "        self.object_labels[index] = 0\n",
    "        self.clusters_to_objects[0].append(index)\n",
    "\n",
    "    def _create_new_cluster(self, index, dist):\n",
    "        self.object_labels[index] = len(self.clusters)\n",
    "        self.clusters_to_objects[len(self.clusters)].append(index)\n",
    "        self.clusters = np.append(self.clusters, [(dist, dist, 0)], axis = 0)\n",
    "\n",
    "    def _add_elem_to_exist_cluster(self, index, dist, cluster_label):\n",
    "        self.object_labels[index] = cluster_label\n",
    "        self.clusters_to_objects[cluster_label].append(index)\n",
    "        self.clusters[cluster_label][0] = min(self.clusters[cluster_label][0], dist)\n",
    "        self.clusters[cluster_label][1] = max(self.clusters[cluster_label][1], dist)\n",
    "\n",
    "\n",
    "class PreTrainWishart:\n",
    "    def __init__(self, wishart_neighbors, significance_level, distances, neighbors):\n",
    "        self.wishart_neighbors = wishart_neighbors  # Number of neighbors\n",
    "        self.significance_level = significance_level  # Significance level\n",
    "        self.distances = distances\n",
    "        self.neighbors = neighbors\n",
    "\n",
    "    def fit(self, X):\n",
    "        from sklearn.neighbors import KDTree\n",
    "        kdt = KDTree(X, metric='euclidean')\n",
    "\n",
    "        #add one because you are your neighb.\n",
    "        neighbors = self.neighbors[:, 1 : self.wishart_neighbors + 1]\n",
    "        distances = self.distances[:, self.wishart_neighbors]\n",
    "        indexes = np.argsort(distances)\n",
    "        \n",
    "        size, dim = X.shape\n",
    "\n",
    "        self.object_labels = np.zeros(size, dtype = int) - 1\n",
    "\n",
    "        #index in tuple\n",
    "        #min_dist, max_dist, flag_to_significant\n",
    "        self.clusters = np.array([(1., 1., 0)])\n",
    "        self.clusters_to_objects = defaultdict(list)\n",
    "\n",
    "        for index in indexes:\n",
    "            neighbors_clusters =\\\n",
    "                np.concatenate([self.object_labels[neighbors[index]], self.object_labels[neighbors[index]]])\n",
    "            unique_clusters = np.unique(neighbors_clusters).astype(int)\n",
    "            unique_clusters = unique_clusters[unique_clusters != -1]\n",
    "\n",
    "\n",
    "            if len(unique_clusters) == 0:\n",
    "                self._create_new_cluster(index, distances[index])\n",
    "            else:\n",
    "                max_cluster = unique_clusters[-1]\n",
    "                min_cluster = unique_clusters[0]\n",
    "                if max_cluster == min_cluster:\n",
    "                    if self.clusters[max_cluster][-1] < 0.5:\n",
    "                        self._add_elem_to_exist_cluster(index, distances[index], max_cluster)\n",
    "                    else:\n",
    "                        self._add_elem_to_noise(index)\n",
    "                else:\n",
    "                    my_clusters = self.clusters[unique_clusters]\n",
    "                    flags = my_clusters[:, -1]\n",
    "                    if np.min(flags) > 0.5:\n",
    "                        self._add_elem_to_noise(index)\n",
    "                    else:\n",
    "                        significan = np.power(my_clusters[:, 0], -dim) - np.power(my_clusters[:, 1], -dim)\n",
    "                        significan *= self.wishart_neighbors\n",
    "                        significan /= size\n",
    "                        significan /= np.power(np.pi, dim / 2)\n",
    "                        significan *= gamma(dim / 2 + 1)\n",
    "                        significan_index = significan >= self.significance_level\n",
    "\n",
    "                        significan_clusters = unique_clusters[significan_index]\n",
    "                        not_significan_clusters = unique_clusters[~significan_index]\n",
    "                        significan_clusters_count = len(significan_clusters)\n",
    "                        if significan_clusters_count > 1 or min_cluster == 0:\n",
    "                            self._add_elem_to_noise(index)\n",
    "                            self.clusters[significan_clusters, -1] = 1\n",
    "                            for not_sig_cluster in not_significan_clusters:\n",
    "                                if not_sig_cluster == 0:\n",
    "                                    continue\n",
    "\n",
    "                                for bad_index in self.clusters_to_objects[not_sig_cluster]:\n",
    "                                    self._add_elem_to_noise(bad_index)\n",
    "                                self.clusters_to_objects[not_sig_cluster].clear()\n",
    "                        else:\n",
    "                            for cur_cluster in unique_clusters:\n",
    "                                if cur_cluster == min_cluster:\n",
    "                                    continue\n",
    "\n",
    "                                for bad_index in self.clusters_to_objects[cur_cluster]:\n",
    "                                    self._add_elem_to_exist_cluster(bad_index, distances[bad_index], min_cluster)\n",
    "                                self.clusters_to_objects[cur_cluster].clear()\n",
    "\n",
    "                            self._add_elem_to_exist_cluster(index, distances[index], min_cluster)\n",
    "\n",
    "        return self.clean_data()\n",
    "\n",
    "    def clean_data(self):\n",
    "        unique = np.unique(self.object_labels)\n",
    "        index = np.argsort(unique)\n",
    "        if unique[0] != 0:\n",
    "            index += 1\n",
    "        true_cluster = {unq :  index for unq, index in zip(unique, index)}\n",
    "        result = np.zeros(len(self.object_labels), dtype = int)\n",
    "        for index, unq in enumerate(self.object_labels):\n",
    "            result[index] = true_cluster[unq]\n",
    "        return result\n",
    "\n",
    "    def _add_elem_to_noise(self, index):\n",
    "        self.object_labels[index] = 0\n",
    "        self.clusters_to_objects[0].append(index)\n",
    "\n",
    "    def _create_new_cluster(self, index, dist):\n",
    "        self.object_labels[index] = len(self.clusters)\n",
    "        self.clusters_to_objects[len(self.clusters)].append(index)\n",
    "        self.clusters = np.append(self.clusters, [(dist, dist, 0)], axis = 0)\n",
    "\n",
    "    def _add_elem_to_exist_cluster(self, index, dist, cluster_label):\n",
    "        self.object_labels[index] = cluster_label\n",
    "        self.clusters_to_objects[cluster_label].append(index)\n",
    "        self.clusters[cluster_label][0] = min(self.clusters[cluster_label][0], dist)\n",
    "        self.clusters[cluster_label][1] = max(self.clusters[cluster_label][1], dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7f043e55"
   },
   "source": [
    "## Create a vector representation based on TfidfVectorizer (on human texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:02:39.113091Z",
     "start_time": "2022-01-20T00:02:39.053009Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7b1f3a37",
    "outputId": "fa8d17f7-312d-4770-fa82-d76cd424ad86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'más', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'mí', 'antes', 'algunos', 'qué', 'unos', 'yo', 'otro', 'otras', 'otra', 'él', 'tanto', 'esa', 'estos', 'mucho', 'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tú', 'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosotros', 'vosotras', 'os', 'mío', 'mía', 'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy', 'estás', 'está', 'estamos', 'estáis', 'están', 'esté', 'estés', 'estemos', 'estéis', 'estén', 'estaré', 'estarás', 'estará', 'estaremos', 'estaréis', 'estarán', 'estaría', 'estarías', 'estaríamos', 'estaríais', 'estarían', 'estaba', 'estabas', 'estábamos', 'estabais', 'estaban', 'estuve', 'estuviste', 'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera', 'estuvieras', 'estuviéramos', 'estuvierais', 'estuvieran', 'estuviese', 'estuvieses', 'estuviésemos', 'estuvieseis', 'estuviesen', 'estando', 'estado', 'estada', 'estados', 'estadas', 'estad', 'he', 'has', 'ha', 'hemos', 'habéis', 'han', 'haya', 'hayas', 'hayamos', 'hayáis', 'hayan', 'habré', 'habrás', 'habrá', 'habremos', 'habréis', 'habrán', 'habría', 'habrías', 'habríamos', 'habríais', 'habrían', 'había', 'habías', 'habíamos', 'habíais', 'habían', 'hube', 'hubiste', 'hubo', 'hubimos', 'hubisteis', 'hubieron', 'hubiera', 'hubieras', 'hubiéramos', 'hubierais', 'hubieran', 'hubiese', 'hubieses', 'hubiésemos', 'hubieseis', 'hubiesen', 'habiendo', 'habido', 'habida', 'habidos', 'habidas', 'soy', 'eres', 'es', 'somos', 'sois', 'son', 'sea', 'seas', 'seamos', 'seáis', 'sean', 'seré', 'serás', 'será', 'seremos', 'seréis', 'serán', 'sería', 'serías', 'seríamos', 'seríais', 'serían', 'era', 'eras', 'éramos', 'erais', 'eran', 'fui', 'fuiste', 'fue', 'fuimos', 'fuisteis', 'fueron', 'fuera', 'fueras', 'fuéramos', 'fuerais', 'fueran', 'fuese', 'fueses', 'fuésemos', 'fueseis', 'fuesen', 'sintiendo', 'sentido', 'sentida', 'sentidos', 'sentidas', 'siente', 'sentid', 'tengo', 'tienes', 'tiene', 'tenemos', 'tenéis', 'tienen', 'tenga', 'tengas', 'tengamos', 'tengáis', 'tengan', 'tendré', 'tendrás', 'tendrá', 'tendremos', 'tendréis', 'tendrán', 'tendría', 'tendrías', 'tendríamos', 'tendríais', 'tendrían', 'tenía', 'tenías', 'teníamos', 'teníais', 'tenían', 'tuve', 'tuviste', 'tuvo', 'tuvimos', 'tuvisteis', 'tuvieron', 'tuviera', 'tuvieras', 'tuviéramos', 'tuvierais', 'tuvieran', 'tuviese', 'tuvieses', 'tuviésemos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened']\n"
     ]
    }
   ],
   "source": [
    "# Stopwords\n",
    "print(stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:03:08.870021Z",
     "start_time": "2022-01-20T00:03:08.861187Z"
    },
    "id": "4d3f9554"
   },
   "outputs": [],
   "source": [
    "def make_corpus(input_path, output_file_path):\n",
    "    i = 0\n",
    "    file_list = glob.glob(input_path + '*')\n",
    "    \n",
    "    with open(output_file_path, 'w+') as output_file:\n",
    "        for file in file_list:\n",
    "            if i % 500 == 0:\n",
    "                now = datetime.now()\n",
    "                dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "                print(dt_string, '| ',  'number of processed files: ' + str(i), '| ', \n",
    "                      'percentage of completion:', str(round(i/len(file_list), 2)* 100) + ' %' )\n",
    "            i+=1\n",
    "            with open(file, 'r') as input_file:\n",
    "                output_file.write(input_file.read().replace('\\n', ' '))\n",
    "                output_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GEKONTR6eLwe",
    "outputId": "fc19ef6d-263c-40a7-deed-0cbc6a62acf0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glob.glob('/content/drive/MyDrive/2022-01-15_Course_project/prep_gen_text_es/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Na8qCNCnd3jZ"
   },
   "outputs": [],
   "source": [
    "# Let's select 10k texts in a folder: '/content/drive/MyDrive/2022-01-15_Course_project/10000_cut_es/'\n",
    "# Because clustering works for a very long time on large datasets\n",
    "\n",
    "import shutil\n",
    "file_list = glob.glob('/content/drive/MyDrive/2022-01-15_Course_project/prep_gen_text_es/*')\n",
    "k = 0\n",
    "\n",
    "for i in file_list:\n",
    "  shutil.copy(i, '/content/drive/MyDrive/2022-01-15_Course_project/10000_prep_gen_text_es/')\n",
    "  k += 1\n",
    "  if k >= 10000:\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XjFcRQcwd3st",
    "outputId": "093f4e47-4847-4fc3-af6c-f3f62f9f910d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glob.glob('/content/drive/MyDrive/2022-01-15_Course_project/10000_prep_gen_text_es/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gY0zi4OpuaBy",
    "outputId": "031e1581-ce7d-41f6-a92d-c87fc82e6092"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/01/2022 21:49:07 |  number of processed files: 0 |  percentage of completion: 0.0 %\n",
      "22/01/2022 21:49:11 |  number of processed files: 500 |  percentage of completion: 5.0 %\n",
      "22/01/2022 21:49:11 |  number of processed files: 1000 |  percentage of completion: 10.0 %\n",
      "22/01/2022 21:49:12 |  number of processed files: 1500 |  percentage of completion: 15.0 %\n",
      "22/01/2022 21:49:12 |  number of processed files: 2000 |  percentage of completion: 20.0 %\n",
      "22/01/2022 21:49:13 |  number of processed files: 2500 |  percentage of completion: 25.0 %\n",
      "22/01/2022 21:49:13 |  number of processed files: 3000 |  percentage of completion: 30.0 %\n",
      "22/01/2022 21:49:13 |  number of processed files: 3500 |  percentage of completion: 35.0 %\n",
      "22/01/2022 21:49:14 |  number of processed files: 4000 |  percentage of completion: 40.0 %\n",
      "22/01/2022 21:49:14 |  number of processed files: 4500 |  percentage of completion: 45.0 %\n",
      "22/01/2022 21:49:15 |  number of processed files: 5000 |  percentage of completion: 50.0 %\n",
      "22/01/2022 21:49:15 |  number of processed files: 5500 |  percentage of completion: 55.00000000000001 %\n",
      "22/01/2022 21:49:15 |  number of processed files: 6000 |  percentage of completion: 60.0 %\n",
      "22/01/2022 21:49:16 |  number of processed files: 6500 |  percentage of completion: 65.0 %\n",
      "22/01/2022 21:49:16 |  number of processed files: 7000 |  percentage of completion: 70.0 %\n",
      "22/01/2022 21:49:16 |  number of processed files: 7500 |  percentage of completion: 75.0 %\n",
      "22/01/2022 21:49:17 |  number of processed files: 8000 |  percentage of completion: 80.0 %\n",
      "22/01/2022 21:49:17 |  number of processed files: 8500 |  percentage of completion: 85.0 %\n",
      "22/01/2022 21:49:18 |  number of processed files: 9000 |  percentage of completion: 90.0 %\n",
      "22/01/2022 21:49:18 |  number of processed files: 9500 |  percentage of completion: 95.0 %\n"
     ]
    }
   ],
   "source": [
    "# Let's make corpus for bot texts\n",
    "\n",
    "make_corpus('/content/drive/MyDrive/2022-01-15_Course_project/10000_prep_gen_text_es/',\n",
    "            '/content/drive/MyDrive/2022-01-15_Course_project/TF_IDF_clustering_bot_es/10000_dataset_generate_es.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:03:42.696966Z",
     "start_time": "2022-01-20T00:03:42.639936Z"
    },
    "id": "07c89c06"
   },
   "outputs": [],
   "source": [
    "# TF-IDF corpus\n",
    "\n",
    "def make_table_and_dict(corpus_path, min_df, max_df, token_pattern = None, use_idf = True, stop_words = 'spanish'):\n",
    "    \n",
    "    with open(corpus_path, 'r') as corpus_file:\n",
    "        if token_pattern:\n",
    "            vectorizer = TfidfVectorizer(analyzer='word', min_df=min_df, token_pattern=token_pattern, use_idf=use_idf)\n",
    "        else:\n",
    "            vectorizer = TfidfVectorizer(analyzer='word', min_df=min_df)\n",
    "        data_vectorized = vectorizer.fit_transform(corpus_file)\n",
    "    return data_vectorized, vectorizer.get_feature_names(), vectorizer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:03:43.839418Z",
     "start_time": "2022-01-20T00:03:43.817476Z"
    },
    "id": "8f4de5bf"
   },
   "outputs": [],
   "source": [
    "def create_table(data_vectorized, k, name, path):\n",
    "    u, sigma, vt = svds(data_vectorized, k)\n",
    "    print(sigma)\n",
    "    dict_ = np.dot(np.diag(sigma), vt).T\n",
    "        \n",
    "    with open(path + name + str(k) + '.pkl', 'wb') as f:\n",
    "        pickle.dump(dict_, f)\n",
    "    return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "JALQb0THBpJl"
   },
   "outputs": [],
   "source": [
    "# Vector representation on human texts (used on the bot)\n",
    "f = open('/content/drive/MyDrive/2022-01-15_Course_project/TF_IDF_clustering_es/10000_TF_IDF_cut_human_es.pkl', 'rb')\n",
    "dict_cut = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:06:30.784942Z",
     "start_time": "2022-01-20T00:06:30.769056Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "895287d1",
    "outputId": "ea1af40f-56f3-404e-8c0d-f09b5167a40d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1310"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_cut.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ce276f30"
   },
   "source": [
    "# Making n-grams and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:06:36.788059Z",
     "start_time": "2022-01-20T00:06:36.765360Z"
    },
    "id": "3edea2f9"
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from math import log\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics import calinski_harabasz_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:06:39.611702Z",
     "start_time": "2022-01-20T00:06:39.591675Z"
    },
    "id": "8b9b8248"
   },
   "outputs": [],
   "source": [
    "def divide(data, labels):\n",
    "    clusters = set(labels)\n",
    "    clusters_data = []\n",
    "    for cluster in clusters:\n",
    "        clusters_data.append(data[labels == cluster, :])\n",
    "    return clusters_data\n",
    "\n",
    "def get_centroids(clusters):\n",
    "    centroids = []\n",
    "    for cluster_data in clusters:\n",
    "        centroids.append(cluster_data.mean(axis=0))\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:06:40.279933Z",
     "start_time": "2022-01-20T00:06:40.241868Z"
    },
    "id": "9eb06fd2"
   },
   "outputs": [],
   "source": [
    "def cohesion(data, labels):\n",
    "    clusters = sorted(set(labels))\n",
    "    sse = 0\n",
    "    for cluster in clusters:\n",
    "        cluster_data = data[labels == cluster, :]\n",
    "        centroid = cluster_data.mean(axis = 0)\n",
    "        sse += ((cluster_data - centroid)**2).sum()\n",
    "    return sse\n",
    "\n",
    "def separation(data, labels, cohesion_score):\n",
    "    # calculate separation as SST - SSE\n",
    "    return cohesion(data, np.zeros(data.shape[0])) - cohesion_score\n",
    "\n",
    "def SST(data):\n",
    "    c = get_centroids([data])\n",
    "    return ((data - c) ** 2).sum()\n",
    "\n",
    "def SSE(clusters, centroids):\n",
    "    result = 0\n",
    "    for cluster, centroid in zip(clusters, centroids):\n",
    "        result += ((cluster - centroid) ** 2).sum()\n",
    "    return result\n",
    "\n",
    "# Clear the store before running each time\n",
    "within_cluster_dist_sum_store = {}\n",
    "def within_cluster_dist_sum(cluster, centroid, cluster_id):\n",
    "    if cluster_id in within_cluster_dist_sum_store:\n",
    "        return within_cluster_dist_sum_store[cluster_id]\n",
    "    else:\n",
    "        result = (((cluster - centroid) ** 2).sum(axis=1)**.5).sum()\n",
    "        within_cluster_dist_sum_store[cluster_id] = result\n",
    "    return result\n",
    "\n",
    "def RMSSTD(data, clusters, centroids):\n",
    "    df = data.shape[0] - len(clusters)\n",
    "    attribute_num = data.shape[1]\n",
    "    return (SSE(clusters, centroids) / (attribute_num * df)) ** .5\n",
    "\n",
    "# equal to separation / (cohesion + separation)\n",
    "def RS(data, clusters, centroids):\n",
    "    sst = SST(data)\n",
    "    sse = SSE(clusters, centroids)\n",
    "    return (sst - sse) / sst\n",
    "\n",
    "def DB_find_max_j(clusters, centroids, i):\n",
    "    max_val = 0\n",
    "    max_j = 0\n",
    "    for j in range(len(clusters)):\n",
    "        if j == i:\n",
    "            continue\n",
    "        cluster_i_stat = within_cluster_dist_sum(clusters[i], centroids[i], i) / clusters[i].shape[0]\n",
    "        cluster_j_stat = within_cluster_dist_sum(clusters[j], centroids[j], j) / clusters[j].shape[0]\n",
    "        val = (cluster_i_stat + cluster_j_stat) / (((centroids[i] - centroids[j]) ** 2).sum() ** .5)\n",
    "        if val > max_val:\n",
    "            max_val = val\n",
    "            max_j = j\n",
    "    return max_val\n",
    "\n",
    "def DB(data, clusters, centroids):\n",
    "    result = 0\n",
    "    for i in range(len(clusters)):\n",
    "        result += DB_find_max_j(clusters, centroids, i)\n",
    "    return result / len(clusters)\n",
    "\n",
    "def XB(data, clusters, centroids):\n",
    "    sse = SSE(clusters, centroids)\n",
    "    min_dist = ((centroids[0] - centroids[1]) ** 2).sum()\n",
    "    for centroid_i, centroid_j in list(product(centroids, centroids)):\n",
    "        if (centroid_i - centroid_j).sum() == 0:\n",
    "            continue\n",
    "        dist = ((centroid_i - centroid_j) ** 2).sum()\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "    return sse / (data.shape[0] * min_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T17:23:21.939658Z",
     "start_time": "2022-01-20T17:23:21.853300Z"
    },
    "id": "93e7bacc"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "# Some metrics can work for a very long time (commented out)\n",
    "\n",
    "def get_validation_scores(data, labels, max_clust = None):\n",
    "    #within_cluster_dist_sum_store.clear()\n",
    "    \n",
    "    clusters = divide(data, labels)\n",
    "    centroids = get_centroids(clusters)\n",
    "    \n",
    "    scores = {}\n",
    "    if max_clust:\n",
    "        if len(clusters) > max_clust:\n",
    "            scores['cohesion'] = cohesion(data, labels)\n",
    "            scores['separation'] = separation(data, labels, scores['cohesion'])\n",
    "            scores['calinski_harabaz_score'] = None\n",
    "            scores['RMSSTD'] = RMSSTD(data, clusters, centroids)\n",
    "            scores['RS'] = RS(data, clusters, centroids)\n",
    "            #scores['DB'] = None\n",
    "            #scores['XB'] = XB(data, clusters, centroids)\n",
    "            scores['silhouette'] = None\n",
    "        else:\n",
    "            scores['cohesion'] = cohesion(data, labels)\n",
    "            scores['separation'] = separation(data, labels, scores['cohesion'])\n",
    "            scores['calinski_harabaz_score'] = calinski_harabasz_score(data, labels)\n",
    "            scores['RMSSTD'] = RMSSTD(data, clusters, centroids)\n",
    "            scores['RS'] = RS(data, clusters, centroids)\n",
    "            #scores['DB'] = DB(data, clusters, centroids)\n",
    "            #scores['XB'] = XB(data, clusters, centroids)\n",
    "            scores['silhouette'] = silhouette_score(data, labels)\n",
    "    else:\n",
    "        scores['cohesion'] = cohesion(data, labels)\n",
    "        scores['separation'] = separation(data, labels, scores['cohesion'])\n",
    "        scores['calinski_harabaz_score'] = calinski_harabasz_score(data, labels)\n",
    "        scores['RMSSTD'] = RMSSTD(data, clusters, centroids)\n",
    "        scores['RS'] = RS(data, clusters, centroids)\n",
    "        #scores['DB'] = DB(data, clusters, centroids)\n",
    "        #scores['XB'] = XB(data, clusters, centroids)\n",
    "        scores['silhouette'] = silhouette_score(data, labels)\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:06:52.939414Z",
     "start_time": "2022-01-20T00:06:52.923459Z"
    },
    "id": "4a8e01ac"
   },
   "outputs": [],
   "source": [
    "def make_ngrams(input_corpus,  dict_, N = 2, m = None, uniq = False):\n",
    "    dict_grams = dict()\n",
    "    num_ = 0\n",
    "    i = 0\n",
    "    j = 0\n",
    "    \n",
    "    print('Count documents: ', len(input_corpus))\n",
    "    for sentence in input_corpus:\n",
    "        sentence = sentence.split(' ')\n",
    "        grams = [sentence[i:i+N] for i in range(len(sentence)-N+1)]\n",
    "        for g in grams:\n",
    "            g_key = '_'.join(elem for elem in g)\n",
    "\n",
    "            if uniq:\n",
    "                if all(elem in dict_.keys()  for elem in g) and (g_key not in dict_grams.keys()):\n",
    "                    dict_grams[g_key] = []\n",
    "                    for elem in g:\n",
    "                            if m:\n",
    "                                dict_grams[g_key] += list(dict_[elem][1][:m])\n",
    "                            else:\n",
    "                                dict_grams[g_key] += list(dict_[elem][1])\n",
    "            else:\n",
    "                if all(elem in dict_.keys()  for elem in g):\n",
    "                    concat = []\n",
    "                    for elem in g:\n",
    "                        if m:\n",
    "                            concat += list(dict_[elem][1][:m])\n",
    "                        else:\n",
    "                            concat += list(dict_[elem][1])\n",
    "                    dict_grams[i] = (j, g_key, concat)\n",
    "                    i += 1\n",
    "            j += 1\n",
    "       \n",
    "            \n",
    "        if num_ % 500 == 0:\n",
    "            now = datetime.now()\n",
    "            dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "            print(dt_string, '| ',  'number of processed documents: ' + str(num_), '| ', \n",
    "                      'percentage of completion:', str(round(num_/len(input_corpus), 2)* 100) + ' %' )\n",
    "        num_ += 1\n",
    "    return dict_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:07:43.802071Z",
     "start_time": "2022-01-20T00:07:43.415738Z"
    },
    "id": "8eae3315"
   },
   "outputs": [],
   "source": [
    "with open('/content/drive/MyDrive/2022-01-15_Course_project/TF_IDF_clustering_bot_es/10000_dataset_generate_es.txt', 'r') as corpus_file:\n",
    "    corpus = corpus_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:09:15.616274Z",
     "start_time": "2022-01-20T00:07:51.301224Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3b29d21d",
    "outputId": "0b0cf867-ec4b-46ed-9b9d-9c91227b5a2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count documents:  10000\n",
      "22/01/2022 21:50:15 |  number of processed documents: 0 |  percentage of completion: 0.0 %\n",
      "22/01/2022 21:50:17 |  number of processed documents: 500 |  percentage of completion: 5.0 %\n",
      "22/01/2022 21:50:19 |  number of processed documents: 1000 |  percentage of completion: 10.0 %\n",
      "22/01/2022 21:50:21 |  number of processed documents: 1500 |  percentage of completion: 15.0 %\n",
      "22/01/2022 21:50:23 |  number of processed documents: 2000 |  percentage of completion: 20.0 %\n",
      "22/01/2022 21:50:25 |  number of processed documents: 2500 |  percentage of completion: 25.0 %\n",
      "22/01/2022 21:50:27 |  number of processed documents: 3000 |  percentage of completion: 30.0 %\n",
      "22/01/2022 21:50:29 |  number of processed documents: 3500 |  percentage of completion: 35.0 %\n",
      "22/01/2022 21:50:31 |  number of processed documents: 4000 |  percentage of completion: 40.0 %\n",
      "22/01/2022 21:50:32 |  number of processed documents: 4500 |  percentage of completion: 45.0 %\n",
      "22/01/2022 21:50:34 |  number of processed documents: 5000 |  percentage of completion: 50.0 %\n",
      "22/01/2022 21:50:36 |  number of processed documents: 5500 |  percentage of completion: 55.00000000000001 %\n",
      "22/01/2022 21:50:38 |  number of processed documents: 6000 |  percentage of completion: 60.0 %\n",
      "22/01/2022 21:50:40 |  number of processed documents: 6500 |  percentage of completion: 65.0 %\n",
      "22/01/2022 21:50:42 |  number of processed documents: 7000 |  percentage of completion: 70.0 %\n",
      "22/01/2022 21:50:44 |  number of processed documents: 7500 |  percentage of completion: 75.0 %\n",
      "22/01/2022 21:50:46 |  number of processed documents: 8000 |  percentage of completion: 80.0 %\n",
      "22/01/2022 21:50:47 |  number of processed documents: 8500 |  percentage of completion: 85.0 %\n",
      "22/01/2022 21:50:49 |  number of processed documents: 9000 |  percentage of completion: 90.0 %\n",
      "22/01/2022 21:50:51 |  number of processed documents: 9500 |  percentage of completion: 95.0 %\n"
     ]
    }
   ],
   "source": [
    "dict_grams_bot = make_ngrams(corpus,  dict_cut, N = 2, m = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T10:05:04.379102Z",
     "start_time": "2022-01-20T10:05:04.303378Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "20e4a2cd",
    "outputId": "e2aad6c0-8a45-4020-803e-8b584b028a9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "487795"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_grams_bot.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "27febfd3"
   },
   "outputs": [],
   "source": [
    "X0 = []\n",
    "for i in dict_grams_bot.keys():\n",
    "    X0.append( dict_grams_bot[i][2])\n",
    "\n",
    "list_gramm = [dict_grams_bot[i][1] for i in dict_grams_bot.keys()]\n",
    "    \n",
    "X_bot = pd.DataFrame(X0)\n",
    "X_bot['ind'] = dict_grams_bot.keys()\n",
    "X_bot['name'] = list_gramm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T10:07:16.799996Z",
     "start_time": "2022-01-20T10:06:40.865Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b90f609b",
    "outputId": "0fb2beef-128c-4440-8757-068a042a720a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a_mi          13012\n",
       "llegar_a      10668\n",
       "pie_ligero     6168\n",
       "llevar_a       4501\n",
       "a_tu           4420\n",
       "Name: name, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bot['name'].value_counts()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T10:07:16.801992Z",
     "start_time": "2022-01-20T10:06:41.199Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d07090f8",
    "outputId": "4949771b-a162-48f3-fc27-408328fc2a9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(487795, 22)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:11:28.047560Z",
     "start_time": "2022-01-20T00:10:38.106966Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d995d904",
    "outputId": "fdffb333-28f1-4283-ef12-4e37778a819b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(487795, 22)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bot.to_csv('/content/drive/MyDrive/2022-01-15_Course_project/TF_IDF_clustering_bot_es/10000_n_2gramm_bot_es.csv')\n",
    "X_bot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "a16b0d8a"
   },
   "outputs": [],
   "source": [
    "list_col = list(X_bot.columns)\n",
    "for i in ['Unnamed: 0', 'ind', 'name']:\n",
    "    if i in list_col:\n",
    "        list_col.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ffc9dd9",
    "outputId": "1d0b851e-4a4d-4100-8ca3-6d14e4f915a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-22 21:51:54.239 | begin | significance:  1000 | neighbors:  50\n",
      "Start clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: divide by zero encountered in power\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-22 22:19:34.749 | end | {'cohesion': 6894.231344275017, 'separation': 9496.168999144247, 'calinski_harabaz_score': None, 'RMSSTD': 0.02948680328095718, 'RS': 0.5793738285933299, 'silhouette': None, 'significance': 1000, 'neighbors': 50, 'cluster_num': 91334}\n",
      "2022-01-22 22:19:54.449 | begin | significance:  1000 | neighbors:  100\n",
      "Start clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: divide by zero encountered in power\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-22 22:52:38.197 | end | {'cohesion': 6929.649070743622, 'separation': 9460.751272675641, 'calinski_harabaz_score': None, 'RMSSTD': 0.028341885707917996, 'RS': 0.577212946264252, 'silhouette': None, 'significance': 1000, 'neighbors': 100, 'cluster_num': 56451}\n",
      "2022-01-22 22:52:58.656 | begin | significance:  100000 | neighbors:  50\n",
      "Start clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: divide by zero encountered in power\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-22 23:19:11.680 | end | {'cohesion': 6894.231344275017, 'separation': 9496.168999144247, 'calinski_harabaz_score': None, 'RMSSTD': 0.02948680328095718, 'RS': 0.5793738285933299, 'silhouette': None, 'significance': 100000, 'neighbors': 50, 'cluster_num': 91334}\n",
      "2022-01-22 23:19:31.518 | begin | significance:  100000 | neighbors:  100\n",
      "Start clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: divide by zero encountered in power\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-22 23:52:26.034 | end | {'cohesion': 6929.649070743622, 'separation': 9460.751272675641, 'calinski_harabaz_score': None, 'RMSSTD': 0.028341885707917996, 'RS': 0.577212946264252, 'silhouette': None, 'significance': 100000, 'neighbors': 100, 'cluster_num': 56451}\n"
     ]
    }
   ],
   "source": [
    "#GridSearch for Clustering\n",
    "grid_result = []\n",
    "for sig in [1000, 100000]:\n",
    "    for nei in [50, 100]:\n",
    "        \n",
    "        print(datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3], '| begin |', 'significance: ', sig, '| neighbors: ', nei )\n",
    "        clust = Wishart(significance_level = sig, wishart_neighbors = nei)\n",
    "        result = clust.fit(X_bot[list_col])\n",
    "        dict_r = get_validation_scores(np.array(X_bot[list_col]), clust.object_labels, max_clust = 10000)\n",
    "        dict_r['significance'] = sig\n",
    "        dict_r['neighbors'] = nei\n",
    "        dict_r['cluster_num'] = len(set(clust.object_labels))\n",
    "        grid_result.append(dict_r)\n",
    "        \n",
    "        #add clustering result to table\n",
    "        name_col = 'cluster_' + str(sig) + str(nei)\n",
    "        X_bot[name_col] = clust.object_labels\n",
    "        \n",
    "        print(datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3], '| end |',  dict_r)\n",
    "\n",
    "        X_bot.to_csv('/content/drive/MyDrive/2022-01-15_Course_project/TF_IDF_clustering_bot_es/10000_n_2gramm_bot_es.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MztZ5X9m1UT0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "name": "TF_IDF_Clustering_bot_es.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
