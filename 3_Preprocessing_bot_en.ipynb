{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SXghWytGaz4E"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from google.colab import drive\n",
    "import os, zipfile, glob\n",
    "import pickle\n",
    "import re\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8gGPuN1LbDYP",
    "outputId": "c5d98b80-aaff-4fd8-8588-dba72ac64919"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PKpR-CJykbXC",
    "outputId": "b3a0c9f7-89fa-4abe-970c-6377ba296851"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unidecode\n",
      "  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\n",
      "\u001b[?25l\r",
      "\u001b[K     |█▍                              | 10 kB 13.3 MB/s eta 0:00:01\r",
      "\u001b[K     |██▉                             | 20 kB 12.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████▏                           | 30 kB 11.9 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▋                          | 40 kB 9.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 51 kB 11.1 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▍                       | 61 kB 11.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▊                      | 71 kB 10.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▏                    | 81 kB 11.5 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▌                   | 92 kB 10.9 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 102 kB 11.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▎                | 112 kB 11.5 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▊               | 122 kB 11.5 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 133 kB 11.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▌            | 143 kB 11.5 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▉           | 153 kB 11.5 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▎         | 163 kB 11.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▋        | 174 kB 11.5 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 184 kB 11.5 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▍     | 194 kB 11.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▉    | 204 kB 11.5 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 215 kB 11.5 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▋ | 225 kB 11.5 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 235 kB 11.5 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 235 kB 11.5 MB/s \n",
      "\u001b[?25hInstalling collected packages: unidecode\n",
      "Successfully installed unidecode-1.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ZTBdWWP1BSWC"
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "from bs4 import BeautifulSoup\n",
    "import unidecode\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nBe-Ea4A7IJ-",
    "outputId": "2220632f-c4d6-42e6-dc74-0e1a7cb17593"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.7.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jbDVXtbK7V0Z",
    "outputId": "647faccc-dae8-4829-d674-3d4e498ddb2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Downloading contractions-0.0.58-py2.py3-none-any.whl (8.0 kB)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
      "Collecting anyascii\n",
      "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
      "\u001b[K     |████████████████████████████████| 284 kB 12.1 MB/s \n",
      "\u001b[?25hCollecting pyahocorasick\n",
      "  Downloading pyahocorasick-1.4.2.tar.gz (321 kB)\n",
      "\u001b[K     |████████████████████████████████| 321 kB 48.1 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
      "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85441 sha256=ca90ce9297db38c02c859b47e6546c6d39ea4615c54072c2ee32dfd5c8717415\n",
      "  Stored in directory: /root/.cache/pip/wheels/25/19/a6/8f363d9939162782bb8439d886469756271abc01f76fbd790f\n",
      "Successfully built pyahocorasick\n",
      "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.0 contractions-0.0.58 pyahocorasick-1.4.2 textsearch-0.0.21\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "i5Fcyejg7IM-"
   },
   "outputs": [],
   "source": [
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "IZFKBq9B7Tt4"
   },
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won['’‘`]t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can['’‘`]t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"ain['’‘`]t\", \"am not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n['’‘`]t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"['’‘`]re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"['’‘`]s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"['’‘`]d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"['’‘`]ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"['’‘`]t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"['’‘`]ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"['’‘`]m\", \" am\", phrase)\n",
    "    \n",
    "    \n",
    "    #expand shortened words, e.g. don't to do not\n",
    "    #optional, spaCy’s tokenization and lemmatization functions will perform the same effect \n",
    "    #to expand contractions such as can’t and don’t. \n",
    "    #The slight difference is that spaCy will expand “we’re” to “we be” while pycontractions will give result “we are”\n",
    "    phrase = contractions.fix(phrase)\n",
    "\n",
    "    phrase = re.sub(r'[^\\w.?!;]', ' ', phrase)\n",
    "    phrase = re.sub(' +', ' ', phrase)\n",
    "    \n",
    "    #remove html tags from text\n",
    "    soup = BeautifulSoup(phrase, \"html.parser\")\n",
    "    phrase = soup.get_text(separator=\" \")\n",
    "    \n",
    "    #remove accented characters from text, e.g. café\n",
    "    phrase = unidecode.unidecode(phrase)\n",
    "    \n",
    "    \n",
    "    \n",
    "    sentences = re.split('([.;!?] *)', phrase)\n",
    "\n",
    "    return ' '.join([i.capitalize() for i in  sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hTRTmwU3lUGq",
    "outputId": "0d1da32a-f972-4f0b-d4b8-fdd03fbca09c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.2.5\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0 MB 13.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.9.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Modules for languages\n",
    "# en_core_web_sm - for English\n",
    "# it_core_news_sm - for Italian\n",
    "# es_core_news_sm - for Spanish\n",
    "\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "zXzdH8ShmuFD"
   },
   "outputs": [],
   "source": [
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "EnR_nWi9mWK-"
   },
   "outputs": [],
   "source": [
    "# initializing the language model\n",
    "\n",
    "nlp = en_core_web_sm.load(disable=['parser'])\n",
    "nlp.max_length = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Mz90fYDi7eWb"
   },
   "outputs": [],
   "source": [
    "def prepare_english_text(input_path, output_path, error_list, nlp, enc = 'utf-8', gutenberg = True, title = ''):\n",
    "    \n",
    "    #English pipeline optimized for CPU. Components: tok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizer.\n",
    "    \n",
    "    #parser: The dependency parser jointly learns sentence segmentation and labelled dependency parsing, \n",
    "    #and can optionally learn to merge tokens that had been over-segmented by the tokenizer.\n",
    "        \n",
    "    pos_dict = {'PROPN': 'PERSON1', 'PRON': 'PRON1', 'NUM': 'ORDINAL1'}\n",
    "\n",
    "    \n",
    "    try:\n",
    "        raw_text = codecs.open(input_path, 'r', enc).read()\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "    if gutenberg:\n",
    "        begin = raw_text.find('*** START OF THIS PROJECT GUTENBERG EBOOK ')\n",
    "        begin_2 = begin + 42 + len(title) + 3\n",
    "        if begin == -1:\n",
    "                begin = raw_text.find('START OF THE PROJECT GUTENBERG EBOOK ')\n",
    "                begin_2 = begin + 37 + len(title) + 3\n",
    "\n",
    "        end = raw_text.find('*** END OF THIS PROJECT GUTENBERG EBOOK')\n",
    "        if end == -1:\n",
    "            end = raw_text.find('*** END')\n",
    "        raw_text = raw_text[begin_2 : end]\n",
    "        \n",
    "    \n",
    "    preprocessed_text = decontracted(raw_text)\n",
    "    \n",
    "    nlp_doc = nlp(preprocessed_text)\n",
    "    sorted_ents = sorted(nlp_doc.ents, key = lambda x: len(x), reverse =  True)\n",
    "\n",
    "\n",
    "    for ent in sorted_ents:\n",
    "        preprocessed_text = preprocessed_text.replace(' ' + ent.text + ' ', ' ' + ent.label_+ '1 ')\n",
    "        \n",
    "        if not ent.text.islower():\n",
    "            preprocessed_text = preprocessed_text.replace(' ' + ent.text.lower() + ' ', ' ' + ent.label_+ '1 ')\n",
    "\n",
    "    new_nlp_doc = nlp(preprocessed_text)\n",
    "    file_name = path.split('/')[-1]\n",
    "    \n",
    "\n",
    "    with open(output_path + file_name, 'w+', ) as prepared_text:\n",
    "        for token in new_nlp_doc:\n",
    "            if token.text[-1] != '1':\n",
    "                if token.pos_ in pos_dict:\n",
    "                    try:\n",
    "                        prepared_text.write(pos_dict[token.pos_])\n",
    "                    except:\n",
    "                        error_list.append(token.pos_)\n",
    "                    prepared_text.write('\\n')\n",
    "                    \n",
    "                        \n",
    "                    \n",
    "                elif token.pos_ != 'PUNCT':\n",
    "                    try:\n",
    "                        prepared_text.write(token.lemma_.lower())\n",
    "                    except:\n",
    "                        error_list.append(token.pos_)\n",
    "                        \n",
    "                    prepared_text.write('\\n')\n",
    "                    \n",
    "\n",
    "            else:\n",
    "                try:\n",
    "                    repared_text.write(token.text)\n",
    "                except:\n",
    "                        error_list.append(token.pos_)\n",
    "                prepared_text.write('\\n')\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "u_mgAWa77hI2"
   },
   "outputs": [],
   "source": [
    "ind_gen = list(glob.glob(\"/content/drive/MyDrive/2022-01-15_Course_project/gen_text_en/*.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QqKKpynH7h86",
    "outputId": "108c03f7-6139-4f42-b21f-f3e8ac06ead7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/content/drive/MyDrive/2022-01-15_Course_project/gen_text_en/doc_14000.txt',\n",
       " '/content/drive/MyDrive/2022-01-15_Course_project/gen_text_en/doc_14001.txt',\n",
       " '/content/drive/MyDrive/2022-01-15_Course_project/gen_text_en/doc_14002.txt',\n",
       " '/content/drive/MyDrive/2022-01-15_Course_project/gen_text_en/doc_14003.txt',\n",
       " '/content/drive/MyDrive/2022-01-15_Course_project/gen_text_en/doc_14004.txt',\n",
       " '/content/drive/MyDrive/2022-01-15_Course_project/gen_text_en/doc_14005.txt',\n",
       " '/content/drive/MyDrive/2022-01-15_Course_project/gen_text_en/doc_14006.txt',\n",
       " '/content/drive/MyDrive/2022-01-15_Course_project/gen_text_en/doc_14007.txt',\n",
       " '/content/drive/MyDrive/2022-01-15_Course_project/gen_text_en/doc_14008.txt',\n",
       " '/content/drive/MyDrive/2022-01-15_Course_project/gen_text_en/doc_14009.txt']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_gen[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uo1uItmX7h_1",
    "outputId": "a970e7ef-1d9a-4075-f67b-14fba0e7733b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/01/2022 19:20:36 |  number of processed files: 0\n",
      "18/01/2022 19:21:52 |  number of processed files: 100\n",
      "18/01/2022 19:23:07 |  number of processed files: 200\n",
      "18/01/2022 19:24:22 |  number of processed files: 300\n",
      "18/01/2022 19:25:37 |  number of processed files: 400\n",
      "18/01/2022 19:26:51 |  number of processed files: 500\n",
      "18/01/2022 19:28:05 |  number of processed files: 600\n",
      "18/01/2022 19:29:19 |  number of processed files: 700\n",
      "18/01/2022 19:30:32 |  number of processed files: 800\n",
      "18/01/2022 19:31:46 |  number of processed files: 900\n",
      "18/01/2022 19:33:07 |  number of processed files: 1000\n",
      "18/01/2022 19:34:21 |  number of processed files: 1100\n",
      "18/01/2022 19:35:35 |  number of processed files: 1200\n",
      "18/01/2022 19:36:49 |  number of processed files: 1300\n",
      "18/01/2022 19:38:05 |  number of processed files: 1400\n",
      "18/01/2022 19:39:18 |  number of processed files: 1500\n",
      "18/01/2022 19:40:32 |  number of processed files: 1600\n",
      "18/01/2022 19:41:44 |  number of processed files: 1700\n",
      "18/01/2022 19:42:57 |  number of processed files: 1800\n",
      "18/01/2022 19:44:10 |  number of processed files: 1900\n",
      "18/01/2022 19:45:22 |  number of processed files: 2000\n",
      "18/01/2022 19:46:36 |  number of processed files: 2100\n",
      "18/01/2022 19:47:49 |  number of processed files: 2200\n",
      "18/01/2022 19:49:02 |  number of processed files: 2300\n",
      "18/01/2022 19:50:15 |  number of processed files: 2400\n",
      "18/01/2022 19:51:28 |  number of processed files: 2500\n",
      "18/01/2022 19:52:41 |  number of processed files: 2600\n",
      "18/01/2022 19:53:55 |  number of processed files: 2700\n",
      "18/01/2022 19:55:16 |  number of processed files: 2800\n",
      "18/01/2022 19:56:29 |  number of processed files: 2900\n",
      "18/01/2022 19:57:44 |  number of processed files: 3000\n",
      "18/01/2022 19:58:58 |  number of processed files: 3100\n",
      "18/01/2022 20:00:11 |  number of processed files: 3200\n",
      "18/01/2022 20:01:25 |  number of processed files: 3300\n",
      "18/01/2022 20:02:37 |  number of processed files: 3400\n",
      "18/01/2022 20:03:50 |  number of processed files: 3500\n",
      "18/01/2022 20:05:04 |  number of processed files: 3600\n",
      "18/01/2022 20:06:17 |  number of processed files: 3700\n",
      "18/01/2022 20:07:31 |  number of processed files: 3800\n",
      "18/01/2022 20:08:45 |  number of processed files: 3900\n",
      "18/01/2022 20:10:00 |  number of processed files: 4000\n",
      "18/01/2022 20:11:25 |  number of processed files: 4100\n",
      "18/01/2022 20:12:38 |  number of processed files: 4200\n",
      "18/01/2022 20:13:51 |  number of processed files: 4300\n",
      "18/01/2022 20:15:03 |  number of processed files: 4400\n",
      "18/01/2022 20:16:16 |  number of processed files: 4500\n",
      "18/01/2022 20:17:29 |  number of processed files: 4600\n",
      "18/01/2022 20:18:42 |  number of processed files: 4700\n",
      "18/01/2022 20:19:55 |  number of processed files: 4800\n",
      "18/01/2022 20:21:10 |  number of processed files: 4900\n",
      "18/01/2022 20:22:22 |  number of processed files: 5000\n",
      "18/01/2022 20:23:37 |  number of processed files: 5100\n",
      "18/01/2022 20:24:51 |  number of processed files: 5200\n",
      "18/01/2022 20:26:05 |  number of processed files: 5300\n",
      "18/01/2022 20:27:18 |  number of processed files: 5400\n",
      "18/01/2022 20:28:32 |  number of processed files: 5500\n",
      "18/01/2022 20:29:47 |  number of processed files: 5600\n",
      "18/01/2022 20:31:00 |  number of processed files: 5700\n",
      "18/01/2022 20:32:12 |  number of processed files: 5800\n",
      "18/01/2022 20:33:27 |  number of processed files: 5900\n",
      "18/01/2022 20:34:40 |  number of processed files: 6000\n",
      "18/01/2022 20:35:55 |  number of processed files: 6100\n",
      "18/01/2022 20:37:08 |  number of processed files: 6200\n",
      "18/01/2022 20:38:22 |  number of processed files: 6300\n",
      "18/01/2022 20:39:37 |  number of processed files: 6400\n",
      "18/01/2022 20:40:51 |  number of processed files: 6500\n",
      "18/01/2022 20:42:16 |  number of processed files: 6600\n",
      "18/01/2022 20:43:29 |  number of processed files: 6700\n",
      "18/01/2022 20:44:42 |  number of processed files: 6800\n",
      "18/01/2022 20:45:55 |  number of processed files: 6900\n",
      "18/01/2022 20:47:08 |  number of processed files: 7000\n",
      "18/01/2022 20:48:21 |  number of processed files: 7100\n",
      "18/01/2022 20:49:34 |  number of processed files: 7200\n",
      "18/01/2022 20:50:47 |  number of processed files: 7300\n",
      "18/01/2022 20:52:00 |  number of processed files: 7400\n",
      "18/01/2022 20:53:12 |  number of processed files: 7500\n",
      "18/01/2022 20:54:25 |  number of processed files: 7600\n",
      "18/01/2022 20:55:37 |  number of processed files: 7700\n",
      "18/01/2022 20:56:50 |  number of processed files: 7800\n",
      "18/01/2022 20:58:02 |  number of processed files: 7900\n",
      "18/01/2022 20:59:16 |  number of processed files: 8000\n",
      "18/01/2022 21:00:29 |  number of processed files: 8100\n",
      "18/01/2022 21:01:44 |  number of processed files: 8200\n",
      "18/01/2022 21:02:57 |  number of processed files: 8300\n",
      "18/01/2022 21:04:11 |  number of processed files: 8400\n",
      "18/01/2022 21:05:24 |  number of processed files: 8500\n",
      "18/01/2022 21:06:37 |  number of processed files: 8600\n",
      "18/01/2022 21:07:50 |  number of processed files: 8700\n",
      "18/01/2022 21:09:03 |  number of processed files: 8800\n",
      "18/01/2022 21:10:15 |  number of processed files: 8900\n",
      "18/01/2022 21:11:28 |  number of processed files: 9000\n",
      "18/01/2022 21:12:40 |  number of processed files: 9100\n",
      "18/01/2022 21:13:51 |  number of processed files: 9200\n",
      "18/01/2022 21:15:03 |  number of processed files: 9300\n",
      "18/01/2022 21:16:16 |  number of processed files: 9400\n",
      "18/01/2022 21:17:28 |  number of processed files: 9500\n",
      "18/01/2022 21:18:42 |  number of processed files: 9600\n",
      "18/01/2022 21:19:56 |  number of processed files: 9700\n",
      "18/01/2022 21:21:10 |  number of processed files: 9800\n",
      "18/01/2022 21:22:23 |  number of processed files: 9900\n",
      "18/01/2022 21:23:37 |  number of processed files: 10000\n",
      "18/01/2022 21:24:49 |  number of processed files: 10100\n",
      "18/01/2022 21:26:01 |  number of processed files: 10200\n",
      "18/01/2022 21:27:14 |  number of processed files: 10300\n",
      "18/01/2022 21:28:26 |  number of processed files: 10400\n",
      "18/01/2022 21:29:37 |  number of processed files: 10500\n",
      "18/01/2022 21:30:50 |  number of processed files: 10600\n",
      "18/01/2022 21:32:02 |  number of processed files: 10700\n",
      "18/01/2022 21:33:14 |  number of processed files: 10800\n",
      "18/01/2022 21:34:26 |  number of processed files: 10900\n",
      "18/01/2022 21:35:38 |  number of processed files: 11000\n",
      "18/01/2022 21:36:50 |  number of processed files: 11100\n",
      "18/01/2022 21:38:12 |  number of processed files: 11200\n",
      "18/01/2022 21:39:23 |  number of processed files: 11300\n",
      "18/01/2022 21:40:35 |  number of processed files: 11400\n",
      "18/01/2022 21:41:45 |  number of processed files: 11500\n",
      "18/01/2022 21:42:57 |  number of processed files: 11600\n",
      "18/01/2022 21:44:07 |  number of processed files: 11700\n",
      "18/01/2022 21:45:18 |  number of processed files: 11800\n",
      "18/01/2022 21:46:30 |  number of processed files: 11900\n",
      "18/01/2022 21:47:41 |  number of processed files: 12000\n",
      "18/01/2022 21:48:52 |  number of processed files: 12100\n",
      "18/01/2022 21:50:07 |  number of processed files: 12200\n",
      "18/01/2022 21:51:20 |  number of processed files: 12300\n",
      "18/01/2022 21:52:35 |  number of processed files: 12400\n",
      "18/01/2022 21:53:57 |  number of processed files: 12500\n",
      "18/01/2022 21:55:08 |  number of processed files: 12600\n",
      "18/01/2022 21:56:20 |  number of processed files: 12700\n",
      "18/01/2022 21:57:31 |  number of processed files: 12800\n",
      "18/01/2022 21:58:50 |  number of processed files: 12900\n",
      "18/01/2022 22:00:01 |  number of processed files: 13000\n",
      "18/01/2022 22:01:17 |  number of processed files: 13100\n",
      "18/01/2022 22:02:28 |  number of processed files: 13200\n",
      "18/01/2022 22:03:41 |  number of processed files: 13300\n",
      "18/01/2022 22:04:53 |  number of processed files: 13400\n",
      "18/01/2022 22:06:04 |  number of processed files: 13500\n",
      "18/01/2022 22:07:15 |  number of processed files: 13600\n",
      "18/01/2022 22:08:28 |  number of processed files: 13700\n",
      "18/01/2022 22:09:38 |  number of processed files: 13800\n",
      "18/01/2022 22:10:49 |  number of processed files: 13900\n",
      "18/01/2022 22:12:00 |  number of processed files: 14000\n",
      "18/01/2022 22:13:11 |  number of processed files: 14100\n",
      "18/01/2022 22:14:22 |  number of processed files: 14200\n",
      "18/01/2022 22:15:33 |  number of processed files: 14300\n",
      "18/01/2022 22:16:45 |  number of processed files: 14400\n",
      "18/01/2022 22:17:56 |  number of processed files: 14500\n",
      "18/01/2022 22:19:07 |  number of processed files: 14600\n",
      "18/01/2022 22:20:18 |  number of processed files: 14700\n",
      "18/01/2022 22:21:33 |  number of processed files: 14800\n",
      "18/01/2022 22:22:43 |  number of processed files: 14900\n"
     ]
    }
   ],
   "source": [
    "# Let's process only big files, \n",
    "# because there are a lot of small files, which is why processing takes a very long time\n",
    "\n",
    "tocken_error = []\n",
    "prepare_book = []\n",
    "\n",
    "num = 0\n",
    "book_err = []\n",
    "sum_len = 0\n",
    "\n",
    "\n",
    "for path in ind_gen:\n",
    "    if num % 100 == 0:\n",
    "        now = datetime.now()\n",
    "        dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "        print(dt_string, '| ',  'number of processed files: ' + str(num))\n",
    "    \n",
    "    title = path.split('/')[-1].split('.txt')[0]\n",
    "    \n",
    "    res = prepare_english_text(path, '/content/drive/MyDrive/2022-01-15_Course_project/prep_get_text_en/', \n",
    "                               tocken_error,  nlp, gutenberg = False, title = title)\n",
    "    if res < 0:\n",
    "      book_err.append(path)\n",
    "      print('ERROR: ', path)\n",
    "    else:\n",
    "        prepare_book.append(path)\n",
    "    num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NxaAVhUfT_Js"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Preprocessing_geneate_en.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
