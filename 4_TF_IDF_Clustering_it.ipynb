{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:05:19.638787Z",
     "start_time": "2022-01-20T00:05:19.628854Z"
    },
    "id": "cdb50c2b"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T23:16:44.565625Z",
     "start_time": "2022-01-19T23:16:40.800174Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6120d187",
    "outputId": "6d952113-a336-4913-a7b4-ea16dc684082"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse.linalg import svds\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WRt8kPDwn-_b",
    "outputId": "be43d06a-8a1e-4c66-e46d-25fd155fc253"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T23:16:51.418514Z",
     "start_time": "2022-01-19T23:16:51.400418Z"
    },
    "id": "3bd2f029"
   },
   "outputs": [],
   "source": [
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5a7425c8",
    "outputId": "5c7ed1f5-c057-4817-dd59-66c48e45b70c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n9yFI2MF4ib5",
    "outputId": "8e0d07a2-9bb5-4212-900e-7e65c7f27002"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13387"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glob.glob('/content/drive/MyDrive/2022-01-15_Course_project/cut_it/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gYDer1mt9Wy6"
   },
   "outputs": [],
   "source": [
    "# Let's select 10k texts in a folder: '/content/drive/MyDrive/2022-01-15_Course_project/10000_cut_es/'\n",
    "# Because clustering works for a very long time on large datasets\n",
    "\n",
    "import shutil\n",
    "file_list = glob.glob('/content/drive/MyDrive/2022-01-15_Course_project/cut_it/*')\n",
    "k = 0\n",
    "\n",
    "for i in file_list:\n",
    "  shutil.copy(i, '/content/drive/MyDrive/2022-01-15_Course_project/10000_cut_it/')\n",
    "  k += 1\n",
    "  if k >= 10000:\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s9w9UUYX9jgj",
    "outputId": "11efa370-ba9d-4f7b-965c-5f18e55ab37e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glob.glob('/content/drive/MyDrive/2022-01-15_Course_project/10000_cut_it/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "MaplzLoMnLuZ"
   },
   "outputs": [],
   "source": [
    "# Wishart clustering function\n",
    "# https://github.com/Radi4/BotDetection/blob/master/Wishart.py\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import gamma\n",
    "from sklearn.neighbors import KDTree\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Wishart:\n",
    "    def __init__(self, wishart_neighbors, significance_level):\n",
    "        self.wishart_neighbors = wishart_neighbors  # Number of neighbors\n",
    "        self.significance_level = significance_level  # Significance level\n",
    "\n",
    "    def fit(self, X):\n",
    "        from sklearn.neighbors import KDTree\n",
    "        kdt = KDTree(X, metric='euclidean')\n",
    "\n",
    "        #add one because you are your neighb.\n",
    "        distances, neighbors = kdt.query(X, k = self.wishart_neighbors + 1, return_distance = True)\n",
    "        neighbors = neighbors[:, 1:]\n",
    "\n",
    "\n",
    "        distances = distances[:, -1]\n",
    "        indexes = np.argsort(distances)\n",
    "        \n",
    "        size, dim = X.shape\n",
    "\n",
    "        self.object_labels = np.zeros(size, dtype = int) - 1\n",
    "\n",
    "        #index in tuple\n",
    "        #min_dist, max_dist, flag_to_significant\n",
    "        self.clusters = np.array([(1., 1., 0)])\n",
    "        self.clusters_to_objects = defaultdict(list)\n",
    "        print('Start clustering')\n",
    "\n",
    "        for index in indexes:\n",
    "            neighbors_clusters =\\\n",
    "                np.concatenate([self.object_labels[neighbors[index]], self.object_labels[neighbors[index]]])\n",
    "            unique_clusters = np.unique(neighbors_clusters).astype(int)\n",
    "            unique_clusters = unique_clusters[unique_clusters != -1]\n",
    "\n",
    "\n",
    "            if len(unique_clusters) == 0:\n",
    "                self._create_new_cluster(index, distances[index])\n",
    "            else:\n",
    "                max_cluster = unique_clusters[-1]\n",
    "                min_cluster = unique_clusters[0]\n",
    "                if max_cluster == min_cluster:\n",
    "                    if self.clusters[max_cluster][-1] < 0.5:\n",
    "                        self._add_elem_to_exist_cluster(index, distances[index], max_cluster)\n",
    "                    else:\n",
    "                        self._add_elem_to_noise(index)\n",
    "                else:\n",
    "                    my_clusters = self.clusters[unique_clusters]\n",
    "                    flags = my_clusters[:, -1]\n",
    "                    if np.min(flags) > 0.5:\n",
    "                        self._add_elem_to_noise(index)\n",
    "                    else:\n",
    "                        significan = np.power(my_clusters[:, 0], -dim) - np.power(my_clusters[:, 1], -dim)\n",
    "                        significan *= self.wishart_neighbors\n",
    "                        significan /= size\n",
    "                        significan /= np.power(np.pi, dim / 2)\n",
    "                        significan *= gamma(dim / 2 + 1)\n",
    "                        significan_index = significan >= self.significance_level\n",
    "\n",
    "                        significan_clusters = unique_clusters[significan_index]\n",
    "                        not_significan_clusters = unique_clusters[~significan_index]\n",
    "                        significan_clusters_count = len(significan_clusters)\n",
    "                        if significan_clusters_count > 1 or min_cluster == 0:\n",
    "                            self._add_elem_to_noise(index)\n",
    "                            self.clusters[significan_clusters, -1] = 1\n",
    "                            for not_sig_cluster in not_significan_clusters:\n",
    "                                if not_sig_cluster == 0:\n",
    "                                    continue\n",
    "\n",
    "                                for bad_index in self.clusters_to_objects[not_sig_cluster]:\n",
    "                                    self._add_elem_to_noise(bad_index)\n",
    "                                self.clusters_to_objects[not_sig_cluster].clear()\n",
    "                        else:\n",
    "                            for cur_cluster in unique_clusters:\n",
    "                                if cur_cluster == min_cluster:\n",
    "                                    continue\n",
    "\n",
    "                                for bad_index in self.clusters_to_objects[cur_cluster]:\n",
    "                                    self._add_elem_to_exist_cluster(bad_index, distances[bad_index], min_cluster)\n",
    "                                self.clusters_to_objects[cur_cluster].clear()\n",
    "\n",
    "                            self._add_elem_to_exist_cluster(index, distances[index], min_cluster)\n",
    "\n",
    "        return self.clean_data()\n",
    "\n",
    "    def clean_data(self):\n",
    "        unique = np.unique(self.object_labels)\n",
    "        index = np.argsort(unique)\n",
    "        if unique[0] != 0:\n",
    "            index += 1\n",
    "        true_cluster = {unq :  index for unq, index in zip(unique, index)}\n",
    "        result = np.zeros(len(self.object_labels), dtype = int)\n",
    "        for index, unq in enumerate(self.object_labels):\n",
    "            result[index] = true_cluster[unq]\n",
    "        return result\n",
    "\n",
    "    def _add_elem_to_noise(self, index):\n",
    "        self.object_labels[index] = 0\n",
    "        self.clusters_to_objects[0].append(index)\n",
    "\n",
    "    def _create_new_cluster(self, index, dist):\n",
    "        self.object_labels[index] = len(self.clusters)\n",
    "        self.clusters_to_objects[len(self.clusters)].append(index)\n",
    "        self.clusters = np.append(self.clusters, [(dist, dist, 0)], axis = 0)\n",
    "\n",
    "    def _add_elem_to_exist_cluster(self, index, dist, cluster_label):\n",
    "        self.object_labels[index] = cluster_label\n",
    "        self.clusters_to_objects[cluster_label].append(index)\n",
    "        self.clusters[cluster_label][0] = min(self.clusters[cluster_label][0], dist)\n",
    "        self.clusters[cluster_label][1] = max(self.clusters[cluster_label][1], dist)\n",
    "\n",
    "\n",
    "class PreTrainWishart:\n",
    "    def __init__(self, wishart_neighbors, significance_level, distances, neighbors):\n",
    "        self.wishart_neighbors = wishart_neighbors  # Number of neighbors\n",
    "        self.significance_level = significance_level  # Significance level\n",
    "        self.distances = distances\n",
    "        self.neighbors = neighbors\n",
    "\n",
    "    def fit(self, X):\n",
    "        from sklearn.neighbors import KDTree\n",
    "        kdt = KDTree(X, metric='euclidean')\n",
    "\n",
    "        #add one because you are your neighb.\n",
    "        neighbors = self.neighbors[:, 1 : self.wishart_neighbors + 1]\n",
    "        distances = self.distances[:, self.wishart_neighbors]\n",
    "        indexes = np.argsort(distances)\n",
    "        \n",
    "        size, dim = X.shape\n",
    "\n",
    "        self.object_labels = np.zeros(size, dtype = int) - 1\n",
    "\n",
    "        #index in tuple\n",
    "        #min_dist, max_dist, flag_to_significant\n",
    "        self.clusters = np.array([(1., 1., 0)])\n",
    "        self.clusters_to_objects = defaultdict(list)\n",
    "\n",
    "        for index in indexes:\n",
    "            neighbors_clusters =\\\n",
    "                np.concatenate([self.object_labels[neighbors[index]], self.object_labels[neighbors[index]]])\n",
    "            unique_clusters = np.unique(neighbors_clusters).astype(int)\n",
    "            unique_clusters = unique_clusters[unique_clusters != -1]\n",
    "\n",
    "\n",
    "            if len(unique_clusters) == 0:\n",
    "                self._create_new_cluster(index, distances[index])\n",
    "            else:\n",
    "                max_cluster = unique_clusters[-1]\n",
    "                min_cluster = unique_clusters[0]\n",
    "                if max_cluster == min_cluster:\n",
    "                    if self.clusters[max_cluster][-1] < 0.5:\n",
    "                        self._add_elem_to_exist_cluster(index, distances[index], max_cluster)\n",
    "                    else:\n",
    "                        self._add_elem_to_noise(index)\n",
    "                else:\n",
    "                    my_clusters = self.clusters[unique_clusters]\n",
    "                    flags = my_clusters[:, -1]\n",
    "                    if np.min(flags) > 0.5:\n",
    "                        self._add_elem_to_noise(index)\n",
    "                    else:\n",
    "                        significan = np.power(my_clusters[:, 0], -dim) - np.power(my_clusters[:, 1], -dim)\n",
    "                        significan *= self.wishart_neighbors\n",
    "                        significan /= size\n",
    "                        significan /= np.power(np.pi, dim / 2)\n",
    "                        significan *= gamma(dim / 2 + 1)\n",
    "                        significan_index = significan >= self.significance_level\n",
    "\n",
    "                        significan_clusters = unique_clusters[significan_index]\n",
    "                        not_significan_clusters = unique_clusters[~significan_index]\n",
    "                        significan_clusters_count = len(significan_clusters)\n",
    "                        if significan_clusters_count > 1 or min_cluster == 0:\n",
    "                            self._add_elem_to_noise(index)\n",
    "                            self.clusters[significan_clusters, -1] = 1\n",
    "                            for not_sig_cluster in not_significan_clusters:\n",
    "                                if not_sig_cluster == 0:\n",
    "                                    continue\n",
    "\n",
    "                                for bad_index in self.clusters_to_objects[not_sig_cluster]:\n",
    "                                    self._add_elem_to_noise(bad_index)\n",
    "                                self.clusters_to_objects[not_sig_cluster].clear()\n",
    "                        else:\n",
    "                            for cur_cluster in unique_clusters:\n",
    "                                if cur_cluster == min_cluster:\n",
    "                                    continue\n",
    "\n",
    "                                for bad_index in self.clusters_to_objects[cur_cluster]:\n",
    "                                    self._add_elem_to_exist_cluster(bad_index, distances[bad_index], min_cluster)\n",
    "                                self.clusters_to_objects[cur_cluster].clear()\n",
    "\n",
    "                            self._add_elem_to_exist_cluster(index, distances[index], min_cluster)\n",
    "\n",
    "        return self.clean_data()\n",
    "\n",
    "    def clean_data(self):\n",
    "        unique = np.unique(self.object_labels)\n",
    "        index = np.argsort(unique)\n",
    "        if unique[0] != 0:\n",
    "            index += 1\n",
    "        true_cluster = {unq :  index for unq, index in zip(unique, index)}\n",
    "        result = np.zeros(len(self.object_labels), dtype = int)\n",
    "        for index, unq in enumerate(self.object_labels):\n",
    "            result[index] = true_cluster[unq]\n",
    "        return result\n",
    "\n",
    "    def _add_elem_to_noise(self, index):\n",
    "        self.object_labels[index] = 0\n",
    "        self.clusters_to_objects[0].append(index)\n",
    "\n",
    "    def _create_new_cluster(self, index, dist):\n",
    "        self.object_labels[index] = len(self.clusters)\n",
    "        self.clusters_to_objects[len(self.clusters)].append(index)\n",
    "        self.clusters = np.append(self.clusters, [(dist, dist, 0)], axis = 0)\n",
    "\n",
    "    def _add_elem_to_exist_cluster(self, index, dist, cluster_label):\n",
    "        self.object_labels[index] = cluster_label\n",
    "        self.clusters_to_objects[cluster_label].append(index)\n",
    "        self.clusters[cluster_label][0] = min(self.clusters[cluster_label][0], dist)\n",
    "        self.clusters[cluster_label][1] = max(self.clusters[cluster_label][1], dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7f043e55"
   },
   "source": [
    "## Create a vector representation based on TfidfVectorizer (on human texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:02:39.113091Z",
     "start_time": "2022-01-20T00:02:39.053009Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7b1f3a37",
    "outputId": "fdfce0b3-1025-4b74-b2b3-f03f282216e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ad', 'al', 'allo', 'ai', 'agli', 'all', 'agl', 'alla', 'alle', 'con', 'col', 'coi', 'da', 'dal', 'dallo', 'dai', 'dagli', 'dall', 'dagl', 'dalla', 'dalle', 'di', 'del', 'dello', 'dei', 'degli', 'dell', 'degl', 'della', 'delle', 'in', 'nel', 'nello', 'nei', 'negli', 'nell', 'negl', 'nella', 'nelle', 'su', 'sul', 'sullo', 'sui', 'sugli', 'sull', 'sugl', 'sulla', 'sulle', 'per', 'tra', 'contro', 'io', 'tu', 'lui', 'lei', 'noi', 'voi', 'loro', 'mio', 'mia', 'miei', 'mie', 'tuo', 'tua', 'tuoi', 'tue', 'suo', 'sua', 'suoi', 'sue', 'nostro', 'nostra', 'nostri', 'nostre', 'vostro', 'vostra', 'vostri', 'vostre', 'mi', 'ti', 'ci', 'vi', 'lo', 'la', 'li', 'le', 'gli', 'ne', 'il', 'un', 'uno', 'una', 'ma', 'ed', 'se', 'perché', 'anche', 'come', 'dov', 'dove', 'che', 'chi', 'cui', 'non', 'più', 'quale', 'quanto', 'quanti', 'quanta', 'quante', 'quello', 'quelli', 'quella', 'quelle', 'questo', 'questi', 'questa', 'queste', 'si', 'tutto', 'tutti', 'a', 'c', 'e', 'i', 'l', 'o', 'ho', 'hai', 'ha', 'abbiamo', 'avete', 'hanno', 'abbia', 'abbiate', 'abbiano', 'avrò', 'avrai', 'avrà', 'avremo', 'avrete', 'avranno', 'avrei', 'avresti', 'avrebbe', 'avremmo', 'avreste', 'avrebbero', 'avevo', 'avevi', 'aveva', 'avevamo', 'avevate', 'avevano', 'ebbi', 'avesti', 'ebbe', 'avemmo', 'aveste', 'ebbero', 'avessi', 'avesse', 'avessimo', 'avessero', 'avendo', 'avuto', 'avuta', 'avuti', 'avute', 'sono', 'sei', 'è', 'siamo', 'siete', 'sia', 'siate', 'siano', 'sarò', 'sarai', 'sarà', 'saremo', 'sarete', 'saranno', 'sarei', 'saresti', 'sarebbe', 'saremmo', 'sareste', 'sarebbero', 'ero', 'eri', 'era', 'eravamo', 'eravate', 'erano', 'fui', 'fosti', 'fu', 'fummo', 'foste', 'furono', 'fossi', 'fosse', 'fossimo', 'fossero', 'essendo', 'faccio', 'fai', 'facciamo', 'fanno', 'faccia', 'facciate', 'facciano', 'farò', 'farai', 'farà', 'faremo', 'farete', 'faranno', 'farei', 'faresti', 'farebbe', 'faremmo', 'fareste', 'farebbero', 'facevo', 'facevi', 'faceva', 'facevamo', 'facevate', 'facevano', 'feci', 'facesti', 'fece', 'facemmo', 'faceste', 'fecero', 'facessi', 'facesse', 'facessimo', 'facessero', 'facendo', 'sto', 'stai', 'sta', 'stiamo', 'stanno', 'stia', 'stiate', 'stiano', 'starò', 'starai', 'starà', 'staremo', 'starete', 'staranno', 'starei', 'staresti', 'starebbe', 'staremmo', 'stareste', 'starebbero', 'stavo', 'stavi', 'stava', 'stavamo', 'stavate', 'stavano', 'stetti', 'stesti', 'stette', 'stemmo', 'steste', 'stettero', 'stessi', 'stesse', 'stessimo', 'stessero', 'stando']\n"
     ]
    }
   ],
   "source": [
    "# Stopwords for Italian\n",
    "print(stopwords.words('italian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:03:08.870021Z",
     "start_time": "2022-01-20T00:03:08.861187Z"
    },
    "id": "4d3f9554"
   },
   "outputs": [],
   "source": [
    "def make_corpus(input_path, output_file_path):\n",
    "    i = 0\n",
    "    file_list = glob.glob(input_path + '*')\n",
    "    \n",
    "    with open(output_file_path, 'w+') as output_file:\n",
    "        for file in file_list:\n",
    "            if i % 500 == 0:\n",
    "                now = datetime.now()\n",
    "                dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "                print(dt_string, '| ',  'number of processed files: ' + str(i), '| ', \n",
    "                      'percentage of completion:', str(round(i/len(file_list), 2)* 100) + ' %' )\n",
    "            i+=1\n",
    "            with open(file, 'r') as input_file:\n",
    "                output_file.write(input_file.read().replace('\\n', ' '))\n",
    "                output_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:03:12.905429Z",
     "start_time": "2022-01-20T00:03:09.705899Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "de124e1e",
    "outputId": "24e86e0b-9278-4f44-95a0-80a816f14970"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/2022 00:10:28 |  number of processed files: 0 |  percentage of completion: 0.0 %\n",
      "23/01/2022 00:10:33 |  number of processed files: 500 |  percentage of completion: 5.0 %\n",
      "23/01/2022 00:10:33 |  number of processed files: 1000 |  percentage of completion: 10.0 %\n",
      "23/01/2022 00:10:34 |  number of processed files: 1500 |  percentage of completion: 15.0 %\n",
      "23/01/2022 00:10:34 |  number of processed files: 2000 |  percentage of completion: 20.0 %\n",
      "23/01/2022 00:10:35 |  number of processed files: 2500 |  percentage of completion: 25.0 %\n",
      "23/01/2022 00:10:35 |  number of processed files: 3000 |  percentage of completion: 30.0 %\n",
      "23/01/2022 00:10:35 |  number of processed files: 3500 |  percentage of completion: 35.0 %\n",
      "23/01/2022 00:10:36 |  number of processed files: 4000 |  percentage of completion: 40.0 %\n",
      "23/01/2022 00:10:36 |  number of processed files: 4500 |  percentage of completion: 45.0 %\n",
      "23/01/2022 00:10:37 |  number of processed files: 5000 |  percentage of completion: 50.0 %\n",
      "23/01/2022 00:10:37 |  number of processed files: 5500 |  percentage of completion: 55.00000000000001 %\n",
      "23/01/2022 00:10:38 |  number of processed files: 6000 |  percentage of completion: 60.0 %\n",
      "23/01/2022 00:10:38 |  number of processed files: 6500 |  percentage of completion: 65.0 %\n",
      "23/01/2022 00:10:38 |  number of processed files: 7000 |  percentage of completion: 70.0 %\n",
      "23/01/2022 00:10:39 |  number of processed files: 7500 |  percentage of completion: 75.0 %\n",
      "23/01/2022 00:10:39 |  number of processed files: 8000 |  percentage of completion: 80.0 %\n",
      "23/01/2022 00:10:39 |  number of processed files: 8500 |  percentage of completion: 85.0 %\n",
      "23/01/2022 00:10:40 |  number of processed files: 9000 |  percentage of completion: 90.0 %\n",
      "23/01/2022 00:10:40 |  number of processed files: 9500 |  percentage of completion: 95.0 %\n"
     ]
    }
   ],
   "source": [
    "# Let's make corpus for human-texts\n",
    "\n",
    "make_corpus('/content/drive/MyDrive/2022-01-15_Course_project/10000_cut_it/', \n",
    "            '/content/drive/MyDrive/2022-01-15_Course_project/TF_IDF_clustering_it/10000_dataset_human_it.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:03:42.696966Z",
     "start_time": "2022-01-20T00:03:42.639936Z"
    },
    "id": "07c89c06"
   },
   "outputs": [],
   "source": [
    "# TF_IDF corpus\n",
    "\n",
    "def make_table_and_dict(corpus_path, min_df, max_df, token_pattern = None, use_idf = True, stop_words = 'italian'):\n",
    "    \n",
    "    with open(corpus_path, 'r') as corpus_file:\n",
    "        if token_pattern:\n",
    "            vectorizer = TfidfVectorizer(analyzer='word', min_df=min_df, token_pattern=token_pattern, use_idf=use_idf)\n",
    "        else:\n",
    "            vectorizer = TfidfVectorizer(analyzer='word', min_df=min_df)\n",
    "        data_vectorized = vectorizer.fit_transform(corpus_file)\n",
    "    return data_vectorized, vectorizer.get_feature_names(), vectorizer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:03:43.839418Z",
     "start_time": "2022-01-20T00:03:43.817476Z"
    },
    "id": "8f4de5bf"
   },
   "outputs": [],
   "source": [
    "def create_table(data_vectorized, k, name, path):\n",
    "    u, sigma, vt = svds(data_vectorized, k)\n",
    "    print(sigma)\n",
    "    dict_ = np.dot(np.diag(sigma), vt).T\n",
    "        \n",
    "    with open(path + name + str(k) + '.pkl', 'wb') as f:\n",
    "        pickle.dump(dict_, f)\n",
    "    return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:04:04.324266Z",
     "start_time": "2022-01-20T00:03:55.376572Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "516f0a4f",
    "outputId": "b821299b-3670-4742-a7b5-a293d5fb89de"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#create TF_IDF on human text\n",
    "it_data_vectorized, it_dictionary, idfs = make_table_and_dict('/content/drive/MyDrive/2022-01-15_Course_project/TF_IDF_clustering_it/10000_dataset_human_it.txt', \n",
    "                                                                0.05,  0.7 , token_pattern = '[A-Za-z]+', \n",
    "                                                                stop_words = stopwords.words('italian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:05:34.206134Z",
     "start_time": "2022-01-20T00:05:25.711481Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51a7a097",
    "outputId": "259b8aeb-bc60-4e12-e745-ebe3a889929e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.55976914  1.5690523   1.57537651  1.5769125   1.58164523  1.59430343\n",
      "  1.60543019  1.60772434  1.61233981  1.61949693  1.63053092  1.64197696\n",
      "  1.6434944   1.65921827  1.66223865  1.67013407  1.67726013  1.6889918\n",
      "  1.69132033  1.7052235   1.7066912   1.72976939  1.73070517  1.74142051\n",
      "  1.7552815   1.76585105  1.77967191  1.78805374  1.79505584  1.79710142\n",
      "  1.81587676  1.82685131  1.83906718  1.85043582  1.85721964  1.86374704\n",
      "  1.88652826  1.89716838  1.90442458  1.91021806  1.93441719  1.94333641\n",
      "  1.95322639  1.96340816  1.97929017  2.01843855  2.02617791  2.0305834\n",
      "  2.0450902   2.06583749  2.08428616  2.14102038  2.15778732  2.16544597\n",
      "  2.17887295  2.18631396  2.22653695  2.24186397  2.2784561   2.3099563\n",
      "  2.31752772  2.38612231  2.39304587  2.42410337  2.47976378  2.49396335\n",
      "  2.55009284  2.60010897  2.61506251  2.68732351  2.72381922  2.80425532\n",
      "  2.84565322  2.88198773  2.97186424  2.99528294  3.08826989  3.12637906\n",
      "  3.16757654  3.29354609  3.45128932  3.47996441  3.67182423  3.77756962\n",
      "  3.8710802   4.03077941  4.17148015  4.4990331   4.64138841  4.88782465\n",
      "  5.20152026  5.5075815   6.21980828  6.37393845  6.8308306   9.01062935\n",
      " 11.69978311 13.82057949 23.89298371 85.79919836]\n"
     ]
    }
   ],
   "source": [
    "dict_ = create_table(it_data_vectorized, 100, '10000_SVD_human_it_', \n",
    "                     '/content/drive/MyDrive/2022-01-15_Course_project/TF_IDF_clustering_it/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:05:47.579186Z",
     "start_time": "2022-01-20T00:05:47.545501Z"
    },
    "id": "3d47f3a1"
   },
   "outputs": [],
   "source": [
    "pairs_0 = list(zip(idfs, dict_))\n",
    "pairs_idf = dict(zip(it_dictionary, pairs_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:05:48.563375Z",
     "start_time": "2022-01-20T00:05:48.480056Z"
    },
    "id": "cb22926f"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    file = open('/content/drive/MyDrive/2022-01-15_Course_project/TF_IDF_clustering_it/10000_TF_IDF_human_it.pkl', 'wb')\n",
    "    pickle.dump(pairs_idf, file)\n",
    "    file.close()\n",
    "except:\n",
    "    print(\"Something went wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:05:49.663116Z",
     "start_time": "2022-01-20T00:05:49.635950Z"
    },
    "id": "b4f4f187"
   },
   "outputs": [],
   "source": [
    "# Removing frequently used words\n",
    "\n",
    "dict_cut = dict()\n",
    "for w in pairs_idf.keys():\n",
    "    if pairs_idf[w][0] > 1.5:\n",
    "        dict_cut[w] = pairs_idf[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:06:00.130576Z",
     "start_time": "2022-01-20T00:06:00.062901Z"
    },
    "id": "15b2dcdb"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    file = open('/content/drive/MyDrive/2022-01-15_Course_project/TF_IDF_clustering_it/10000_TF_IDF_cut_human_it.pkl', 'wb')\n",
    "    pickle.dump(dict_cut, file)\n",
    "    file.close()\n",
    "except:\n",
    "    print(\"Something went wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:06:16.299773Z",
     "start_time": "2022-01-20T00:06:16.281797Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ed1449ad",
    "outputId": "fb81e27c-0206-494b-8b0b-39a24619c8a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1656"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(it_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:06:30.784942Z",
     "start_time": "2022-01-20T00:06:30.769056Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "895287d1",
    "outputId": "9e0c7ee2-8ea6-424f-b1b7-e2d2dfd436c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1582"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_cut.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ce276f30"
   },
   "source": [
    "# Making n-grams and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:06:36.788059Z",
     "start_time": "2022-01-20T00:06:36.765360Z"
    },
    "id": "3edea2f9"
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from math import log\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics import calinski_harabasz_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:06:39.611702Z",
     "start_time": "2022-01-20T00:06:39.591675Z"
    },
    "id": "8b9b8248"
   },
   "outputs": [],
   "source": [
    "def divide(data, labels):\n",
    "    clusters = set(labels)\n",
    "    clusters_data = []\n",
    "    for cluster in clusters:\n",
    "        clusters_data.append(data[labels == cluster, :])\n",
    "    return clusters_data\n",
    "\n",
    "def get_centroids(clusters):\n",
    "    centroids = []\n",
    "    for cluster_data in clusters:\n",
    "        centroids.append(cluster_data.mean(axis=0))\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:06:40.279933Z",
     "start_time": "2022-01-20T00:06:40.241868Z"
    },
    "id": "9eb06fd2"
   },
   "outputs": [],
   "source": [
    "def cohesion(data, labels):\n",
    "    clusters = sorted(set(labels))\n",
    "    sse = 0\n",
    "    for cluster in clusters:\n",
    "        cluster_data = data[labels == cluster, :]\n",
    "        centroid = cluster_data.mean(axis = 0)\n",
    "        sse += ((cluster_data - centroid)**2).sum()\n",
    "    return sse\n",
    "\n",
    "def separation(data, labels, cohesion_score):\n",
    "    # calculate separation as SST - SSE\n",
    "    return cohesion(data, np.zeros(data.shape[0])) - cohesion_score\n",
    "\n",
    "def SST(data):\n",
    "    c = get_centroids([data])\n",
    "    return ((data - c) ** 2).sum()\n",
    "\n",
    "def SSE(clusters, centroids):\n",
    "    result = 0\n",
    "    for cluster, centroid in zip(clusters, centroids):\n",
    "        result += ((cluster - centroid) ** 2).sum()\n",
    "    return result\n",
    "\n",
    "# Clear the store before running each time\n",
    "within_cluster_dist_sum_store = {}\n",
    "def within_cluster_dist_sum(cluster, centroid, cluster_id):\n",
    "    if cluster_id in within_cluster_dist_sum_store:\n",
    "        return within_cluster_dist_sum_store[cluster_id]\n",
    "    else:\n",
    "        result = (((cluster - centroid) ** 2).sum(axis=1)**.5).sum()\n",
    "        within_cluster_dist_sum_store[cluster_id] = result\n",
    "    return result\n",
    "\n",
    "def RMSSTD(data, clusters, centroids):\n",
    "    df = data.shape[0] - len(clusters)\n",
    "    attribute_num = data.shape[1]\n",
    "    return (SSE(clusters, centroids) / (attribute_num * df)) ** .5\n",
    "\n",
    "# equal to separation / (cohesion + separation)\n",
    "def RS(data, clusters, centroids):\n",
    "    sst = SST(data)\n",
    "    sse = SSE(clusters, centroids)\n",
    "    return (sst - sse) / sst\n",
    "\n",
    "def DB_find_max_j(clusters, centroids, i):\n",
    "    max_val = 0\n",
    "    max_j = 0\n",
    "    for j in range(len(clusters)):\n",
    "        if j == i:\n",
    "            continue\n",
    "        cluster_i_stat = within_cluster_dist_sum(clusters[i], centroids[i], i) / clusters[i].shape[0]\n",
    "        cluster_j_stat = within_cluster_dist_sum(clusters[j], centroids[j], j) / clusters[j].shape[0]\n",
    "        val = (cluster_i_stat + cluster_j_stat) / (((centroids[i] - centroids[j]) ** 2).sum() ** .5)\n",
    "        if val > max_val:\n",
    "            max_val = val\n",
    "            max_j = j\n",
    "    return max_val\n",
    "\n",
    "def DB(data, clusters, centroids):\n",
    "    result = 0\n",
    "    for i in range(len(clusters)):\n",
    "        result += DB_find_max_j(clusters, centroids, i)\n",
    "    return result / len(clusters)\n",
    "\n",
    "def XB(data, clusters, centroids):\n",
    "    sse = SSE(clusters, centroids)\n",
    "    min_dist = ((centroids[0] - centroids[1]) ** 2).sum()\n",
    "    for centroid_i, centroid_j in list(product(centroids, centroids)):\n",
    "        if (centroid_i - centroid_j).sum() == 0:\n",
    "            continue\n",
    "        dist = ((centroid_i - centroid_j) ** 2).sum()\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "    return sse / (data.shape[0] * min_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T17:23:21.939658Z",
     "start_time": "2022-01-20T17:23:21.853300Z"
    },
    "id": "93e7bacc"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "# Some metrics can work for a very long time (commented out)\n",
    "\n",
    "def get_validation_scores(data, labels, max_clust = None):\n",
    "    #within_cluster_dist_sum_store.clear()\n",
    "    \n",
    "    clusters = divide(data, labels)\n",
    "    centroids = get_centroids(clusters)\n",
    "    \n",
    "    scores = {}\n",
    "    if max_clust:\n",
    "        if len(clusters) > max_clust:\n",
    "            scores['cohesion'] = cohesion(data, labels)\n",
    "            scores['separation'] = separation(data, labels, scores['cohesion'])\n",
    "            scores['calinski_harabaz_score'] = None\n",
    "            scores['RMSSTD'] = RMSSTD(data, clusters, centroids)\n",
    "            scores['RS'] = RS(data, clusters, centroids)\n",
    "            #scores['DB'] = None\n",
    "            #scores['XB'] = XB(data, clusters, centroids)\n",
    "            scores['silhouette'] = None\n",
    "        else:\n",
    "            scores['cohesion'] = cohesion(data, labels)\n",
    "            scores['separation'] = separation(data, labels, scores['cohesion'])\n",
    "            scores['calinski_harabaz_score'] = calinski_harabasz_score(data, labels)\n",
    "            scores['RMSSTD'] = RMSSTD(data, clusters, centroids)\n",
    "            scores['RS'] = RS(data, clusters, centroids)\n",
    "            #scores['DB'] = DB(data, clusters, centroids)\n",
    "            #scores['XB'] = XB(data, clusters, centroids)\n",
    "            scores['silhouette'] = silhouette_score(data, labels)\n",
    "    else:\n",
    "        scores['cohesion'] = cohesion(data, labels)\n",
    "        scores['separation'] = separation(data, labels, scores['cohesion'])\n",
    "        scores['calinski_harabaz_score'] = calinski_harabasz_score(data, labels)\n",
    "        scores['RMSSTD'] = RMSSTD(data, clusters, centroids)\n",
    "        scores['RS'] = RS(data, clusters, centroids)\n",
    "        #scores['DB'] = DB(data, clusters, centroids)\n",
    "        #scores['XB'] = XB(data, clusters, centroids)\n",
    "        scores['silhouette'] = silhouette_score(data, labels)\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:06:52.939414Z",
     "start_time": "2022-01-20T00:06:52.923459Z"
    },
    "id": "4a8e01ac"
   },
   "outputs": [],
   "source": [
    "def make_ngrams(input_corpus,  dict_, N = 2, m = None, uniq = False):\n",
    "    dict_grams = dict()\n",
    "    num_ = 0\n",
    "    i = 0\n",
    "    j = 0\n",
    "    \n",
    "    print('Count documents: ', len(input_corpus))\n",
    "    for sentence in input_corpus:\n",
    "        sentence = sentence.split(' ')\n",
    "        grams = [sentence[i:i+N] for i in range(len(sentence)-N+1)]\n",
    "        for g in grams:\n",
    "            g_key = '_'.join(elem for elem in g)\n",
    "\n",
    "            if uniq:\n",
    "                if all(elem in dict_.keys()  for elem in g) and (g_key not in dict_grams.keys()):\n",
    "                    dict_grams[g_key] = []\n",
    "                    for elem in g:\n",
    "                            if m:\n",
    "                                dict_grams[g_key] += list(dict_[elem][1][:m])\n",
    "                            else:\n",
    "                                dict_grams[g_key] += list(dict_[elem][1])\n",
    "            else:\n",
    "                if all(elem in dict_.keys()  for elem in g):\n",
    "                    concat = []\n",
    "                    for elem in g:\n",
    "                        if m:\n",
    "                            concat += list(dict_[elem][1][:m])\n",
    "                        else:\n",
    "                            concat += list(dict_[elem][1])\n",
    "                    dict_grams[i] = (j, g_key, concat)\n",
    "                    i += 1\n",
    "            j += 1\n",
    "       \n",
    "            \n",
    "        if num_ % 500 == 0:\n",
    "            now = datetime.now()\n",
    "            dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "            print(dt_string, '| ',  'number of processed documents: ' + str(num_), '| ', \n",
    "                      'percentage of completion:', str(round(num_/len(input_corpus), 2)* 100) + ' %' )\n",
    "        num_ += 1\n",
    "    return dict_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:07:43.802071Z",
     "start_time": "2022-01-20T00:07:43.415738Z"
    },
    "id": "8eae3315"
   },
   "outputs": [],
   "source": [
    "with open('/content/drive/MyDrive/2022-01-15_Course_project/TF_IDF_clustering_it/10000_dataset_human_it.txt', 'r') as corpus_file:\n",
    "    corpus = corpus_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:09:15.616274Z",
     "start_time": "2022-01-20T00:07:51.301224Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3b29d21d",
    "outputId": "431bf38f-a467-4fdb-9075-4deeb90c0d6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count documents:  10000\n",
      "23/01/2022 00:12:05 |  number of processed documents: 0 |  percentage of completion: 0.0 %\n",
      "23/01/2022 00:12:07 |  number of processed documents: 500 |  percentage of completion: 5.0 %\n",
      "23/01/2022 00:12:09 |  number of processed documents: 1000 |  percentage of completion: 10.0 %\n",
      "23/01/2022 00:12:11 |  number of processed documents: 1500 |  percentage of completion: 15.0 %\n",
      "23/01/2022 00:12:14 |  number of processed documents: 2000 |  percentage of completion: 20.0 %\n",
      "23/01/2022 00:12:16 |  number of processed documents: 2500 |  percentage of completion: 25.0 %\n",
      "23/01/2022 00:12:18 |  number of processed documents: 3000 |  percentage of completion: 30.0 %\n",
      "23/01/2022 00:12:20 |  number of processed documents: 3500 |  percentage of completion: 35.0 %\n",
      "23/01/2022 00:12:22 |  number of processed documents: 4000 |  percentage of completion: 40.0 %\n",
      "23/01/2022 00:12:24 |  number of processed documents: 4500 |  percentage of completion: 45.0 %\n",
      "23/01/2022 00:12:26 |  number of processed documents: 5000 |  percentage of completion: 50.0 %\n",
      "23/01/2022 00:12:28 |  number of processed documents: 5500 |  percentage of completion: 55.00000000000001 %\n",
      "23/01/2022 00:12:30 |  number of processed documents: 6000 |  percentage of completion: 60.0 %\n",
      "23/01/2022 00:12:32 |  number of processed documents: 6500 |  percentage of completion: 65.0 %\n",
      "23/01/2022 00:12:34 |  number of processed documents: 7000 |  percentage of completion: 70.0 %\n",
      "23/01/2022 00:12:36 |  number of processed documents: 7500 |  percentage of completion: 75.0 %\n",
      "23/01/2022 00:12:38 |  number of processed documents: 8000 |  percentage of completion: 80.0 %\n",
      "23/01/2022 00:12:40 |  number of processed documents: 8500 |  percentage of completion: 85.0 %\n",
      "23/01/2022 00:12:41 |  number of processed documents: 9000 |  percentage of completion: 90.0 %\n",
      "23/01/2022 00:12:43 |  number of processed documents: 9500 |  percentage of completion: 95.0 %\n"
     ]
    }
   ],
   "source": [
    "dict_grams_human = make_ngrams(corpus,  dict_cut, N = 2, m = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T10:05:04.379102Z",
     "start_time": "2022-01-20T10:05:04.303378Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "20e4a2cd",
    "outputId": "09b969a5-e267-4484-a5fa-f28c5c167443"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "568473"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_grams_human.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "27febfd3"
   },
   "outputs": [],
   "source": [
    "X0 = []\n",
    "for i in dict_grams_human.keys():\n",
    "    X0.append( dict_grams_human[i][2])\n",
    "\n",
    "list_gramm = [dict_grams_human[i][1] for i in dict_grams_human.keys()]\n",
    "    \n",
    "X_human = pd.DataFrame(X0)\n",
    "X_human['ind'] = dict_grams_human.keys()\n",
    "X_human['name'] = list_gramm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T10:07:16.799996Z",
     "start_time": "2022-01-20T10:06:40.865Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b90f609b",
    "outputId": "6e0fa79f-341d-4815-adc2-4a00714a0880"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "d_orare         887\n",
       "qualche_cosa    791\n",
       "pur_troppo      606\n",
       "re_domandare    552\n",
       "tale_modo       551\n",
       "Name: name, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_human['name'].value_counts()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T10:07:16.801992Z",
     "start_time": "2022-01-20T10:06:41.199Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d07090f8",
    "outputId": "7fcb962b-477d-4f45-8697-d21d88874e07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568473, 22)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_human.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:11:28.047560Z",
     "start_time": "2022-01-20T00:10:38.106966Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d995d904",
    "outputId": "95492066-7ed9-4865-e2e6-4daed715e1bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568473, 22)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_human.to_csv('/content/drive/MyDrive/2022-01-15_Course_project/TF_IDF_clustering_it/10000_n_2gramm_human_it.csv')\n",
    "X_human.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "a16b0d8a"
   },
   "outputs": [],
   "source": [
    "list_col = list(X_human.columns)\n",
    "for i in ['Unnamed: 0', 'ind', 'name']:\n",
    "    if i in list_col:\n",
    "        list_col.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ffc9dd9",
    "outputId": "bc7f2b18-0995-40dd-fae9-5271c61dbb1d"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-23 00:13:53.835 | begin | significance:  1000 | neighbors:  50\n",
      "Start clustering\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: divide by zero encountered in power\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-23 03:27:04.681 | end | {'cohesion': 9939.224741905155, 'separation': 17214.17648725398, 'calinski_harabaz_score': 280.4582057788344, 'RMSSTD': 0.029658116540761992, 'RS': 0.6339602299533745, 'silhouette': -0.1697011337164882, 'significance': 1000, 'neighbors': 50, 'cluster_num': 3490}\n",
      "2022-01-23 03:27:28.331 | begin | significance:  1000 | neighbors:  100\n",
      "Start clustering\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: divide by zero encountered in power\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-23 07:20:16.538 | end | {'cohesion': 9107.560601068471, 'separation': 18045.84062809066, 'calinski_harabaz_score': 1043.8662323405745, 'RMSSTD': 0.028329785406267922, 'RS': 0.6645885896869458, 'silhouette': -0.0921719499961675, 'significance': 1000, 'neighbors': 100, 'cluster_num': 1078}\n",
      "2022-01-23 07:20:40.648 | begin | significance:  100000 | neighbors:  50\n",
      "Start clustering\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: divide by zero encountered in power\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-23 10:37:11.984 | end | {'cohesion': 9939.224741905155, 'separation': 17214.17648725398, 'calinski_harabaz_score': 280.4582057788344, 'RMSSTD': 0.029658116540761992, 'RS': 0.6339602299533745, 'silhouette': -0.1697011337164882, 'significance': 100000, 'neighbors': 50, 'cluster_num': 3490}\n",
      "2022-01-23 10:37:36.649 | begin | significance:  100000 | neighbors:  100\n",
      "Start clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: divide by zero encountered in power\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-23 14:34:49.620 | end | {'cohesion': 9107.560601068471, 'separation': 18045.84062809066, 'calinski_harabaz_score': 1043.8662323405745, 'RMSSTD': 0.028329785406267922, 'RS': 0.6645885896869458, 'silhouette': -0.0921719499961675, 'significance': 100000, 'neighbors': 100, 'cluster_num': 1078}\n"
     ]
    }
   ],
   "source": [
    "#GridSearch for Clustering\n",
    "grid_result = []\n",
    "for sig in [1000, 100000]:\n",
    "    for nei in [50, 100]:\n",
    "        \n",
    "        print(datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3], '| begin |', 'significance: ', sig, '| neighbors: ', nei )\n",
    "        clust = Wishart(significance_level = sig, wishart_neighbors = nei)\n",
    "        result = clust.fit(X_human[list_col])\n",
    "        dict_r = get_validation_scores(np.array(X_human[list_col]), clust.object_labels, max_clust = 10000)\n",
    "        dict_r['significance'] = sig\n",
    "        dict_r['neighbors'] = nei\n",
    "        dict_r['cluster_num'] = len(set(clust.object_labels))\n",
    "        grid_result.append(dict_r)\n",
    "        \n",
    "        #add clustering result to table\n",
    "        name_col = 'cluster_' + str(sig) + str(nei)\n",
    "        X_human[name_col] = clust.object_labels\n",
    "        \n",
    "        print(datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3], '| end |',  dict_r)\n",
    "\n",
    "        X_human.to_csv('/content/drive/MyDrive/2022-01-15_Course_project/TF_IDF_clustering_it/10000_n_2gramm_human_it.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LUinoZ--sc7b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "name": "TF_IDF_Clustering_it.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
