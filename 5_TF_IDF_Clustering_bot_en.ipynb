{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:05:19.638787Z",
     "start_time": "2022-01-20T00:05:19.628854Z"
    },
    "id": "cdb50c2b"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T23:16:44.565625Z",
     "start_time": "2022-01-19T23:16:40.800174Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6120d187",
    "outputId": "c0dc6424-4e18-4ee4-da73-2aeafe9b0c85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse.linalg import svds\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WRt8kPDwn-_b",
    "outputId": "305e8dc4-258e-46a9-ecc9-7a1f686f79e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T23:16:51.418514Z",
     "start_time": "2022-01-19T23:16:51.400418Z"
    },
    "id": "3bd2f029"
   },
   "outputs": [],
   "source": [
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5a7425c8",
    "outputId": "c460aa5c-a80c-4baa-943b-7ba908f51e7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OyHG--A2ocGd",
    "outputId": "a7ae3b4a-767d-4bc9-d902-cc0f575c5848"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glob.glob('/content/drive/MyDrive/2022-01-15_Course_project/prep_gen_text_en/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "EgDyD1Avn6vD"
   },
   "outputs": [],
   "source": [
    "# Let's select 10k texts in a folder: '/content/drive/MyDrive/2022-01-15_Course_project/10000_cut_en/'\n",
    "# Because clustering works for a very long time on large datasets\n",
    "\n",
    "import shutil\n",
    "file_list = glob.glob('/content/drive/MyDrive/2022-01-15_Course_project/prep_gen_text_en/*')\n",
    "k = 0\n",
    "\n",
    "for i in file_list:\n",
    "  shutil.copy(i, '/content/drive/MyDrive/2022-01-15_Course_project/10000_prep_gen_text_en/')\n",
    "  k += 1\n",
    "  if k >= 10000:\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C4h15_R-n8QT",
    "outputId": "a2dc0c52-2546-4cf8-bde2-4032a446a88c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glob.glob('/content/drive/MyDrive/2022-01-15_Course_project/10000_prep_gen_text_en/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "MaplzLoMnLuZ"
   },
   "outputs": [],
   "source": [
    "# Wishart clustering function\n",
    "# https://github.com/Radi4/BotDetection/blob/master/Wishart.py\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import gamma\n",
    "from sklearn.neighbors import KDTree\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Wishart:\n",
    "    def __init__(self, wishart_neighbors, significance_level):\n",
    "        self.wishart_neighbors = wishart_neighbors  # Number of neighbors\n",
    "        self.significance_level = significance_level  # Significance level\n",
    "\n",
    "    def fit(self, X):\n",
    "        from sklearn.neighbors import KDTree\n",
    "        kdt = KDTree(X, metric='euclidean')\n",
    "\n",
    "        #add one because you are your neighb.\n",
    "        distances, neighbors = kdt.query(X, k = self.wishart_neighbors + 1, return_distance = True)\n",
    "        neighbors = neighbors[:, 1:]\n",
    "\n",
    "\n",
    "        distances = distances[:, -1]\n",
    "        indexes = np.argsort(distances)\n",
    "        \n",
    "        size, dim = X.shape\n",
    "\n",
    "        self.object_labels = np.zeros(size, dtype = int) - 1\n",
    "\n",
    "        #index in tuple\n",
    "        #min_dist, max_dist, flag_to_significant\n",
    "        self.clusters = np.array([(1., 1., 0)])\n",
    "        self.clusters_to_objects = defaultdict(list)\n",
    "        print('Start clustering')\n",
    "\n",
    "        for index in indexes:\n",
    "            neighbors_clusters =\\\n",
    "                np.concatenate([self.object_labels[neighbors[index]], self.object_labels[neighbors[index]]])\n",
    "            unique_clusters = np.unique(neighbors_clusters).astype(int)\n",
    "            unique_clusters = unique_clusters[unique_clusters != -1]\n",
    "\n",
    "\n",
    "            if len(unique_clusters) == 0:\n",
    "                self._create_new_cluster(index, distances[index])\n",
    "            else:\n",
    "                max_cluster = unique_clusters[-1]\n",
    "                min_cluster = unique_clusters[0]\n",
    "                if max_cluster == min_cluster:\n",
    "                    if self.clusters[max_cluster][-1] < 0.5:\n",
    "                        self._add_elem_to_exist_cluster(index, distances[index], max_cluster)\n",
    "                    else:\n",
    "                        self._add_elem_to_noise(index)\n",
    "                else:\n",
    "                    my_clusters = self.clusters[unique_clusters]\n",
    "                    flags = my_clusters[:, -1]\n",
    "                    if np.min(flags) > 0.5:\n",
    "                        self._add_elem_to_noise(index)\n",
    "                    else:\n",
    "                        significan = np.power(my_clusters[:, 0], -dim) - np.power(my_clusters[:, 1], -dim)\n",
    "                        significan *= self.wishart_neighbors\n",
    "                        significan /= size\n",
    "                        significan /= np.power(np.pi, dim / 2)\n",
    "                        significan *= gamma(dim / 2 + 1)\n",
    "                        significan_index = significan >= self.significance_level\n",
    "\n",
    "                        significan_clusters = unique_clusters[significan_index]\n",
    "                        not_significan_clusters = unique_clusters[~significan_index]\n",
    "                        significan_clusters_count = len(significan_clusters)\n",
    "                        if significan_clusters_count > 1 or min_cluster == 0:\n",
    "                            self._add_elem_to_noise(index)\n",
    "                            self.clusters[significan_clusters, -1] = 1\n",
    "                            for not_sig_cluster in not_significan_clusters:\n",
    "                                if not_sig_cluster == 0:\n",
    "                                    continue\n",
    "\n",
    "                                for bad_index in self.clusters_to_objects[not_sig_cluster]:\n",
    "                                    self._add_elem_to_noise(bad_index)\n",
    "                                self.clusters_to_objects[not_sig_cluster].clear()\n",
    "                        else:\n",
    "                            for cur_cluster in unique_clusters:\n",
    "                                if cur_cluster == min_cluster:\n",
    "                                    continue\n",
    "\n",
    "                                for bad_index in self.clusters_to_objects[cur_cluster]:\n",
    "                                    self._add_elem_to_exist_cluster(bad_index, distances[bad_index], min_cluster)\n",
    "                                self.clusters_to_objects[cur_cluster].clear()\n",
    "\n",
    "                            self._add_elem_to_exist_cluster(index, distances[index], min_cluster)\n",
    "\n",
    "        return self.clean_data()\n",
    "\n",
    "    def clean_data(self):\n",
    "        unique = np.unique(self.object_labels)\n",
    "        index = np.argsort(unique)\n",
    "        if unique[0] != 0:\n",
    "            index += 1\n",
    "        true_cluster = {unq :  index for unq, index in zip(unique, index)}\n",
    "        result = np.zeros(len(self.object_labels), dtype = int)\n",
    "        for index, unq in enumerate(self.object_labels):\n",
    "            result[index] = true_cluster[unq]\n",
    "        return result\n",
    "\n",
    "    def _add_elem_to_noise(self, index):\n",
    "        self.object_labels[index] = 0\n",
    "        self.clusters_to_objects[0].append(index)\n",
    "\n",
    "    def _create_new_cluster(self, index, dist):\n",
    "        self.object_labels[index] = len(self.clusters)\n",
    "        self.clusters_to_objects[len(self.clusters)].append(index)\n",
    "        self.clusters = np.append(self.clusters, [(dist, dist, 0)], axis = 0)\n",
    "\n",
    "    def _add_elem_to_exist_cluster(self, index, dist, cluster_label):\n",
    "        self.object_labels[index] = cluster_label\n",
    "        self.clusters_to_objects[cluster_label].append(index)\n",
    "        self.clusters[cluster_label][0] = min(self.clusters[cluster_label][0], dist)\n",
    "        self.clusters[cluster_label][1] = max(self.clusters[cluster_label][1], dist)\n",
    "\n",
    "\n",
    "class PreTrainWishart:\n",
    "    def __init__(self, wishart_neighbors, significance_level, distances, neighbors):\n",
    "        self.wishart_neighbors = wishart_neighbors  # Number of neighbors\n",
    "        self.significance_level = significance_level  # Significance level\n",
    "        self.distances = distances\n",
    "        self.neighbors = neighbors\n",
    "\n",
    "    def fit(self, X):\n",
    "        from sklearn.neighbors import KDTree\n",
    "        kdt = KDTree(X, metric='euclidean')\n",
    "\n",
    "        #add one because you are your neighb.\n",
    "        neighbors = self.neighbors[:, 1 : self.wishart_neighbors + 1]\n",
    "        distances = self.distances[:, self.wishart_neighbors]\n",
    "        indexes = np.argsort(distances)\n",
    "        \n",
    "        size, dim = X.shape\n",
    "\n",
    "        self.object_labels = np.zeros(size, dtype = int) - 1\n",
    "\n",
    "        #index in tuple\n",
    "        #min_dist, max_dist, flag_to_significant\n",
    "        self.clusters = np.array([(1., 1., 0)])\n",
    "        self.clusters_to_objects = defaultdict(list)\n",
    "\n",
    "        for index in indexes:\n",
    "            neighbors_clusters =\\\n",
    "                np.concatenate([self.object_labels[neighbors[index]], self.object_labels[neighbors[index]]])\n",
    "            unique_clusters = np.unique(neighbors_clusters).astype(int)\n",
    "            unique_clusters = unique_clusters[unique_clusters != -1]\n",
    "\n",
    "\n",
    "            if len(unique_clusters) == 0:\n",
    "                self._create_new_cluster(index, distances[index])\n",
    "            else:\n",
    "                max_cluster = unique_clusters[-1]\n",
    "                min_cluster = unique_clusters[0]\n",
    "                if max_cluster == min_cluster:\n",
    "                    if self.clusters[max_cluster][-1] < 0.5:\n",
    "                        self._add_elem_to_exist_cluster(index, distances[index], max_cluster)\n",
    "                    else:\n",
    "                        self._add_elem_to_noise(index)\n",
    "                else:\n",
    "                    my_clusters = self.clusters[unique_clusters]\n",
    "                    flags = my_clusters[:, -1]\n",
    "                    if np.min(flags) > 0.5:\n",
    "                        self._add_elem_to_noise(index)\n",
    "                    else:\n",
    "                        significan = np.power(my_clusters[:, 0], -dim) - np.power(my_clusters[:, 1], -dim)\n",
    "                        significan *= self.wishart_neighbors\n",
    "                        significan /= size\n",
    "                        significan /= np.power(np.pi, dim / 2)\n",
    "                        significan *= gamma(dim / 2 + 1)\n",
    "                        significan_index = significan >= self.significance_level\n",
    "\n",
    "                        significan_clusters = unique_clusters[significan_index]\n",
    "                        not_significan_clusters = unique_clusters[~significan_index]\n",
    "                        significan_clusters_count = len(significan_clusters)\n",
    "                        if significan_clusters_count > 1 or min_cluster == 0:\n",
    "                            self._add_elem_to_noise(index)\n",
    "                            self.clusters[significan_clusters, -1] = 1\n",
    "                            for not_sig_cluster in not_significan_clusters:\n",
    "                                if not_sig_cluster == 0:\n",
    "                                    continue\n",
    "\n",
    "                                for bad_index in self.clusters_to_objects[not_sig_cluster]:\n",
    "                                    self._add_elem_to_noise(bad_index)\n",
    "                                self.clusters_to_objects[not_sig_cluster].clear()\n",
    "                        else:\n",
    "                            for cur_cluster in unique_clusters:\n",
    "                                if cur_cluster == min_cluster:\n",
    "                                    continue\n",
    "\n",
    "                                for bad_index in self.clusters_to_objects[cur_cluster]:\n",
    "                                    self._add_elem_to_exist_cluster(bad_index, distances[bad_index], min_cluster)\n",
    "                                self.clusters_to_objects[cur_cluster].clear()\n",
    "\n",
    "                            self._add_elem_to_exist_cluster(index, distances[index], min_cluster)\n",
    "\n",
    "        return self.clean_data()\n",
    "\n",
    "    def clean_data(self):\n",
    "        unique = np.unique(self.object_labels)\n",
    "        index = np.argsort(unique)\n",
    "        if unique[0] != 0:\n",
    "            index += 1\n",
    "        true_cluster = {unq :  index for unq, index in zip(unique, index)}\n",
    "        result = np.zeros(len(self.object_labels), dtype = int)\n",
    "        for index, unq in enumerate(self.object_labels):\n",
    "            result[index] = true_cluster[unq]\n",
    "        return result\n",
    "\n",
    "    def _add_elem_to_noise(self, index):\n",
    "        self.object_labels[index] = 0\n",
    "        self.clusters_to_objects[0].append(index)\n",
    "\n",
    "    def _create_new_cluster(self, index, dist):\n",
    "        self.object_labels[index] = len(self.clusters)\n",
    "        self.clusters_to_objects[len(self.clusters)].append(index)\n",
    "        self.clusters = np.append(self.clusters, [(dist, dist, 0)], axis = 0)\n",
    "\n",
    "    def _add_elem_to_exist_cluster(self, index, dist, cluster_label):\n",
    "        self.object_labels[index] = cluster_label\n",
    "        self.clusters_to_objects[cluster_label].append(index)\n",
    "        self.clusters[cluster_label][0] = min(self.clusters[cluster_label][0], dist)\n",
    "        self.clusters[cluster_label][1] = max(self.clusters[cluster_label][1], dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7f043e55"
   },
   "source": [
    "## Create a vector representation based on TfidfVectorizer (on the bot texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:02:39.113091Z",
     "start_time": "2022-01-20T00:02:39.053009Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7b1f3a37",
    "outputId": "bd99d257-6db8-4585-ee26-86abbf84decb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:03:08.870021Z",
     "start_time": "2022-01-20T00:03:08.861187Z"
    },
    "id": "4d3f9554"
   },
   "outputs": [],
   "source": [
    "def make_corpus(input_path, output_file_path):\n",
    "    i = 0\n",
    "    file_list = glob.glob(input_path + '*')\n",
    "    \n",
    "    with open(output_file_path, 'w+') as output_file:\n",
    "        for file in file_list:\n",
    "            if i % 500 == 0:\n",
    "                now = datetime.now()\n",
    "                dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "                print(dt_string, '| ',  'number of processed files: ' + str(i), '| ', \n",
    "                      'percentage of completion:', str(round(i/len(file_list), 2)* 100) + ' %' )\n",
    "            i+=1\n",
    "            with open(file, 'r') as input_file:\n",
    "                output_file.write(input_file.read().replace('\\n', ' '))\n",
    "                output_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gY0zi4OpuaBy",
    "outputId": "93a65e9a-01bd-4717-9b94-80760eb881b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/2022 07:32:17 |  number of processed files: 0 |  percentage of completion: 0.0 %\n",
      "23/01/2022 07:32:24 |  number of processed files: 500 |  percentage of completion: 5.0 %\n",
      "23/01/2022 07:32:25 |  number of processed files: 1000 |  percentage of completion: 10.0 %\n",
      "23/01/2022 07:32:25 |  number of processed files: 1500 |  percentage of completion: 15.0 %\n",
      "23/01/2022 07:32:26 |  number of processed files: 2000 |  percentage of completion: 20.0 %\n",
      "23/01/2022 07:32:26 |  number of processed files: 2500 |  percentage of completion: 25.0 %\n",
      "23/01/2022 07:32:27 |  number of processed files: 3000 |  percentage of completion: 30.0 %\n",
      "23/01/2022 07:32:27 |  number of processed files: 3500 |  percentage of completion: 35.0 %\n",
      "23/01/2022 07:32:28 |  number of processed files: 4000 |  percentage of completion: 40.0 %\n",
      "23/01/2022 07:32:29 |  number of processed files: 4500 |  percentage of completion: 45.0 %\n",
      "23/01/2022 07:32:29 |  number of processed files: 5000 |  percentage of completion: 50.0 %\n",
      "23/01/2022 07:32:30 |  number of processed files: 5500 |  percentage of completion: 55.00000000000001 %\n",
      "23/01/2022 07:32:31 |  number of processed files: 6000 |  percentage of completion: 60.0 %\n",
      "23/01/2022 07:32:31 |  number of processed files: 6500 |  percentage of completion: 65.0 %\n",
      "23/01/2022 07:32:32 |  number of processed files: 7000 |  percentage of completion: 70.0 %\n",
      "23/01/2022 07:32:32 |  number of processed files: 7500 |  percentage of completion: 75.0 %\n",
      "23/01/2022 07:32:33 |  number of processed files: 8000 |  percentage of completion: 80.0 %\n",
      "23/01/2022 07:32:34 |  number of processed files: 8500 |  percentage of completion: 85.0 %\n",
      "23/01/2022 07:32:34 |  number of processed files: 9000 |  percentage of completion: 90.0 %\n",
      "23/01/2022 07:32:35 |  number of processed files: 9500 |  percentage of completion: 95.0 %\n"
     ]
    }
   ],
   "source": [
    "# Let's make corpus for bot texts\n",
    "\n",
    "make_corpus('/content/drive/MyDrive/2022-01-15_Course_project/10000_prep_gen_text_en/',\n",
    "            '/content/drive/MyDrive/2022-01-15_Course_project/TF_IDF_clustering_bot_en/10000_dataset_generate_en.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:03:42.696966Z",
     "start_time": "2022-01-20T00:03:42.639936Z"
    },
    "id": "07c89c06"
   },
   "outputs": [],
   "source": [
    "# TF-IDF corpus\n",
    "\n",
    "def make_table_and_dict(corpus_path, min_df, max_df, token_pattern = None, use_idf = True, stop_words = 'english'):\n",
    "    \n",
    "    with open(corpus_path, 'r') as corpus_file:\n",
    "        if token_pattern:\n",
    "            vectorizer = TfidfVectorizer(analyzer='word', min_df=min_df, token_pattern=token_pattern, use_idf=use_idf)\n",
    "        else:\n",
    "            vectorizer = TfidfVectorizer(analyzer='word', min_df=min_df)\n",
    "        data_vectorized = vectorizer.fit_transform(corpus_file)\n",
    "    return data_vectorized, vectorizer.get_feature_names(), vectorizer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:03:43.839418Z",
     "start_time": "2022-01-20T00:03:43.817476Z"
    },
    "id": "8f4de5bf"
   },
   "outputs": [],
   "source": [
    "def create_table(data_vectorized, k, name, path):\n",
    "    u, sigma, vt = svds(data_vectorized, k)\n",
    "    print(sigma)\n",
    "    dict_ = np.dot(np.diag(sigma), vt).T\n",
    "        \n",
    "    with open(path + name + str(k) + '.pkl', 'wb') as f:\n",
    "        pickle.dump(dict_, f)\n",
    "    return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "JALQb0THBpJl"
   },
   "outputs": [],
   "source": [
    "# Vector representation on human texts (used on the bot)\n",
    "f = open('/content/drive/MyDrive/2022-01-15_Course_project/TF_IDF_clustering_en/10000_TF_IDF_cut_human_en.pkl', 'rb')\n",
    "dict_cut = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:06:30.784942Z",
     "start_time": "2022-01-20T00:06:30.769056Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "895287d1",
    "outputId": "97df8d44-bc9e-4e7a-f148-b9096452caf5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1528"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_cut.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ce276f30"
   },
   "source": [
    "# Making n-grams and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:06:36.788059Z",
     "start_time": "2022-01-20T00:06:36.765360Z"
    },
    "id": "3edea2f9"
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from math import log\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics import calinski_harabasz_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:06:39.611702Z",
     "start_time": "2022-01-20T00:06:39.591675Z"
    },
    "id": "8b9b8248"
   },
   "outputs": [],
   "source": [
    "def divide(data, labels):\n",
    "    clusters = set(labels)\n",
    "    clusters_data = []\n",
    "    for cluster in clusters:\n",
    "        clusters_data.append(data[labels == cluster, :])\n",
    "    return clusters_data\n",
    "\n",
    "def get_centroids(clusters):\n",
    "    centroids = []\n",
    "    for cluster_data in clusters:\n",
    "        centroids.append(cluster_data.mean(axis=0))\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:06:40.279933Z",
     "start_time": "2022-01-20T00:06:40.241868Z"
    },
    "id": "9eb06fd2"
   },
   "outputs": [],
   "source": [
    "def cohesion(data, labels):\n",
    "    clusters = sorted(set(labels))\n",
    "    sse = 0\n",
    "    for cluster in clusters:\n",
    "        cluster_data = data[labels == cluster, :]\n",
    "        centroid = cluster_data.mean(axis = 0)\n",
    "        sse += ((cluster_data - centroid)**2).sum()\n",
    "    return sse\n",
    "\n",
    "def separation(data, labels, cohesion_score):\n",
    "    # calculate separation as SST - SSE\n",
    "    return cohesion(data, np.zeros(data.shape[0])) - cohesion_score\n",
    "\n",
    "def SST(data):\n",
    "    c = get_centroids([data])\n",
    "    return ((data - c) ** 2).sum()\n",
    "\n",
    "def SSE(clusters, centroids):\n",
    "    result = 0\n",
    "    for cluster, centroid in zip(clusters, centroids):\n",
    "        result += ((cluster - centroid) ** 2).sum()\n",
    "    return result\n",
    "\n",
    "# Clear the store before running each time\n",
    "within_cluster_dist_sum_store = {}\n",
    "def within_cluster_dist_sum(cluster, centroid, cluster_id):\n",
    "    if cluster_id in within_cluster_dist_sum_store:\n",
    "        return within_cluster_dist_sum_store[cluster_id]\n",
    "    else:\n",
    "        result = (((cluster - centroid) ** 2).sum(axis=1)**.5).sum()\n",
    "        within_cluster_dist_sum_store[cluster_id] = result\n",
    "    return result\n",
    "\n",
    "def RMSSTD(data, clusters, centroids):\n",
    "    df = data.shape[0] - len(clusters)\n",
    "    attribute_num = data.shape[1]\n",
    "    return (SSE(clusters, centroids) / (attribute_num * df)) ** .5\n",
    "\n",
    "# equal to separation / (cohesion + separation)\n",
    "def RS(data, clusters, centroids):\n",
    "    sst = SST(data)\n",
    "    sse = SSE(clusters, centroids)\n",
    "    return (sst - sse) / sst\n",
    "\n",
    "def DB_find_max_j(clusters, centroids, i):\n",
    "    max_val = 0\n",
    "    max_j = 0\n",
    "    for j in range(len(clusters)):\n",
    "        if j == i:\n",
    "            continue\n",
    "        cluster_i_stat = within_cluster_dist_sum(clusters[i], centroids[i], i) / clusters[i].shape[0]\n",
    "        cluster_j_stat = within_cluster_dist_sum(clusters[j], centroids[j], j) / clusters[j].shape[0]\n",
    "        val = (cluster_i_stat + cluster_j_stat) / (((centroids[i] - centroids[j]) ** 2).sum() ** .5)\n",
    "        if val > max_val:\n",
    "            max_val = val\n",
    "            max_j = j\n",
    "    return max_val\n",
    "\n",
    "def DB(data, clusters, centroids):\n",
    "    result = 0\n",
    "    for i in range(len(clusters)):\n",
    "        result += DB_find_max_j(clusters, centroids, i)\n",
    "    return result / len(clusters)\n",
    "\n",
    "def XB(data, clusters, centroids):\n",
    "    sse = SSE(clusters, centroids)\n",
    "    min_dist = ((centroids[0] - centroids[1]) ** 2).sum()\n",
    "    for centroid_i, centroid_j in list(product(centroids, centroids)):\n",
    "        if (centroid_i - centroid_j).sum() == 0:\n",
    "            continue\n",
    "        dist = ((centroid_i - centroid_j) ** 2).sum()\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "    return sse / (data.shape[0] * min_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T17:23:21.939658Z",
     "start_time": "2022-01-20T17:23:21.853300Z"
    },
    "id": "93e7bacc"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "# Some metrics can work for a very long time (commented out)\n",
    "\n",
    "def get_validation_scores(data, labels, max_clust = None):\n",
    "    #within_cluster_dist_sum_store.clear()\n",
    "    \n",
    "    clusters = divide(data, labels)\n",
    "    centroids = get_centroids(clusters)\n",
    "    \n",
    "    scores = {}\n",
    "    if max_clust:\n",
    "        if len(clusters) > max_clust:\n",
    "            scores['cohesion'] = cohesion(data, labels)\n",
    "            scores['separation'] = separation(data, labels, scores['cohesion'])\n",
    "            scores['calinski_harabaz_score'] = None\n",
    "            scores['RMSSTD'] = RMSSTD(data, clusters, centroids)\n",
    "            scores['RS'] = RS(data, clusters, centroids)\n",
    "            #scores['DB'] = None\n",
    "            #scores['XB'] = XB(data, clusters, centroids)\n",
    "            scores['silhouette'] = None\n",
    "        else:\n",
    "            scores['cohesion'] = cohesion(data, labels)\n",
    "            scores['separation'] = separation(data, labels, scores['cohesion'])\n",
    "            scores['calinski_harabaz_score'] = calinski_harabasz_score(data, labels)\n",
    "            scores['RMSSTD'] = RMSSTD(data, clusters, centroids)\n",
    "            scores['RS'] = RS(data, clusters, centroids)\n",
    "            #scores['DB'] = DB(data, clusters, centroids)\n",
    "            #scores['XB'] = XB(data, clusters, centroids)\n",
    "            scores['silhouette'] = silhouette_score(data, labels)\n",
    "    else:\n",
    "        scores['cohesion'] = cohesion(data, labels)\n",
    "        scores['separation'] = separation(data, labels, scores['cohesion'])\n",
    "        scores['calinski_harabaz_score'] = calinski_harabasz_score(data, labels)\n",
    "        scores['RMSSTD'] = RMSSTD(data, clusters, centroids)\n",
    "        scores['RS'] = RS(data, clusters, centroids)\n",
    "        #scores['DB'] = DB(data, clusters, centroids)\n",
    "        #scores['XB'] = XB(data, clusters, centroids)\n",
    "        scores['silhouette'] = silhouette_score(data, labels)\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:06:52.939414Z",
     "start_time": "2022-01-20T00:06:52.923459Z"
    },
    "id": "4a8e01ac"
   },
   "outputs": [],
   "source": [
    "def make_ngrams(input_corpus,  dict_, N = 2, m = None, uniq = False):\n",
    "    dict_grams = dict()\n",
    "    num_ = 0\n",
    "    i = 0\n",
    "    j = 0\n",
    "    \n",
    "    print('Count documents: ', len(input_corpus))\n",
    "    for sentence in input_corpus:\n",
    "        sentence = sentence.split(' ')\n",
    "        grams = [sentence[i:i+N] for i in range(len(sentence)-N+1)]\n",
    "        for g in grams:\n",
    "            g_key = '_'.join(elem for elem in g)\n",
    "\n",
    "            if uniq:\n",
    "                if all(elem in dict_.keys()  for elem in g) and (g_key not in dict_grams.keys()):\n",
    "                    dict_grams[g_key] = []\n",
    "                    for elem in g:\n",
    "                            if m:\n",
    "                                dict_grams[g_key] += list(dict_[elem][1][:m])\n",
    "                            else:\n",
    "                                dict_grams[g_key] += list(dict_[elem][1])\n",
    "            else:\n",
    "                if all(elem in dict_.keys()  for elem in g):\n",
    "                    concat = []\n",
    "                    for elem in g:\n",
    "                        if m:\n",
    "                            concat += list(dict_[elem][1][:m])\n",
    "                        else:\n",
    "                            concat += list(dict_[elem][1])\n",
    "                    dict_grams[i] = (j, g_key, concat)\n",
    "                    i += 1\n",
    "            j += 1\n",
    "       \n",
    "            \n",
    "        if num_ % 500 == 0:\n",
    "            now = datetime.now()\n",
    "            dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "            print(dt_string, '| ',  'number of processed documents: ' + str(num_), '| ', \n",
    "                      'percentage of completion:', str(round(num_/len(input_corpus), 2)* 100) + ' %' )\n",
    "        num_ += 1\n",
    "    return dict_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:07:43.802071Z",
     "start_time": "2022-01-20T00:07:43.415738Z"
    },
    "id": "8eae3315"
   },
   "outputs": [],
   "source": [
    "with open('/content/drive/MyDrive/2022-01-15_Course_project/TF_IDF_clustering_bot_en/10000_dataset_generate_en.txt', 'r') as corpus_file:\n",
    "    corpus = corpus_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:09:15.616274Z",
     "start_time": "2022-01-20T00:07:51.301224Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3b29d21d",
    "outputId": "c82bc2cb-f320-4a63-80de-569e3e092497"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count documents:  10000\n",
      "23/01/2022 07:33:20 |  number of processed documents: 0 |  percentage of completion: 0.0 %\n",
      "23/01/2022 07:33:21 |  number of processed documents: 500 |  percentage of completion: 5.0 %\n",
      "23/01/2022 07:33:23 |  number of processed documents: 1000 |  percentage of completion: 10.0 %\n",
      "23/01/2022 07:33:25 |  number of processed documents: 1500 |  percentage of completion: 15.0 %\n",
      "23/01/2022 07:33:26 |  number of processed documents: 2000 |  percentage of completion: 20.0 %\n",
      "23/01/2022 07:33:28 |  number of processed documents: 2500 |  percentage of completion: 25.0 %\n",
      "23/01/2022 07:33:30 |  number of processed documents: 3000 |  percentage of completion: 30.0 %\n",
      "23/01/2022 07:33:31 |  number of processed documents: 3500 |  percentage of completion: 35.0 %\n",
      "23/01/2022 07:33:33 |  number of processed documents: 4000 |  percentage of completion: 40.0 %\n",
      "23/01/2022 07:33:35 |  number of processed documents: 4500 |  percentage of completion: 45.0 %\n",
      "23/01/2022 07:33:36 |  number of processed documents: 5000 |  percentage of completion: 50.0 %\n",
      "23/01/2022 07:33:38 |  number of processed documents: 5500 |  percentage of completion: 55.00000000000001 %\n",
      "23/01/2022 07:33:39 |  number of processed documents: 6000 |  percentage of completion: 60.0 %\n",
      "23/01/2022 07:33:41 |  number of processed documents: 6500 |  percentage of completion: 65.0 %\n",
      "23/01/2022 07:33:43 |  number of processed documents: 7000 |  percentage of completion: 70.0 %\n",
      "23/01/2022 07:33:45 |  number of processed documents: 7500 |  percentage of completion: 75.0 %\n",
      "23/01/2022 07:33:46 |  number of processed documents: 8000 |  percentage of completion: 80.0 %\n",
      "23/01/2022 07:33:48 |  number of processed documents: 8500 |  percentage of completion: 85.0 %\n",
      "23/01/2022 07:33:50 |  number of processed documents: 9000 |  percentage of completion: 90.0 %\n",
      "23/01/2022 07:33:51 |  number of processed documents: 9500 |  percentage of completion: 95.0 %\n"
     ]
    }
   ],
   "source": [
    "dict_grams_bot = make_ngrams(corpus,  dict_cut, N = 2, m = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T10:05:04.379102Z",
     "start_time": "2022-01-20T10:05:04.303378Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "20e4a2cd",
    "outputId": "752c6f9b-94e3-47b6-dc93-8984675549d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185136"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_grams_bot.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "27febfd3"
   },
   "outputs": [],
   "source": [
    "X0 = []\n",
    "for i in dict_grams_bot.keys():\n",
    "    X0.append( dict_grams_bot[i][2])\n",
    "\n",
    "list_gramm = [dict_grams_bot[i][1] for i in dict_grams_bot.keys()]\n",
    "    \n",
    "X_bot = pd.DataFrame(X0)\n",
    "X_bot['ind'] = dict_grams_bot.keys()\n",
    "X_bot['name'] = list_gramm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T10:07:16.799996Z",
     "start_time": "2022-01-20T10:06:40.865Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b90f609b",
    "outputId": "909b04b9-2279-4c24-f274-5836c2aac143"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "though_never    123\n",
       "must_never      104\n",
       "though_still     89\n",
       "still_seem       85\n",
       "never_think      84\n",
       "Name: name, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bot['name'].value_counts()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T10:07:16.801992Z",
     "start_time": "2022-01-20T10:06:41.199Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d07090f8",
    "outputId": "03fa2385-790b-406c-d687-a785f9d4847b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(185136, 22)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-20T00:11:28.047560Z",
     "start_time": "2022-01-20T00:10:38.106966Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d995d904",
    "outputId": "8a4c59fa-9422-44e8-a0fe-cfbaa2a2883f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(185136, 22)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bot.to_csv('/content/drive/MyDrive/2022-01-15_Course_project/TF_IDF_clustering_bot_en/10000_n_2gramm_bot_en.csv')\n",
    "X_bot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "a16b0d8a"
   },
   "outputs": [],
   "source": [
    "list_col = list(X_bot.columns)\n",
    "for i in ['Unnamed: 0', 'ind', 'name']:\n",
    "    if i in list_col:\n",
    "        list_col.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ffc9dd9",
    "outputId": "87cebf18-a844-4e69-ed96-4e1d883ca66d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-23 07:34:36.546 | begin | significance:  1000 | neighbors:  50\n",
      "Start clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: divide by zero encountered in power\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-23 07:56:23.114 | end | {'cohesion': 2749.449792574569, 'separation': 5000.656659175672, 'calinski_harabaz_score': 777.6272060819349, 'RMSSTD': 0.027281668292086288, 'RS': 0.6452371577485043, 'silhouette': -0.07547212191489622, 'significance': 1000, 'neighbors': 50, 'cluster_num': 433}\n",
      "2022-01-23 07:56:29.516 | begin | significance:  1000 | neighbors:  100\n",
      "Start clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: divide by zero encountered in power\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-23 08:23:23.745 | end | {'cohesion': 2780.602040187679, 'separation': 4969.504411562562, 'calinski_harabaz_score': 1167.3792288173, 'RMSSTD': 0.027424728941625044, 'RS': 0.6412175681071163, 'silhouette': -0.060082838204908326, 'significance': 1000, 'neighbors': 100, 'cluster_num': 284}\n",
      "2022-01-23 08:23:30.760 | begin | significance:  100000 | neighbors:  50\n",
      "Start clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: divide by zero encountered in power\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-23 08:45:27.840 | end | {'cohesion': 2749.449792574569, 'separation': 5000.656659175672, 'calinski_harabaz_score': 777.6272060819349, 'RMSSTD': 0.027281668292086288, 'RS': 0.6452371577485043, 'silhouette': -0.07547212191489622, 'significance': 100000, 'neighbors': 50, 'cluster_num': 433}\n",
      "2022-01-23 08:45:34.856 | begin | significance:  100000 | neighbors:  100\n",
      "Start clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: divide by zero encountered in power\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-23 09:11:46.908 | end | {'cohesion': 2780.602040187679, 'separation': 4969.504411562562, 'calinski_harabaz_score': 1167.3792288173, 'RMSSTD': 0.027424728941625044, 'RS': 0.6412175681071163, 'silhouette': -0.060082838204908326, 'significance': 100000, 'neighbors': 100, 'cluster_num': 284}\n"
     ]
    }
   ],
   "source": [
    "#GridSearch for Clustering\n",
    "grid_result = []\n",
    "for sig in [1000, 100000]:\n",
    "    for nei in [50, 100]:\n",
    "        \n",
    "        print(datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3], '| begin |', 'significance: ', sig, '| neighbors: ', nei )\n",
    "        clust = Wishart(significance_level = sig, wishart_neighbors = nei)\n",
    "        result = clust.fit(X_bot[list_col])\n",
    "        dict_r = get_validation_scores(np.array(X_bot[list_col]), clust.object_labels, max_clust = 10000)\n",
    "        dict_r['significance'] = sig\n",
    "        dict_r['neighbors'] = nei\n",
    "        dict_r['cluster_num'] = len(set(clust.object_labels))\n",
    "        grid_result.append(dict_r)\n",
    "        \n",
    "        #add clustering result to table\n",
    "        name_col = 'cluster_' + str(sig) + str(nei)\n",
    "        X_bot[name_col] = clust.object_labels\n",
    "        \n",
    "        print(datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3], '| end |',  dict_r)\n",
    "\n",
    "        X_bot.to_csv('/content/drive/MyDrive/2022-01-15_Course_project/TF_IDF_clustering_bot_en/10000_n_2gramm_bot_en.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MztZ5X9m1UT0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "name": "TF_IDF_Clustering_bot_en.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
