{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SXghWytGaz4E"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "import os, zipfile, glob\n",
        "import pickle\n",
        "import re\n",
        "import gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gGPuN1LbDYP",
        "outputId": "d949cb81-b998-4899-d982-8de61c92963b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9iyNXytUuWT",
        "outputId": "0b2da891-11b7-422b-9e3c-814dfe91fe18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 29.4 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20 kB 33.8 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30 kB 28.9 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 40 kB 21.6 MB/s eta 0:00:01\r\u001b[K     |███████                         | 51 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 61 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 71 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 81 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 92 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 102 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 112 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 122 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 133 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 143 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 153 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 163 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 174 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 184 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 194 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 204 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 215 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 225 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235 kB 14.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install unidecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZTBdWWP1BSWC"
      },
      "outputs": [],
      "source": [
        "import codecs\n",
        "from bs4 import BeautifulSoup\n",
        "import unidecode\n",
        "import random\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBe-Ea4A7IJ-",
        "outputId": "f94ab3d0-1afb-42e0-ce50-6aec9a6813a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbDVXtbK7V0Z",
        "outputId": "5ffa9d26-4f5a-423c-b31d-7951edbbfb76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.66-py2.py3-none-any.whl (8.0 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting anyascii\n",
            "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▏                              | 10 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20 kB 31.2 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 30 kB 37.0 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 40 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 51 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 61 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |████████                        | 71 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 81 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 92 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 102 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 112 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 122 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 133 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 143 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 153 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 163 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 174 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 184 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 194 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 204 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 215 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 225 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 235 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 245 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 256 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 266 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 276 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 284 kB 13.4 MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.2.tar.gz (321 kB)\n",
            "\u001b[K     |████████████████████████████████| 321 kB 64.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85445 sha256=9e666135719670d9ed9554830cc3af3828f036ce20110b775a6b00c43731a1db\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/19/a6/8f363d9939162782bb8439d886469756271abc01f76fbd790f\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.0 contractions-0.1.66 pyahocorasick-1.4.2 textsearch-0.0.21\n"
          ]
        }
      ],
      "source": [
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "i5Fcyejg7IM-"
      },
      "outputs": [],
      "source": [
        "import contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IZFKBq9B7Tt4"
      },
      "outputs": [],
      "source": [
        "def decontracted(phrase):\n",
        "\n",
        "    phrase = re.sub(r'[^\\w.?!;]', ' ', phrase)\n",
        "    phrase = re.sub(' +', ' ', phrase)\n",
        "    \n",
        "    #remove html tags from text\n",
        "    soup = BeautifulSoup(phrase, \"html.parser\")\n",
        "    phrase = soup.get_text(separator=\" \")\n",
        "    \n",
        "    #remove accented characters from text, e.g. café\n",
        "    phrase = unidecode.unidecode(phrase)\n",
        "    \n",
        "    \n",
        "    \n",
        "    sentences = re.split('([.;!?] *)', phrase)\n",
        "\n",
        "    return ' '.join([i.capitalize() for i in  sentences])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ry555SJcVs0I",
        "outputId": "40fd7bd6-6143-4bca-ddb8-51cd16729c2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting es_core_news_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.2.5/es_core_news_sm-2.2.5.tar.gz (16.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.2 MB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from es_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.9.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (4.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (1.24.3)\n",
            "Building wheels for collected packages: es-core-news-sm\n",
            "  Building wheel for es-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for es-core-news-sm: filename=es_core_news_sm-2.2.5-py3-none-any.whl size=16172933 sha256=8a2bc5ec897cf948f1723057f61e251e328e6cf1094ff87d4f55bd6e9bf8cc3b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ocahk5tl/wheels/21/8d/a9/6c1a2809c55dd22cd9644ae503a52ba6206b04aa57ba83a3d8\n",
            "Successfully built es-core-news-sm\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('es_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "# Modules for languages\n",
        "# en_core_web_sm - for English\n",
        "# it_core_news_sm - for Italian\n",
        "# es_core_news_sm - for Spanish\n",
        "\n",
        "\n",
        "!python -m spacy download es_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "C1fdXByJV8TH"
      },
      "outputs": [],
      "source": [
        "import es_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KF62ZohcWE4U"
      },
      "outputs": [],
      "source": [
        "# initializing the language model\n",
        "\n",
        "nlp = es_core_news_sm.load(disable=['parser'])\n",
        "nlp.max_length = 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Mz90fYDi7eWb"
      },
      "outputs": [],
      "source": [
        "def prepare_english_text(input_path, output_path, error_list, nlp, enc = 'utf-8', gutenberg = True, title = ''):\n",
        "    \n",
        "    #English pipeline optimized for CPU. Components: tok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizer.\n",
        "    \n",
        "    #parser: The dependency parser jointly learns sentence segmentation and labelled dependency parsing, \n",
        "    #and can optionally learn to merge tokens that had been over-segmented by the tokenizer.\n",
        "    \n",
        "    pos_dict = {'PROPN': 'PERSON1', 'PRON': 'PRON1', 'NUM': 'ORDINAL1'}\n",
        "\n",
        "    \n",
        "    try:\n",
        "        raw_text = codecs.open(input_path, 'r', enc).read()\n",
        "    except:\n",
        "        return -1\n",
        "    \n",
        "    if gutenberg:\n",
        "        begin = raw_text.find('*** START OF THIS PROJECT GUTENBERG EBOOK ')\n",
        "        begin_2 = begin + 42 + len(title) + 3\n",
        "        if begin == -1:\n",
        "                begin = raw_text.find('START OF THE PROJECT GUTENBERG EBOOK ')\n",
        "                begin_2 = begin + 37 + len(title) + 3\n",
        "\n",
        "        end = raw_text.find('*** END OF THIS PROJECT GUTENBERG EBOOK')\n",
        "        if end == -1:\n",
        "            end = raw_text.find('*** END')\n",
        "        raw_text = raw_text[begin_2 : end]\n",
        "        \n",
        "    \n",
        "    preprocessed_text = decontracted(raw_text)\n",
        "    \n",
        "    nlp_doc = nlp(preprocessed_text)\n",
        "    sorted_ents = sorted(nlp_doc.ents, key = lambda x: len(x), reverse =  True)\n",
        "\n",
        "\n",
        "    for ent in sorted_ents:\n",
        "        preprocessed_text = preprocessed_text.replace(' ' + ent.text + ' ', ' ' + ent.label_+ '1 ')\n",
        "        \n",
        "        if not ent.text.islower():\n",
        "            preprocessed_text = preprocessed_text.replace(' ' + ent.text.lower() + ' ', ' ' + ent.label_+ '1 ')\n",
        "\n",
        "    new_nlp_doc = nlp(preprocessed_text)\n",
        "    file_name = path.split('/')[-1]\n",
        "    \n",
        "\n",
        "    with open(output_path + file_name, 'w+', ) as prepared_text:\n",
        "        for token in new_nlp_doc:\n",
        "            if token.text[-1] != '1':\n",
        "                if token.pos_ in pos_dict:\n",
        "                    try:\n",
        "                        prepared_text.write(pos_dict[token.pos_])\n",
        "                    except:\n",
        "                        error_list.append(token.pos_)\n",
        "                    prepared_text.write('\\n')\n",
        "                    \n",
        "                        \n",
        "                    \n",
        "                elif token.pos_ != 'PUNCT':\n",
        "                    try:\n",
        "                        prepared_text.write(token.lemma_.lower())\n",
        "                    except:\n",
        "                        error_list.append(token.pos_)\n",
        "                        \n",
        "                    prepared_text.write('\\n')\n",
        "                    \n",
        "\n",
        "            else:\n",
        "                try:\n",
        "                    repared_text.write(token.text)\n",
        "                except:\n",
        "                        error_list.append(token.pos_)\n",
        "                prepared_text.write('\\n')\n",
        "    return 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "u_mgAWa77hI2"
      },
      "outputs": [],
      "source": [
        "ind_gen = list(glob.glob(\"/content/drive/MyDrive/2022-01-15_Course_project/new_gen_text_es/*.txt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqKKpynH7h86",
        "outputId": "e47dbc06-86ac-4a53-e085-41bf767b531a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/2022-01-15_Course_project/new_gen_text_es/doc_9000.txt',\n",
              " '/content/drive/MyDrive/2022-01-15_Course_project/new_gen_text_es/doc_9001.txt',\n",
              " '/content/drive/MyDrive/2022-01-15_Course_project/new_gen_text_es/doc_9002.txt',\n",
              " '/content/drive/MyDrive/2022-01-15_Course_project/new_gen_text_es/doc_9003.txt',\n",
              " '/content/drive/MyDrive/2022-01-15_Course_project/new_gen_text_es/doc_9004.txt',\n",
              " '/content/drive/MyDrive/2022-01-15_Course_project/new_gen_text_es/doc_9005.txt',\n",
              " '/content/drive/MyDrive/2022-01-15_Course_project/new_gen_text_es/doc_9006.txt',\n",
              " '/content/drive/MyDrive/2022-01-15_Course_project/new_gen_text_es/doc_9007.txt',\n",
              " '/content/drive/MyDrive/2022-01-15_Course_project/new_gen_text_es/doc_9008.txt',\n",
              " '/content/drive/MyDrive/2022-01-15_Course_project/new_gen_text_es/doc_9009.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "ind_gen[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uo1uItmX7h_1",
        "outputId": "52f9aaeb-8793-4d50-e616-35bb011e8239"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29/01/2022 22:11:49 |  number of processed files: 0\n",
            "29/01/2022 22:13:02 |  number of processed files: 100\n",
            "29/01/2022 22:16:14 |  number of processed files: 200\n",
            "29/01/2022 22:16:40 |  number of processed files: 300\n",
            "29/01/2022 22:17:06 |  number of processed files: 400\n",
            "29/01/2022 22:17:32 |  number of processed files: 500\n",
            "29/01/2022 22:17:58 |  number of processed files: 600\n",
            "29/01/2022 22:18:24 |  number of processed files: 700\n",
            "29/01/2022 22:18:50 |  number of processed files: 800\n",
            "29/01/2022 22:19:16 |  number of processed files: 900\n",
            "29/01/2022 22:19:43 |  number of processed files: 1000\n",
            "29/01/2022 22:20:09 |  number of processed files: 1100\n",
            "29/01/2022 22:20:36 |  number of processed files: 1200\n",
            "29/01/2022 22:21:02 |  number of processed files: 1300\n",
            "29/01/2022 22:21:28 |  number of processed files: 1400\n",
            "29/01/2022 22:21:55 |  number of processed files: 1500\n",
            "29/01/2022 22:22:21 |  number of processed files: 1600\n",
            "29/01/2022 22:22:47 |  number of processed files: 1700\n",
            "29/01/2022 22:23:13 |  number of processed files: 1800\n",
            "29/01/2022 22:23:39 |  number of processed files: 1900\n",
            "29/01/2022 22:24:05 |  number of processed files: 2000\n",
            "29/01/2022 22:24:30 |  number of processed files: 2100\n",
            "29/01/2022 22:24:56 |  number of processed files: 2200\n",
            "29/01/2022 22:25:22 |  number of processed files: 2300\n",
            "29/01/2022 22:25:48 |  number of processed files: 2400\n",
            "29/01/2022 22:26:14 |  number of processed files: 2500\n",
            "29/01/2022 22:26:40 |  number of processed files: 2600\n",
            "29/01/2022 22:27:06 |  number of processed files: 2700\n",
            "29/01/2022 22:27:31 |  number of processed files: 2800\n",
            "29/01/2022 22:27:57 |  number of processed files: 2900\n",
            "29/01/2022 22:28:22 |  number of processed files: 3000\n",
            "29/01/2022 22:28:47 |  number of processed files: 3100\n",
            "29/01/2022 22:29:13 |  number of processed files: 3200\n",
            "29/01/2022 22:29:38 |  number of processed files: 3300\n",
            "29/01/2022 22:30:03 |  number of processed files: 3400\n",
            "29/01/2022 22:30:29 |  number of processed files: 3500\n",
            "29/01/2022 22:30:55 |  number of processed files: 3600\n",
            "29/01/2022 22:31:21 |  number of processed files: 3700\n",
            "29/01/2022 22:31:48 |  number of processed files: 3800\n",
            "29/01/2022 22:32:14 |  number of processed files: 3900\n",
            "29/01/2022 22:32:41 |  number of processed files: 4000\n",
            "29/01/2022 22:33:08 |  number of processed files: 4100\n",
            "29/01/2022 22:33:34 |  number of processed files: 4200\n",
            "29/01/2022 22:34:00 |  number of processed files: 4300\n",
            "29/01/2022 22:34:26 |  number of processed files: 4400\n",
            "29/01/2022 22:34:52 |  number of processed files: 4500\n",
            "29/01/2022 22:35:18 |  number of processed files: 4600\n",
            "29/01/2022 22:35:44 |  number of processed files: 4700\n",
            "29/01/2022 22:36:09 |  number of processed files: 4800\n",
            "29/01/2022 22:36:36 |  number of processed files: 4900\n",
            "29/01/2022 22:37:02 |  number of processed files: 5000\n",
            "29/01/2022 22:37:28 |  number of processed files: 5100\n",
            "29/01/2022 22:37:53 |  number of processed files: 5200\n",
            "29/01/2022 22:38:19 |  number of processed files: 5300\n",
            "29/01/2022 22:38:44 |  number of processed files: 5400\n",
            "29/01/2022 22:39:10 |  number of processed files: 5500\n",
            "29/01/2022 22:39:36 |  number of processed files: 5600\n",
            "29/01/2022 22:40:01 |  number of processed files: 5700\n",
            "29/01/2022 22:40:26 |  number of processed files: 5800\n",
            "29/01/2022 22:40:51 |  number of processed files: 5900\n",
            "29/01/2022 22:41:16 |  number of processed files: 6000\n",
            "29/01/2022 22:41:42 |  number of processed files: 6100\n",
            "29/01/2022 22:42:07 |  number of processed files: 6200\n",
            "29/01/2022 22:42:33 |  number of processed files: 6300\n",
            "29/01/2022 22:43:00 |  number of processed files: 6400\n",
            "29/01/2022 22:43:26 |  number of processed files: 6500\n",
            "29/01/2022 22:43:52 |  number of processed files: 6600\n",
            "29/01/2022 22:44:18 |  number of processed files: 6700\n",
            "29/01/2022 22:44:46 |  number of processed files: 6800\n",
            "29/01/2022 22:45:14 |  number of processed files: 6900\n",
            "29/01/2022 22:45:40 |  number of processed files: 7000\n",
            "29/01/2022 22:46:06 |  number of processed files: 7100\n",
            "29/01/2022 22:46:33 |  number of processed files: 7200\n",
            "29/01/2022 22:46:59 |  number of processed files: 7300\n",
            "29/01/2022 22:47:26 |  number of processed files: 7400\n",
            "29/01/2022 22:47:52 |  number of processed files: 7500\n",
            "29/01/2022 22:48:19 |  number of processed files: 7600\n",
            "29/01/2022 22:48:45 |  number of processed files: 7700\n",
            "29/01/2022 22:49:11 |  number of processed files: 7800\n",
            "29/01/2022 22:49:37 |  number of processed files: 7900\n",
            "29/01/2022 22:50:03 |  number of processed files: 8000\n",
            "29/01/2022 22:50:29 |  number of processed files: 8100\n",
            "29/01/2022 22:50:55 |  number of processed files: 8200\n",
            "29/01/2022 22:51:22 |  number of processed files: 8300\n",
            "29/01/2022 22:51:49 |  number of processed files: 8400\n",
            "29/01/2022 22:52:15 |  number of processed files: 8500\n",
            "29/01/2022 22:52:41 |  number of processed files: 8600\n",
            "29/01/2022 22:53:08 |  number of processed files: 8700\n",
            "29/01/2022 22:53:34 |  number of processed files: 8800\n",
            "29/01/2022 22:54:00 |  number of processed files: 8900\n",
            "29/01/2022 22:54:26 |  number of processed files: 9000\n",
            "29/01/2022 22:54:51 |  number of processed files: 9100\n",
            "29/01/2022 22:55:17 |  number of processed files: 9200\n",
            "29/01/2022 22:55:43 |  number of processed files: 9300\n",
            "29/01/2022 22:56:10 |  number of processed files: 9400\n",
            "29/01/2022 22:56:36 |  number of processed files: 9500\n",
            "29/01/2022 22:57:02 |  number of processed files: 9600\n",
            "29/01/2022 22:57:28 |  number of processed files: 9700\n",
            "29/01/2022 22:57:54 |  number of processed files: 9800\n",
            "29/01/2022 22:58:20 |  number of processed files: 9900\n"
          ]
        }
      ],
      "source": [
        "# Let's process only big files, \n",
        "# because there are a lot of small files, which is why processing takes a very long time\n",
        "\n",
        "tocken_error = []\n",
        "prepare_book = []\n",
        "num = 0\n",
        "book_err = []\n",
        "sum_len = 0\n",
        "\n",
        "for path in ind_gen:\n",
        "    if num % 100 == 0:\n",
        "        now = datetime.now()\n",
        "        dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
        "        print(dt_string, '| ',  'number of processed files: ' + str(num))\n",
        "    \n",
        "    title = path.split('/')[-1].split('.txt')[0]\n",
        "    \n",
        "    res = prepare_english_text(path, '/content/drive/MyDrive/2022-01-15_Course_project/new_prep_gen_text_es/', \n",
        "                               tocken_error, nlp, gutenberg = False, title = title)\n",
        "    if res < 0:\n",
        "      book_err.append(path)\n",
        "      print('ERROR: ', path)\n",
        "    else:\n",
        "        prepare_book.append(path)\n",
        "    num += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYLouVH89AoC"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "3_Preprocessing_bot_es.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}