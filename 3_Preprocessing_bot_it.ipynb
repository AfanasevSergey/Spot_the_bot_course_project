{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SXghWytGaz4E"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from google.colab import drive\n",
    "import os, zipfile, glob\n",
    "import pickle\n",
    "import re\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8gGPuN1LbDYP",
    "outputId": "73be236a-a446-421a-f177-b83a656c1d84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w_MSI2ySRthF",
    "outputId": "717fc2b5-cc50-4d89-8822-b1dd644ac540"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unidecode\n",
      "  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\n",
      "\u001b[?25l\r",
      "\u001b[K     |█▍                              | 10 kB 38.9 MB/s eta 0:00:01\r",
      "\u001b[K     |██▉                             | 20 kB 9.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████▏                           | 30 kB 8.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▋                          | 40 kB 7.9 MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 51 kB 4.9 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▍                       | 61 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▊                      | 71 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▏                    | 81 kB 6.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▌                   | 92 kB 4.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 102 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▎                | 112 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▊               | 122 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 133 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▌            | 143 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▉           | 153 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▎         | 163 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▋        | 174 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 184 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▍     | 194 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▉    | 204 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 215 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▋ | 225 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 235 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 235 kB 5.2 MB/s \n",
      "\u001b[?25hInstalling collected packages: unidecode\n",
      "Successfully installed unidecode-1.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ZTBdWWP1BSWC"
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "from bs4 import BeautifulSoup\n",
    "import unidecode\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nBe-Ea4A7IJ-",
    "outputId": "e7d0a71e-8c83-4322-c6f0-360a77bbedd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.10.0.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jbDVXtbK7V0Z",
    "outputId": "fb13933b-d7f7-45b2-b777-d5be7cbe200e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Downloading contractions-0.0.58-py2.py3-none-any.whl (8.0 kB)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
      "Collecting pyahocorasick\n",
      "  Downloading pyahocorasick-1.4.2.tar.gz (321 kB)\n",
      "\u001b[K     |████████████████████████████████| 321 kB 5.0 MB/s \n",
      "\u001b[?25hCollecting anyascii\n",
      "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
      "\u001b[K     |████████████████████████████████| 284 kB 54.1 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
      "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85446 sha256=87254d21436e7b9963fc4f3e0eef44f26058d809f6f91a7461d5e01d0f5b9b1c\n",
      "  Stored in directory: /root/.cache/pip/wheels/25/19/a6/8f363d9939162782bb8439d886469756271abc01f76fbd790f\n",
      "Successfully built pyahocorasick\n",
      "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.0 contractions-0.0.58 pyahocorasick-1.4.2 textsearch-0.0.21\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "i5Fcyejg7IM-"
   },
   "outputs": [],
   "source": [
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "IZFKBq9B7Tt4"
   },
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "\n",
    "    phrase = re.sub(r'[^\\w.?!;]', ' ', phrase)\n",
    "    phrase = re.sub(' +', ' ', phrase)\n",
    "    \n",
    "    #remove html tags from text\n",
    "    soup = BeautifulSoup(phrase, \"html.parser\")\n",
    "    phrase = soup.get_text(separator=\" \")\n",
    "    \n",
    "    #remove accented characters from text, e.g. café\n",
    "    phrase = unidecode.unidecode(phrase)\n",
    "    \n",
    "    \n",
    "    \n",
    "    sentences = re.split('([.;!?] *)', phrase)\n",
    "\n",
    "    return ' '.join([i.capitalize() for i in  sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VSKvYU-MjEFx",
    "outputId": "86704b25-ac02-42bd-8e14-5bf6ac536505"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting it_core_news_sm==2.2.5\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-2.2.5/it_core_news_sm-2.2.5.tar.gz (14.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.5 MB 4.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from it_core_news_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (0.9.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (3.0.6)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (2.23.0)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (2.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (1.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (4.62.3)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (57.4.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (1.19.5)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->it_core_news_sm==2.2.5) (4.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->it_core_news_sm==2.2.5) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->it_core_news_sm==2.2.5) (3.10.0.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->it_core_news_sm==2.2.5) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->it_core_news_sm==2.2.5) (2021.10.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->it_core_news_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->it_core_news_sm==2.2.5) (2.10)\n",
      "Building wheels for collected packages: it-core-news-sm\n",
      "  Building wheel for it-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for it-core-news-sm: filename=it_core_news_sm-2.2.5-py3-none-any.whl size=14471128 sha256=46ed4c0f7a9847434ced897da034f4671ca99207728874f00b70add694aa271f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-oz9rvjb0/wheels/87/88/46/36fd0cabbebd89b2ee247bf113c1ca4f2cb184f8b7a6758ba2\n",
      "Successfully built it-core-news-sm\n",
      "Installing collected packages: it-core-news-sm\n",
      "Successfully installed it-core-news-sm-2.2.5\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('it_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "# Modules for languages\n",
    "# en_core_web_sm - for English\n",
    "# it_core_news_sm - for Italian\n",
    "# es_core_news_sm - for Spanish\n",
    "\n",
    "!python -m spacy download it_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "GXNP0n1xjRP0"
   },
   "outputs": [],
   "source": [
    "import it_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "xP1wqp9KjalU"
   },
   "outputs": [],
   "source": [
    "# initializing the language model\n",
    "\n",
    "nlp = it_core_news_sm.load(disable=['parser'])\n",
    "nlp.max_length = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Mz90fYDi7eWb"
   },
   "outputs": [],
   "source": [
    "def prepare_italian_text(input_path, output_path, error_list, nlp, enc = 'utf-8', gutenberg = True, title = ''):\n",
    "    \n",
    "    #English pipeline optimized for CPU. Components: tok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizer.\n",
    "    \n",
    "    #parser: The dependency parser jointly learns sentence segmentation and labelled dependency parsing, \n",
    "    #and can optionally learn to merge tokens that had been over-segmented by the tokenizer.\n",
    "\n",
    "    \n",
    "    pos_dict = {'PROPN': 'PERSON1', 'PRON': 'PRON1', 'NUM': 'ORDINAL1'}\n",
    "\n",
    "    \n",
    "    try:\n",
    "        raw_text = codecs.open(input_path, 'r', enc).read()\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "    if gutenberg:\n",
    "        begin = raw_text.find('*** START OF THIS PROJECT GUTENBERG EBOOK ')\n",
    "        begin_2 = begin + 42 + len(title) + 3\n",
    "        if begin == -1:\n",
    "                begin = raw_text.find('START OF THE PROJECT GUTENBERG EBOOK ')\n",
    "                begin_2 = begin + 37 + len(title) + 3\n",
    "\n",
    "        end = raw_text.find('*** END OF THIS PROJECT GUTENBERG EBOOK')\n",
    "        if end == -1:\n",
    "            end = raw_text.find('*** END')\n",
    "        raw_text = raw_text[begin_2 : end]\n",
    "        \n",
    "    \n",
    "    preprocessed_text = decontracted(raw_text)\n",
    "    \n",
    "    nlp_doc = nlp(preprocessed_text)\n",
    "    sorted_ents = sorted(nlp_doc.ents, key = lambda x: len(x), reverse =  True)\n",
    "\n",
    "\n",
    "    for ent in sorted_ents:\n",
    "        preprocessed_text = preprocessed_text.replace(' ' + ent.text + ' ', ' ' + ent.label_+ '1 ')\n",
    "        \n",
    "        if not ent.text.islower():\n",
    "            preprocessed_text = preprocessed_text.replace(' ' + ent.text.lower() + ' ', ' ' + ent.label_+ '1 ')\n",
    "\n",
    "    new_nlp_doc = nlp(preprocessed_text)\n",
    "    file_name = path.split('/')[-1]\n",
    "    \n",
    "\n",
    "    with open(output_path + file_name, 'w+', ) as prepared_text:\n",
    "        for token in new_nlp_doc:\n",
    "            if token.text[-1] != '1':\n",
    "                if token.pos_ in pos_dict:\n",
    "                    try:\n",
    "                        prepared_text.write(pos_dict[token.pos_])\n",
    "                    except:\n",
    "                        error_list.append(token.pos_)\n",
    "                    prepared_text.write('\\n')\n",
    "                    \n",
    "                        \n",
    "                    \n",
    "                elif token.pos_ != 'PUNCT':\n",
    "                    try:\n",
    "                        prepared_text.write(token.lemma_.lower())\n",
    "                    except:\n",
    "                        error_list.append(token.pos_)\n",
    "                        \n",
    "                    prepared_text.write('\\n')\n",
    "                    \n",
    "\n",
    "            else:\n",
    "                try:\n",
    "                    repared_text.write(token.text)\n",
    "                except:\n",
    "                        error_list.append(token.pos_)\n",
    "                prepared_text.write('\\n')\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "u_mgAWa77hI2"
   },
   "outputs": [],
   "source": [
    "ind_gen = list(glob.glob(\"/content/drive/MyDrive/2022-01-15_Course_project/gen_text_it/*.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QqKKpynH7h86",
    "outputId": "87193f2a-a59e-430d-d95e-326932c0d6a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/content/drive/MyDrive/2022-01-15_Course_project/gen_text_it/doc_14000.txt',\n",
       " '/content/drive/MyDrive/2022-01-15_Course_project/gen_text_it/doc_14001.txt',\n",
       " '/content/drive/MyDrive/2022-01-15_Course_project/gen_text_it/doc_14002.txt',\n",
       " '/content/drive/MyDrive/2022-01-15_Course_project/gen_text_it/doc_14003.txt',\n",
       " '/content/drive/MyDrive/2022-01-15_Course_project/gen_text_it/doc_14004.txt',\n",
       " '/content/drive/MyDrive/2022-01-15_Course_project/gen_text_it/doc_14005.txt',\n",
       " '/content/drive/MyDrive/2022-01-15_Course_project/gen_text_it/doc_14006.txt',\n",
       " '/content/drive/MyDrive/2022-01-15_Course_project/gen_text_it/doc_14007.txt',\n",
       " '/content/drive/MyDrive/2022-01-15_Course_project/gen_text_it/doc_14008.txt',\n",
       " '/content/drive/MyDrive/2022-01-15_Course_project/gen_text_it/doc_14009.txt']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_gen[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uo1uItmX7h_1",
    "outputId": "2291b184-d722-44db-ea63-76137a324afd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/01/2022 09:06:11 |  number of processed files: 0\n",
      "19/01/2022 09:06:48 |  number of processed files: 100\n",
      "19/01/2022 09:07:25 |  number of processed files: 200\n",
      "19/01/2022 09:08:02 |  number of processed files: 300\n",
      "19/01/2022 09:08:39 |  number of processed files: 400\n",
      "19/01/2022 09:09:17 |  number of processed files: 500\n",
      "19/01/2022 09:09:54 |  number of processed files: 600\n",
      "19/01/2022 09:10:32 |  number of processed files: 700\n",
      "19/01/2022 09:11:08 |  number of processed files: 800\n",
      "19/01/2022 09:11:45 |  number of processed files: 900\n",
      "19/01/2022 09:12:22 |  number of processed files: 1000\n",
      "19/01/2022 09:13:01 |  number of processed files: 1100\n",
      "19/01/2022 09:13:38 |  number of processed files: 1200\n",
      "19/01/2022 09:14:16 |  number of processed files: 1300\n",
      "19/01/2022 09:14:52 |  number of processed files: 1400\n",
      "19/01/2022 09:15:28 |  number of processed files: 1500\n",
      "19/01/2022 09:16:05 |  number of processed files: 1600\n",
      "19/01/2022 09:16:42 |  number of processed files: 1700\n",
      "19/01/2022 09:17:19 |  number of processed files: 1800\n",
      "19/01/2022 09:17:55 |  number of processed files: 1900\n",
      "19/01/2022 09:18:31 |  number of processed files: 2000\n",
      "19/01/2022 09:19:08 |  number of processed files: 2100\n",
      "19/01/2022 09:19:47 |  number of processed files: 2200\n",
      "19/01/2022 09:20:25 |  number of processed files: 2300\n",
      "19/01/2022 09:21:03 |  number of processed files: 2400\n",
      "19/01/2022 09:21:41 |  number of processed files: 2500\n",
      "19/01/2022 09:22:19 |  number of processed files: 2600\n",
      "19/01/2022 09:22:56 |  number of processed files: 2700\n",
      "19/01/2022 09:23:34 |  number of processed files: 2800\n",
      "19/01/2022 09:24:11 |  number of processed files: 2900\n",
      "19/01/2022 09:24:49 |  number of processed files: 3000\n",
      "19/01/2022 09:25:28 |  number of processed files: 3100\n",
      "19/01/2022 09:26:08 |  number of processed files: 3200\n",
      "19/01/2022 09:26:46 |  number of processed files: 3300\n",
      "19/01/2022 09:27:24 |  number of processed files: 3400\n",
      "19/01/2022 09:28:01 |  number of processed files: 3500\n",
      "19/01/2022 09:28:37 |  number of processed files: 3600\n",
      "19/01/2022 09:29:14 |  number of processed files: 3700\n",
      "19/01/2022 09:29:51 |  number of processed files: 3800\n",
      "19/01/2022 09:30:29 |  number of processed files: 3900\n",
      "19/01/2022 09:31:06 |  number of processed files: 4000\n",
      "19/01/2022 09:31:46 |  number of processed files: 4100\n",
      "19/01/2022 09:32:23 |  number of processed files: 4200\n",
      "19/01/2022 09:33:01 |  number of processed files: 4300\n",
      "19/01/2022 09:33:39 |  number of processed files: 4400\n",
      "19/01/2022 09:34:18 |  number of processed files: 4500\n",
      "19/01/2022 09:34:57 |  number of processed files: 4600\n",
      "19/01/2022 09:35:35 |  number of processed files: 4700\n",
      "19/01/2022 09:36:12 |  number of processed files: 4800\n",
      "19/01/2022 09:36:50 |  number of processed files: 4900\n",
      "19/01/2022 09:37:28 |  number of processed files: 5000\n",
      "19/01/2022 09:38:07 |  number of processed files: 5100\n",
      "19/01/2022 09:38:45 |  number of processed files: 5200\n",
      "19/01/2022 09:39:23 |  number of processed files: 5300\n",
      "19/01/2022 09:40:02 |  number of processed files: 5400\n",
      "19/01/2022 09:40:40 |  number of processed files: 5500\n",
      "19/01/2022 09:41:18 |  number of processed files: 5600\n",
      "19/01/2022 09:41:56 |  number of processed files: 5700\n",
      "19/01/2022 09:42:33 |  number of processed files: 5800\n",
      "19/01/2022 09:43:10 |  number of processed files: 5900\n",
      "19/01/2022 09:43:48 |  number of processed files: 6000\n",
      "19/01/2022 09:44:27 |  number of processed files: 6100\n",
      "19/01/2022 09:45:05 |  number of processed files: 6200\n",
      "19/01/2022 09:45:44 |  number of processed files: 6300\n",
      "19/01/2022 09:46:22 |  number of processed files: 6400\n",
      "19/01/2022 09:47:00 |  number of processed files: 6500\n",
      "19/01/2022 09:47:38 |  number of processed files: 6600\n",
      "19/01/2022 09:48:16 |  number of processed files: 6700\n",
      "19/01/2022 09:48:55 |  number of processed files: 6800\n",
      "19/01/2022 09:49:33 |  number of processed files: 6900\n",
      "19/01/2022 09:50:10 |  number of processed files: 7000\n",
      "19/01/2022 09:50:49 |  number of processed files: 7100\n",
      "19/01/2022 09:51:27 |  number of processed files: 7200\n",
      "19/01/2022 09:52:05 |  number of processed files: 7300\n",
      "19/01/2022 09:52:43 |  number of processed files: 7400\n",
      "19/01/2022 09:53:21 |  number of processed files: 7500\n",
      "19/01/2022 09:53:59 |  number of processed files: 7600\n",
      "19/01/2022 09:54:36 |  number of processed files: 7700\n",
      "19/01/2022 09:55:13 |  number of processed files: 7800\n",
      "19/01/2022 09:55:50 |  number of processed files: 7900\n",
      "19/01/2022 09:56:28 |  number of processed files: 8000\n",
      "19/01/2022 09:57:05 |  number of processed files: 8100\n",
      "19/01/2022 09:57:43 |  number of processed files: 8200\n",
      "19/01/2022 09:58:20 |  number of processed files: 8300\n",
      "19/01/2022 09:58:58 |  number of processed files: 8400\n",
      "19/01/2022 09:59:36 |  number of processed files: 8500\n",
      "19/01/2022 10:00:14 |  number of processed files: 8600\n",
      "19/01/2022 10:00:51 |  number of processed files: 8700\n",
      "19/01/2022 10:01:29 |  number of processed files: 8800\n",
      "19/01/2022 10:02:06 |  number of processed files: 8900\n",
      "19/01/2022 10:02:44 |  number of processed files: 9000\n",
      "19/01/2022 10:03:21 |  number of processed files: 9100\n",
      "19/01/2022 10:04:00 |  number of processed files: 9200\n",
      "19/01/2022 10:04:41 |  number of processed files: 9300\n",
      "19/01/2022 10:05:23 |  number of processed files: 9400\n",
      "19/01/2022 10:06:03 |  number of processed files: 9500\n",
      "19/01/2022 10:06:42 |  number of processed files: 9600\n",
      "19/01/2022 10:07:20 |  number of processed files: 9700\n",
      "19/01/2022 10:07:58 |  number of processed files: 9800\n",
      "19/01/2022 10:08:36 |  number of processed files: 9900\n",
      "19/01/2022 10:09:12 |  number of processed files: 10000\n",
      "19/01/2022 10:09:50 |  number of processed files: 10100\n",
      "19/01/2022 10:10:29 |  number of processed files: 10200\n",
      "19/01/2022 10:11:05 |  number of processed files: 10300\n",
      "19/01/2022 10:11:44 |  number of processed files: 10400\n",
      "19/01/2022 10:12:21 |  number of processed files: 10500\n",
      "19/01/2022 10:12:58 |  number of processed files: 10600\n",
      "19/01/2022 10:13:34 |  number of processed files: 10700\n",
      "19/01/2022 10:14:12 |  number of processed files: 10800\n",
      "19/01/2022 10:14:49 |  number of processed files: 10900\n",
      "19/01/2022 10:15:26 |  number of processed files: 11000\n",
      "19/01/2022 10:16:03 |  number of processed files: 11100\n",
      "19/01/2022 10:16:41 |  number of processed files: 11200\n",
      "19/01/2022 10:17:19 |  number of processed files: 11300\n",
      "19/01/2022 10:17:57 |  number of processed files: 11400\n",
      "19/01/2022 10:18:34 |  number of processed files: 11500\n",
      "19/01/2022 10:19:13 |  number of processed files: 11600\n",
      "19/01/2022 10:19:50 |  number of processed files: 11700\n",
      "19/01/2022 10:20:27 |  number of processed files: 11800\n",
      "19/01/2022 10:21:04 |  number of processed files: 11900\n",
      "19/01/2022 10:21:41 |  number of processed files: 12000\n",
      "19/01/2022 10:22:19 |  number of processed files: 12100\n",
      "19/01/2022 10:22:56 |  number of processed files: 12200\n",
      "19/01/2022 10:23:34 |  number of processed files: 12300\n",
      "19/01/2022 10:24:11 |  number of processed files: 12400\n",
      "19/01/2022 10:24:49 |  number of processed files: 12500\n",
      "19/01/2022 10:25:27 |  number of processed files: 12600\n",
      "19/01/2022 10:26:04 |  number of processed files: 12700\n",
      "19/01/2022 10:26:40 |  number of processed files: 12800\n",
      "19/01/2022 10:27:17 |  number of processed files: 12900\n",
      "19/01/2022 10:27:55 |  number of processed files: 13000\n",
      "19/01/2022 10:28:35 |  number of processed files: 13100\n",
      "19/01/2022 10:29:12 |  number of processed files: 13200\n",
      "19/01/2022 10:29:49 |  number of processed files: 13300\n",
      "19/01/2022 10:30:26 |  number of processed files: 13400\n",
      "19/01/2022 10:31:04 |  number of processed files: 13500\n",
      "19/01/2022 10:31:41 |  number of processed files: 13600\n",
      "19/01/2022 10:32:18 |  number of processed files: 13700\n",
      "19/01/2022 10:32:55 |  number of processed files: 13800\n",
      "19/01/2022 10:33:32 |  number of processed files: 13900\n",
      "19/01/2022 10:34:09 |  number of processed files: 14000\n",
      "19/01/2022 10:34:46 |  number of processed files: 14100\n",
      "19/01/2022 10:35:23 |  number of processed files: 14200\n",
      "19/01/2022 10:36:00 |  number of processed files: 14300\n",
      "19/01/2022 10:36:38 |  number of processed files: 14400\n",
      "19/01/2022 10:37:15 |  number of processed files: 14500\n",
      "19/01/2022 10:37:51 |  number of processed files: 14600\n",
      "19/01/2022 10:38:28 |  number of processed files: 14700\n",
      "19/01/2022 10:39:04 |  number of processed files: 14800\n",
      "19/01/2022 10:39:40 |  number of processed files: 14900\n"
     ]
    }
   ],
   "source": [
    "# Let's process only big files, \n",
    "# because there are a lot of small files, which is why processing takes a very long time\n",
    "\n",
    "tocken_error = []\n",
    "prepare_book = []\n",
    "\n",
    "num = 0\n",
    "book_err = []\n",
    "sum_len = 0\n",
    "\n",
    "for path in ind_gen:\n",
    "    if num % 100 == 0:\n",
    "        now = datetime.now()\n",
    "        dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "        print(dt_string, '| ',  'number of processed files: ' + str(num))\n",
    "    \n",
    "    title = path.split('/')[-1].split('.txt')[0]\n",
    "    \n",
    "    res = prepare_italian_text(path, '/content/drive/MyDrive/2022-01-15_Course_project/prep_get_text_it/', \n",
    "                               tocken_error,  nlp, gutenberg = False, title = title)\n",
    "    if res < 0:\n",
    "      book_err.append(path)\n",
    "      print('ERROR: ', path)\n",
    "    else:\n",
    "        prepare_book.append(path)\n",
    "    num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYLouVH89AoC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Preprocessing_geneate_it.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
